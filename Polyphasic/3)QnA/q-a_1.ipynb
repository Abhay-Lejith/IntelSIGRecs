{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e998caae",
   "metadata": {
    "id": "mI1zNQQ5WaGW",
    "papermill": {
     "duration": 0.010559,
     "end_time": "2023-10-20T18:29:48.399573",
     "exception": false,
     "start_time": "2023-10-20T18:29:48.389014",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Question Answering‚ùì**\n",
    "with fine-tuned BERT on newsQA.  \n",
    "\n",
    "Question answering comes in many forms. We‚Äôll look at the particular type of extractive QA that involves answering a question about a passage by highlighting the segment of the passage that answers the question. This involves fine-tuning a model which predicts a start position and an end position in the passage. More specifically, we will fine tune the [bert-base-uncased](https://huggingface.co/bert-base-uncased) model on the [NewsQA](https://huggingface.co/datasets/lucadiliello/newsqa) dataset.\n",
    "\n",
    "I have followed [this tutorial](https://github.com/angelosps/Question-Answering) from the for how to fine tune BERT on SQuAD 2.0 which in our case is a custom newsQA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa5b39a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T18:29:48.420735Z",
     "iopub.status.busy": "2023-10-20T18:29:48.420387Z",
     "iopub.status.idle": "2023-10-20T18:30:00.757447Z",
     "shell.execute_reply": "2023-10-20T18:30:00.756241Z"
    },
    "id": "NLAz1cug_MSc",
    "outputId": "62884f19-0941-4706-8777-876f09e53f3a",
    "papermill": {
     "duration": 12.350558,
     "end_time": "2023-10-20T18:30:00.759958",
     "exception": false,
     "start_time": "2023-10-20T18:29:48.409400",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.33.0)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.16.4)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.6.3)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\r\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\r\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.3.3)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.9.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.6.3)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.1.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56c8beda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T18:30:00.781913Z",
     "iopub.status.busy": "2023-10-20T18:30:00.781598Z",
     "iopub.status.idle": "2023-10-20T18:30:13.464419Z",
     "shell.execute_reply": "2023-10-20T18:30:13.463429Z"
    },
    "id": "NbYipki39qLw",
    "papermill": {
     "duration": 12.696535,
     "end_time": "2023-10-20T18:30:13.466900",
     "exception": false,
     "start_time": "2023-10-20T18:30:00.770365",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bab4dadb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T18:30:13.489183Z",
     "iopub.status.busy": "2023-10-20T18:30:13.488413Z",
     "iopub.status.idle": "2023-10-20T18:30:24.807349Z",
     "shell.execute_reply": "2023-10-20T18:30:24.806155Z"
    },
    "id": "mlYX0bLL_YcI",
    "outputId": "f648648f-5cdc-42ce-9495-da9cc41490ed",
    "papermill": {
     "duration": 11.332519,
     "end_time": "2023-10-20T18:30:24.809913",
     "exception": false,
     "start_time": "2023-10-20T18:30:13.477394",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.23.5)\r\n",
      "Requirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\r\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.7)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.0.2)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\r\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\r\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.3.0)\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.15)\r\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2023.9.0)\r\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.4)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.16.4)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\r\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.18.0)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (3.1.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.12.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.6.3)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.15)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.7.22)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4df47cd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T18:30:24.833056Z",
     "iopub.status.busy": "2023-10-20T18:30:24.832721Z",
     "iopub.status.idle": "2023-10-20T18:30:28.863671Z",
     "shell.execute_reply": "2023-10-20T18:30:28.862810Z"
    },
    "id": "rxbbQQ-j9x6x",
    "outputId": "9fa4e963-6491-4461-bc22-ca8c5c3d1a2c",
    "papermill": {
     "duration": 4.045571,
     "end_time": "2023-10-20T18:30:28.866448",
     "exception": false,
     "start_time": "2023-10-20T18:30:24.820877",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset parquet/lucadiliello--newsqa to /root/.cache/huggingface/datasets/parquet/lucadiliello--newsqa-206550e86bcc3ded/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76f780247f4d43759cc01829bbfb31c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23e30e622f4c40cdb01f19722eb5c5d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/29.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b011aa71e9844c89a648d20281ef3bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.63M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4133d3569a024b02b04cd4c5e7d4e676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/parquet/lucadiliello--newsqa-206550e86bcc3ded/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12c9e76379244ff6b6ee489a536c1ae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the NewsQA dataset\n",
    "from datasets import load_dataset\n",
    "newsqa_dataset = load_dataset('lucadiliello/newsqa')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe6d354",
   "metadata": {
    "id": "65heqpOoXiRs",
    "papermill": {
     "duration": 0.011543,
     "end_time": "2023-10-20T18:30:28.891054",
     "exception": false,
     "start_time": "2023-10-20T18:30:28.879511",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### **Get data üìÅ**\n",
    "\n",
    "Let's extract our data and store them into some data structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa4d81ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T18:30:28.916379Z",
     "iopub.status.busy": "2023-10-20T18:30:28.915777Z",
     "iopub.status.idle": "2023-10-20T18:30:28.922624Z",
     "shell.execute_reply": "2023-10-20T18:30:28.921813Z"
    },
    "id": "lks0zjIo90PS",
    "papermill": {
     "duration": 0.021633,
     "end_time": "2023-10-20T18:30:28.924454",
     "exception": false,
     "start_time": "2023-10-20T18:30:28.902821",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_newsqa_data(dataset):\n",
    "    contexts = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "    string_ans = []\n",
    "\n",
    "    for item in dataset:\n",
    "        context = item['context']\n",
    "        question = item['question']\n",
    "        answer = {'answer_start': item['labels'][0]['start'][0], 'answer_end': item['labels'][0]['end'][0]}  # Assuming there's only one answer\n",
    "        string_answer = item['answers'][0]\n",
    "        \n",
    "        contexts.append(context)\n",
    "        questions.append(question)\n",
    "        answers.append(answer)\n",
    "        string_ans.append(string_answer)\n",
    "    return contexts, questions, answers, string_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ec9c14b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T18:30:28.948979Z",
     "iopub.status.busy": "2023-10-20T18:30:28.948696Z",
     "iopub.status.idle": "2023-10-20T18:30:29.810000Z",
     "shell.execute_reply": "2023-10-20T18:30:29.809068Z"
    },
    "id": "YEWSoAaM91rV",
    "papermill": {
     "duration": 0.876066,
     "end_time": "2023-10-20T18:30:29.812330",
     "exception": false,
     "start_time": "2023-10-20T18:30:28.936264",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_contexts, train_questions, train_answers, train_str_ans = read_newsqa_data(newsqa_dataset['train'].select(list(range(5000))))\n",
    "valid_contexts, valid_questions, valid_answers, valid_str_ans = read_newsqa_data(newsqa_dataset['validation'].select(list(range(1000))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c9606ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T18:30:29.838715Z",
     "iopub.status.busy": "2023-10-20T18:30:29.838397Z",
     "iopub.status.idle": "2023-10-20T18:30:29.844887Z",
     "shell.execute_reply": "2023-10-20T18:30:29.844040Z"
    },
    "papermill": {
     "duration": 0.021545,
     "end_time": "2023-10-20T18:30:29.846895",
     "exception": false,
     "start_time": "2023-10-20T18:30:29.825350",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['19',\n",
       " 'February.',\n",
       " 'rape and murder',\n",
       " 'Moninder Singh Pandher',\n",
       " 'Moninder Singh Pandher']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_str_ans[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fd9cd7",
   "metadata": {
    "id": "UWQhWWW3Xs8-",
    "papermill": {
     "duration": 0.011514,
     "end_time": "2023-10-20T18:30:29.870179",
     "exception": false,
     "start_time": "2023-10-20T18:30:29.858665",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### **Tokenization üî¢**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71508d25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T18:30:29.895778Z",
     "iopub.status.busy": "2023-10-20T18:30:29.895453Z",
     "iopub.status.idle": "2023-10-20T18:30:38.021515Z",
     "shell.execute_reply": "2023-10-20T18:30:38.020717Z"
    },
    "id": "2M0x8hjV-ELe",
    "papermill": {
     "duration": 8.141661,
     "end_time": "2023-10-20T18:30:38.023764",
     "exception": false,
     "start_time": "2023-10-20T18:30:29.882103",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3c8000900dc41c9921f3496d868c141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)okenizer_config.json:   0%|          | 0.00/79.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a67a0ed5e3e4edfb4cbcc4153c29d86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2121a1e89cdc4c28bf77d38b9782bcb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f6093f02ba645c58788f21e176ce607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b6c3d075f9d4faf967c4c50bb2a6e31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)cial_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the RoBERTa tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('deepset/roberta-base-squad2')\n",
    "train_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True)\n",
    "valid_encodings = tokenizer(valid_contexts, valid_questions, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0beeecd9",
   "metadata": {
    "id": "EvTnonkjX7lj",
    "papermill": {
     "duration": 0.012304,
     "end_time": "2023-10-20T18:30:38.049255",
     "exception": false,
     "start_time": "2023-10-20T18:30:38.036951",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Next we need to convert our character start/end positions to token start/end positions. Why is that? Because our words converted into tokens, so the answer start/end needs to show the index of start/end token which contains the answer and not the specific characters in the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6dfae909",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T18:30:38.075670Z",
     "iopub.status.busy": "2023-10-20T18:30:38.075329Z",
     "iopub.status.idle": "2023-10-20T18:30:38.082059Z",
     "shell.execute_reply": "2023-10-20T18:30:38.081300Z"
    },
    "id": "YBY8gSt8-MV3",
    "papermill": {
     "duration": 0.022215,
     "end_time": "2023-10-20T18:30:38.083947",
     "exception": false,
     "start_time": "2023-10-20T18:30:38.061732",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert character start/end positions to token start/end positions\n",
    "def add_token_positions(encodings, answers):\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    for i in range(len(answers)):\n",
    "        char_start = answers[i]['answer_start']\n",
    "        char_end = answers[i]['answer_end']\n",
    "\n",
    "        token_start = encodings.char_to_token(i, char_start)\n",
    "        token_end = encodings.char_to_token(i, char_end)\n",
    "\n",
    "        start_positions.append(token_start)\n",
    "        end_positions.append(token_end)\n",
    "\n",
    "        if token_start is None:\n",
    "            start_positions[-1] = tokenizer.model_max_length\n",
    "        if token_end is None:\n",
    "            end_positions[-1] = tokenizer.model_max_length\n",
    "\n",
    "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "995e9c93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T18:30:38.109983Z",
     "iopub.status.busy": "2023-10-20T18:30:38.109702Z",
     "iopub.status.idle": "2023-10-20T18:30:38.130166Z",
     "shell.execute_reply": "2023-10-20T18:30:38.129401Z"
    },
    "id": "Cfvg6_K8DG1F",
    "papermill": {
     "duration": 0.035618,
     "end_time": "2023-10-20T18:30:38.132008",
     "exception": false,
     "start_time": "2023-10-20T18:30:38.096390",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "add_token_positions(train_encodings, train_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25f74708",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T18:30:38.158115Z",
     "iopub.status.busy": "2023-10-20T18:30:38.157811Z",
     "iopub.status.idle": "2023-10-20T18:30:38.165161Z",
     "shell.execute_reply": "2023-10-20T18:30:38.164370Z"
    },
    "id": "2okra58q-Sew",
    "papermill": {
     "duration": 0.022574,
     "end_time": "2023-10-20T18:30:38.167088",
     "exception": false,
     "start_time": "2023-10-20T18:30:38.144514",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "add_token_positions(valid_encodings, valid_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad1e8581",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T18:30:38.235278Z",
     "iopub.status.busy": "2023-10-20T18:30:38.234898Z",
     "iopub.status.idle": "2023-10-20T18:30:38.240636Z",
     "shell.execute_reply": "2023-10-20T18:30:38.239752Z"
    },
    "id": "CxfKEWKe-VrA",
    "papermill": {
     "duration": 0.063035,
     "end_time": "2023-10-20T18:30:38.242577",
     "exception": false,
     "start_time": "2023-10-20T18:30:38.179542",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NewsQA_Dataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad916755",
   "metadata": {
    "id": "oCbSpm-4X-qs",
    "papermill": {
     "duration": 0.012321,
     "end_time": "2023-10-20T18:30:38.267561",
     "exception": false,
     "start_time": "2023-10-20T18:30:38.255240",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Creating the dataset using the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38f285b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T18:30:38.294424Z",
     "iopub.status.busy": "2023-10-20T18:30:38.294110Z",
     "iopub.status.idle": "2023-10-20T18:30:38.298314Z",
     "shell.execute_reply": "2023-10-20T18:30:38.297496Z"
    },
    "id": "NONwsopL9oVe",
    "papermill": {
     "duration": 0.019868,
     "end_time": "2023-10-20T18:30:38.300252",
     "exception": false,
     "start_time": "2023-10-20T18:30:38.280384",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = NewsQA_Dataset(train_encodings)\n",
    "valid_dataset = NewsQA_Dataset(valid_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adf14930",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T18:30:38.326345Z",
     "iopub.status.busy": "2023-10-20T18:30:38.326067Z",
     "iopub.status.idle": "2023-10-20T18:30:38.330704Z",
     "shell.execute_reply": "2023-10-20T18:30:38.329857Z"
    },
    "id": "8iSTLk80IQhj",
    "papermill": {
     "duration": 0.019825,
     "end_time": "2023-10-20T18:30:38.332509",
     "exception": false,
     "start_time": "2023-10-20T18:30:38.312684",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create dataloaders for training and validation\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed18f22",
   "metadata": {
    "id": "vb1REJlnYLDI",
    "papermill": {
     "duration": 0.01232,
     "end_time": "2023-10-20T18:30:38.357383",
     "exception": false,
     "start_time": "2023-10-20T18:30:38.345063",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Importing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45b7b8df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T18:30:38.383315Z",
     "iopub.status.busy": "2023-10-20T18:30:38.383036Z",
     "iopub.status.idle": "2023-10-20T18:30:42.387828Z",
     "shell.execute_reply": "2023-10-20T18:30:42.387000Z"
    },
    "id": "Q9p5C52IGRwI",
    "papermill": {
     "duration": 4.020418,
     "end_time": "2023-10-20T18:30:42.390163",
     "exception": false,
     "start_time": "2023-10-20T18:30:38.369745",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f647949d462a4593b2c97fff91effc10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/496M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the RoBERTa model for question answering\n",
    "model = AutoModelForQuestionAnswering.from_pretrained('deepset/roberta-base-squad2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35522070",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T18:30:42.421895Z",
     "iopub.status.busy": "2023-10-20T18:30:42.421078Z",
     "iopub.status.idle": "2023-10-20T18:30:42.429218Z",
     "shell.execute_reply": "2023-10-20T18:30:42.428115Z"
    },
    "id": "QHlMqyHXGTRI",
    "outputId": "406ff893-c6b7-46b9-d111-ff75259cbb11",
    "papermill": {
     "duration": 0.028451,
     "end_time": "2023-10-20T18:30:42.431730",
     "exception": false,
     "start_time": "2023-10-20T18:30:42.403279",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 12\n"
     ]
    }
   ],
   "source": [
    "num_layers = model.config.num_hidden_layers\n",
    "print(f\"Number of layers: {num_layers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08522ebb",
   "metadata": {
    "id": "JigsVROrYPTv",
    "papermill": {
     "duration": 0.01419,
     "end_time": "2023-10-20T18:30:42.463612",
     "exception": false,
     "start_time": "2023-10-20T18:30:42.449422",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Fine tuning only the last 3 layers of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fcfd7a39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T18:30:42.494292Z",
     "iopub.status.busy": "2023-10-20T18:30:42.493896Z",
     "iopub.status.idle": "2023-10-20T18:30:42.500122Z",
     "shell.execute_reply": "2023-10-20T18:30:42.499230Z"
    },
    "id": "iwLrBwi-GWam",
    "papermill": {
     "duration": 0.022467,
     "end_time": "2023-10-20T18:30:42.502134",
     "exception": false,
     "start_time": "2023-10-20T18:30:42.479667",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_layers_to_freeze = 9\n",
    "for param in model.roberta.embeddings.parameters():\n",
    "    param.requires_grad = False\n",
    "for layer in model.roberta.encoder.layer[:num_layers_to_freeze]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a7b0fd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T18:30:42.530986Z",
     "iopub.status.busy": "2023-10-20T18:30:42.530692Z",
     "iopub.status.idle": "2023-10-20T18:30:47.976146Z",
     "shell.execute_reply": "2023-10-20T18:30:47.975215Z"
    },
    "id": "ScpO0TtC-Z-V",
    "outputId": "dc15296a-c9d0-4100-a05c-d677ca7030b8",
    "papermill": {
     "duration": 5.462773,
     "end_time": "2023-10-20T18:30:47.978144",
     "exception": false,
     "start_time": "2023-10-20T18:30:42.515371",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForQuestionAnswering(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if GPU is available and move the model accordingly\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5815ec5b",
   "metadata": {
    "id": "C5ztdFA1YUav",
    "papermill": {
     "duration": 0.012918,
     "end_time": "2023-10-20T18:30:48.004627",
     "exception": false,
     "start_time": "2023-10-20T18:30:47.991709",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d7e72ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T18:30:48.031655Z",
     "iopub.status.busy": "2023-10-20T18:30:48.031335Z",
     "iopub.status.idle": "2023-10-20T18:30:48.040454Z",
     "shell.execute_reply": "2023-10-20T18:30:48.039513Z"
    },
    "id": "_cnwNywP-dce",
    "outputId": "d2da7e25-a123-4d69-ffb3-335508d1e09c",
    "papermill": {
     "duration": 0.024921,
     "end_time": "2023-10-20T18:30:48.042403",
     "exception": false,
     "start_time": "2023-10-20T18:30:48.017482",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=0.0001)\n",
    "# Training loop\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf6b96a",
   "metadata": {
    "id": "cCMxvgZVYZFY",
    "papermill": {
     "duration": 0.013136,
     "end_time": "2023-10-20T18:30:48.068544",
     "exception": false,
     "start_time": "2023-10-20T18:30:48.055408",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db13dd6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T18:30:48.097376Z",
     "iopub.status.busy": "2023-10-20T18:30:48.097090Z",
     "iopub.status.idle": "2023-10-20T22:15:13.948900Z",
     "shell.execute_reply": "2023-10-20T22:15:13.947978Z"
    },
    "id": "Qxfu7pff-gXN",
    "outputId": "5b79d805-3671-4fa2-e44c-456b4fd46beb",
    "papermill": {
     "duration": 13465.869452,
     "end_time": "2023-10-20T22:15:13.951680",
     "exception": false,
     "start_time": "2023-10-20T18:30:48.082228",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:16<00:00,  9.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 3.4383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Avg Loss: 3.0022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Avg Loss: 2.7062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Avg Loss: 2.4936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Avg Loss: 2.2517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Avg Loss: 2.0617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Avg Loss: 1.9097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Avg Loss: 1.7577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Avg Loss: 1.6726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Avg Loss: 1.4255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 - Avg Loss: 1.1824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 - Avg Loss: 1.0629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 - Avg Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 - Avg Loss: 0.8650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 - Avg Loss: 0.8057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 - Avg Loss: 0.7628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 - Avg Loss: 0.7043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 - Avg Loss: 0.6661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 - Avg Loss: 0.6304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 - Avg Loss: 0.5955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 - Avg Loss: 0.5625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 - Avg Loss: 0.5554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 - Avg Loss: 0.5267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 - Avg Loss: 0.4931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 - Avg Loss: 0.5115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 - Avg Loss: 0.4578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 - Avg Loss: 0.4635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 - Avg Loss: 0.4405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 - Avg Loss: 0.4231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 - Avg Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 - Avg Loss: 0.3982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 - Avg Loss: 0.4161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 - Avg Loss: 0.3930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 - Avg Loss: 0.3790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 - Avg Loss: 0.3695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 - Avg Loss: 0.3486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37 - Avg Loss: 0.3866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 - Avg Loss: 0.3426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 - Avg Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 - Avg Loss: 0.3356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41 - Avg Loss: 0.3317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42 - Avg Loss: 0.3275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 - Avg Loss: 0.3453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44 - Avg Loss: 0.3174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45 - Avg Loss: 0.3126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46 - Avg Loss: 0.3102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47 - Avg Loss: 0.3164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48 - Avg Loss: 0.2965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 - Avg Loss: 0.3081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 - Avg Loss: 0.2842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51 - Avg Loss: 0.2936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52 - Avg Loss: 0.2937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53 - Avg Loss: 0.2806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54 - Avg Loss: 0.2756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55 - Avg Loss: 0.2777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56 - Avg Loss: 0.2650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57 - Avg Loss: 0.2779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58 - Avg Loss: 0.2768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59 - Avg Loss: 0.2572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60 - Avg Loss: 0.2685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61 - Avg Loss: 0.2695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62 - Avg Loss: 0.2614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63 - Avg Loss: 0.2624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64 - Avg Loss: 0.2526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65 - Avg Loss: 0.2542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66 - Avg Loss: 0.2390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67 - Avg Loss: 0.2581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68 - Avg Loss: 0.2558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69 - Avg Loss: 0.2565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70 - Avg Loss: 0.2253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71 - Avg Loss: 0.2368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72 - Avg Loss: 0.2596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73 - Avg Loss: 0.2328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74 - Avg Loss: 0.2433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75 - Avg Loss: 0.2309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76 - Avg Loss: 0.2334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 77: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77 - Avg Loss: 0.2276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 78: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78 - Avg Loss: 0.2202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 79: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79 - Avg Loss: 0.2347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 80: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80 - Avg Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 81: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81 - Avg Loss: 0.2413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 82: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82 - Avg Loss: 0.2082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 83: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83 - Avg Loss: 0.2238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 84: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84 - Avg Loss: 0.2087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 85: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85 - Avg Loss: 0.2236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 86: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86 - Avg Loss: 0.2228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 87: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87 - Avg Loss: 0.2307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 88: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88 - Avg Loss: 0.2067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 89: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89 - Avg Loss: 0.2122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 90: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90 - Avg Loss: 0.2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 91: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91 - Avg Loss: 0.2039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 92: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92 - Avg Loss: 0.2045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 93: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93 - Avg Loss: 0.2078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 94: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94 - Avg Loss: 0.2215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 95: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95 - Avg Loss: 0.2069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 96: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96 - Avg Loss: 0.1999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 97: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97 - Avg Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 98: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98 - Avg Loss: 0.1961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99 - Avg Loss: 0.2107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [02:14<00:00,  9.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100 - Avg Loss: 0.2155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f'Epoch {epoch + 1}', dynamic_ncols=True):\n",
    "        inputs = {key: value.to(device) for key, value in batch.items()}\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # Calculate and print the average loss for this epoch\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch + 1} - Avg Loss: {avg_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4ebaf8",
   "metadata": {
    "id": "3bhW5mqWYcZV",
    "papermill": {
     "duration": 9.873549,
     "end_time": "2023-10-20T22:15:33.803356",
     "exception": false,
     "start_time": "2023-10-20T22:15:23.929807",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b4e79ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T22:15:53.462472Z",
     "iopub.status.busy": "2023-10-20T22:15:53.461972Z",
     "iopub.status.idle": "2023-10-20T22:15:54.325249Z",
     "shell.execute_reply": "2023-10-20T22:15:54.324320Z"
    },
    "id": "-iz3t_u3SG0f",
    "outputId": "091372a7-3130-4f1d-def6-d3f1985a4229",
    "papermill": {
     "duration": 10.651459,
     "end_time": "2023-10-20T22:15:54.327374",
     "exception": false,
     "start_time": "2023-10-20T22:15:43.675915",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('local_fine_tuned_roberta_on_newsqa/tokenizer_config.json',\n",
       " 'local_fine_tuned_roberta_on_newsqa/special_tokens_map.json',\n",
       " 'local_fine_tuned_roberta_on_newsqa/vocab.json',\n",
       " 'local_fine_tuned_roberta_on_newsqa/merges.txt',\n",
       " 'local_fine_tuned_roberta_on_newsqa/added_tokens.json',\n",
       " 'local_fine_tuned_roberta_on_newsqa/tokenizer.json')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the fine-tuned model if needed\n",
    "model.save_pretrained('local_fine_tuned_roberta_on_newsqa')\n",
    "tokenizer.save_pretrained('local_fine_tuned_roberta_on_newsqa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e740da54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T22:16:14.183765Z",
     "iopub.status.busy": "2023-10-20T22:16:14.183420Z",
     "iopub.status.idle": "2023-10-20T22:16:15.647980Z",
     "shell.execute_reply": "2023-10-20T22:16:15.647201Z"
    },
    "id": "-Padf5ds-9fm",
    "outputId": "b6b58675-5766-432d-bfcc-3b8d679647f5",
    "papermill": {
     "duration": 11.375086,
     "end_time": "2023-10-20T22:16:15.650405",
     "exception": false,
     "start_time": "2023-10-20T22:16:04.275319",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    " # Initialize the tokenizer and model\n",
    "fine_tuned_tokenizer = AutoTokenizer.from_pretrained('local_fine_tuned_roberta_on_newsqa')\n",
    "fine_tuned_model = AutoModelForQuestionAnswering.from_pretrained('local_fine_tuned_roberta_on_newsqa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7a32b33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T22:16:35.612998Z",
     "iopub.status.busy": "2023-10-20T22:16:35.612270Z",
     "iopub.status.idle": "2023-10-20T22:16:35.748934Z",
     "shell.execute_reply": "2023-10-20T22:16:35.747950Z"
    },
    "id": "mbFcaKKNTUx8",
    "outputId": "2cc83057-9c85-454b-a750-0ddcd00daa1b",
    "papermill": {
     "duration": 10.362651,
     "end_time": "2023-10-20T22:16:35.750916",
     "exception": false,
     "start_time": "2023-10-20T22:16:25.388265",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForQuestionAnswering(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tuned_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4d859d",
   "metadata": {
    "id": "RbRz-K-SYgvd",
    "papermill": {
     "duration": 10.267044,
     "end_time": "2023-10-20T22:16:55.657342",
     "exception": false,
     "start_time": "2023-10-20T22:16:45.390298",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f921873",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T22:17:15.347407Z",
     "iopub.status.busy": "2023-10-20T22:17:15.346981Z",
     "iopub.status.idle": "2023-10-20T22:17:15.352437Z",
     "shell.execute_reply": "2023-10-20T22:17:15.351469Z"
    },
    "id": "nUMaTWVC_CfB",
    "papermill": {
     "duration": 10.025921,
     "end_time": "2023-10-20T22:17:15.354495",
     "exception": false,
     "start_time": "2023-10-20T22:17:05.328574",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform inference\n",
    "question = \"What war was the Iwo Jima battle a part of?\"\n",
    "context = \"One of the Marines shown in a famous World War II photograph raising the U.S. flag on Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\\n\\nThe Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\\n\\nSgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\\n\\nAt a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank\\'s younger sister, Mary Pero.\\n\\nStrank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\\n\\nStrank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\\n\\nJonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\\n\\nHe hailed Strank as a true American hero and a wonderful example of the remarkable contribution and sacrifices that immigrants have made to our great republic throughout its history.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f91f0c94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T22:17:35.195683Z",
     "iopub.status.busy": "2023-10-20T22:17:35.194852Z",
     "iopub.status.idle": "2023-10-20T22:17:35.222192Z",
     "shell.execute_reply": "2023-10-20T22:17:35.221238Z"
    },
    "id": "yb4mvE5C_FQT",
    "outputId": "27d6d6a3-490c-4291-8bc2-e21edfd2bf19",
    "papermill": {
     "duration": 10.12833,
     "end_time": "2023-10-20T22:17:35.224163",
     "exception": false,
     "start_time": "2023-10-20T22:17:25.095833",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What war was the Iwo Jima battle a part of?\n",
      "Answer:  II\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the passage and question\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "inputs.to(device)\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    outputs = fine_tuned_model(**inputs)\n",
    "    start_idx = torch.argmax(outputs[0])\n",
    "    end_idx = torch.argmax(outputs[1]) + 1\n",
    "\n",
    "# Get the answer text from the passage\n",
    "answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][start_idx:end_idx]))\n",
    "\n",
    "print(\"Question:\", question)\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a3b6a57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T22:17:55.019793Z",
     "iopub.status.busy": "2023-10-20T22:17:55.018894Z",
     "iopub.status.idle": "2023-10-20T22:17:55.032726Z",
     "shell.execute_reply": "2023-10-20T22:17:55.031860Z"
    },
    "id": "-I9jRNYnXA59",
    "papermill": {
     "duration": 10.135376,
     "end_time": "2023-10-20T22:17:55.034687",
     "exception": false,
     "start_time": "2023-10-20T22:17:44.899311",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_prediction(context, question):\n",
    "  inputs = tokenizer.encode_plus(question, context, return_tensors='pt').to(device)\n",
    "  outputs = model(**inputs)\n",
    "\n",
    "  answer_start = torch.argmax(outputs[0])\n",
    "  answer_end = torch.argmax(outputs[1]) + 1\n",
    "\n",
    "  answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][answer_start:answer_end]))\n",
    "\n",
    "  return answer\n",
    "\n",
    "def normalize_text(s):\n",
    "  \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\n",
    "  import string, re\n",
    "  def remove_articles(text):\n",
    "    regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "    return re.sub(regex, \" \", text)\n",
    "  def white_space_fix(text):\n",
    "    return \" \".join(text.split())\n",
    "  def remove_punc(text):\n",
    "    exclude = set(string.punctuation)\n",
    "    return \"\".join(ch for ch in text if ch not in exclude)\n",
    "  def lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "  return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def exact_match(prediction, truth):\n",
    "    return bool(normalize_text(prediction) == normalize_text(truth))\n",
    "\n",
    "def compute_f1(prediction, truth):\n",
    "  pred_tokens = normalize_text(prediction).split()\n",
    "  truth_tokens = normalize_text(truth).split()\n",
    "\n",
    "  # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n",
    "  if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
    "    return int(pred_tokens == truth_tokens)\n",
    "\n",
    "  common_tokens = set(pred_tokens) & set(truth_tokens)\n",
    "\n",
    "  # if there are no common tokens then f1 = 0\n",
    "  if len(common_tokens) == 0:\n",
    "    return 0\n",
    "\n",
    "  prec = len(common_tokens) / len(pred_tokens)\n",
    "  rec = len(common_tokens) / len(truth_tokens)\n",
    "\n",
    "  return round(2 * (prec * rec) / (prec + rec), 2)\n",
    "\n",
    "def question_answer(context, question,answer):\n",
    "  prediction = get_prediction(context,question)\n",
    "  em_score = exact_match(prediction, answer)\n",
    "  f1_score = compute_f1(prediction, answer)\n",
    "\n",
    "  print(f'Question: {question}')\n",
    "  print(f'Prediction: {prediction}')\n",
    "  print(f'True Answer: {answer}')\n",
    "  print(f'Exact match: {em_score}')\n",
    "  print(f'F1 score: {f1_score}\\n')\n",
    "    \n",
    "  return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "93d074a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T22:18:14.819626Z",
     "iopub.status.busy": "2023-10-20T22:18:14.818883Z",
     "iopub.status.idle": "2023-10-20T22:18:32.831374Z",
     "shell.execute_reply": "2023-10-20T22:18:32.830179Z"
    },
    "papermill": {
     "duration": 28.118304,
     "end_time": "2023-10-20T22:18:32.833447",
     "exception": false,
     "start_time": "2023-10-20T22:18:04.715143",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What will be nominated?\n",
      "Prediction:  Strank\n",
      "True Answer: three different videos\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does the Harrison Ford video feature?\n",
      "Prediction: \n",
      "True Answer: getting his chest waxed,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What videos will you send?\n",
      "Prediction:  videos\n",
      "True Answer: environmental\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is Ford getting waxed?\n",
      "Prediction: \n",
      "True Answer: his chest\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who got his chest waxed?\n",
      "Prediction: One of the Marines\n",
      "True Answer: Harrison Ford\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How do you send in your video?\n",
      "Prediction:  World War II photograph raising the U.S. flag\n",
      "True Answer: Use the iReport form\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What type of videos should you nominate?\n",
      "Prediction:  videos\n",
      "True Answer: think are the best.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did Steve Bruce describe Amire Zaki as?\n",
      "Prediction:  true American hero and\n",
      "True Answer: unprofessional.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Which team does Zaki play for?\n",
      "Prediction: \n",
      "True Answer: Wigan Athletic\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Which club did Amir Zaki fail to return to?\n",
      "Prediction:  citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "\n",
      "He hailed Strank as a true American hero and a wonderful example of the remarkable contribution and sacrifices that immigrants have made to our great republic throughout its history.\n",
      "True Answer: Wigan Athletic\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What punishment will be meted out for his disappearance?\n",
      "Prediction: \n",
      "True Answer: a fine\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who went missing for two weeks and said he was taking a break from football?\n",
      "Prediction:  Michael Strank,\n",
      "True Answer: Adriano\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who said there is no immediate plans for deployment?\n",
      "Prediction:  U.S. Citizenship and Immigration Services\n",
      "True Answer: President Obama\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many troops does Canada have in Afghanistan?\n",
      "Prediction:  five\n",
      "True Answer: 35,000.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many more troops is the US planning to send?\n",
      "Prediction:  planning to send?\n",
      "True Answer: 6,000\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What are the plans of Obama after this deployment?\n",
      "Prediction: \n",
      "True Answer: to commit more U.S. troops to the ongoing war in Afghanistan,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many troops are being send to Afghanistan this year?\n",
      "Prediction:  U.S. Citizenship and Immigration Services\n",
      "True Answer: 6,000\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Women who don't conform will risk spending how long in jail?\n",
      "Prediction:  Michael Strank,\n",
      "True Answer: 12 hours\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What are the rules of the new order?\n",
      "Prediction:  never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "True Answer: to close their shops during daily prayers,\n",
      "Exact match: False\n",
      "F1 score: 0.06\n",
      "\n",
      "Question: What controls Baidoa?\n",
      "Prediction:  Japanese and U.S.\n",
      "True Answer: Al-Shabaab,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What can happen to the women who don't obey the order?\n",
      "Prediction:  happen\n",
      "True Answer: face jail time,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Women's clothing must cover what?\n",
      "Prediction:  U.S. flag\n",
      "True Answer: their bodies and heads\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When is the order in effect?\n",
      "Prediction:  March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "\n",
      "He hailed Strank as a true American hero and a wonderful example of the remarkable contribution and sacrifices that immigrants have made to our great republic throughout its history.\n",
      "True Answer: Tuesday.\n",
      "Exact match: False\n",
      "F1 score: 0.04\n",
      "\n",
      "Question: What must the clothing cover?\n",
      "Prediction:  the flag-raising\n",
      "True Answer: their bodies and heads from view,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where was Forrest killed?\n",
      "Prediction:  in action on the island\n",
      "True Answer: in southwest Atlanta, Georgia,\n",
      "Exact match: False\n",
      "F1 score: 0.25\n",
      "\n",
      "Question: What three men were accused in the death?\n",
      "Prediction: \n",
      "True Answer: and Jquante Crews,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Which boxing champion was killed?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Vernon Forrest,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What was the age of Vernon Forrest at the time of his death?\n",
      "Prediction:  3,\n",
      "True Answer: 38,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What reward was offered?\n",
      "Prediction:  certificate of U.S. citizenship\n",
      "True Answer: $17,000\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many men were accused of murder?\n",
      "Prediction:  five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five\n",
      "True Answer: three\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What boxing champion?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Vernon Forrest,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What day in July was Vernon Forrest killed?\n",
      "Prediction:  March 1,\n",
      "True Answer: 25.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What charges included  murder?\n",
      "Prediction:  U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "True Answer: aggravated assault with a deadly weapon and possession of a firearm by a convicted felon,\n",
      "Exact match: False\n",
      "F1 score: 0.04\n",
      "\n",
      "Question: What football star cleared of charge?\n",
      "Prediction: \n",
      "True Answer: Steven Gerrard\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What was Gerrard's rationale?\n",
      "Prediction:  a true American hero\n",
      "True Answer: he believed he was about to be attacked himself.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did Gerrard admitted?\n",
      "Prediction:  a certificate of U.S. citizenship\n",
      "True Answer: throwing three punches\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What player has cleared waivers?\n",
      "Prediction:  Michael Strank,\n",
      "True Answer: Steven Gerrard\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What was the verdict of the jury?\n",
      "Prediction:  awarded a certificate of U.S. citizenship on Tuesday.\n",
      "True Answer: not guilty\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What was the reason for the punishment?\n",
      "Prediction:  the battle between Japanese and U.S. forces\n",
      "True Answer: threatening behavior.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Iran criticizes who?\n",
      "Prediction:  of the Marines\n",
      "True Answer: U.S. President-elect Barack Obama\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What are US and Iran relations tensioned about?\n",
      "Prediction:  citizenship papers.\n",
      "True Answer: nuclear program.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who said Obama should apply campaign message?\n",
      "Prediction: Jonathan Scharfen, the acting director of CIS,\n",
      "True Answer: Ali Larijani\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What should Obama apply according to speaker?\n",
      "Prediction:  citizenship papers.\n",
      "True Answer: his campaign message of change\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: US - Iran tensions are high over what?\n",
      "Prediction:  World War II photograph raising the U.S. flag\n",
      "True Answer: nuclear program.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who was the President of the US at this time?\n",
      "Prediction: Jonathan Scharfen,\n",
      "True Answer: Barack Obama\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who has tensions over Tehran's nuclear ambitions?\n",
      "Prediction:  Czechoslovakia\n",
      "True Answer: U.S.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who criticized Obama ?\n",
      "Prediction: \n",
      "True Answer: Iran's parliament speaker\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Parliamentary speaker says who should apply campaign message of change?\n",
      "Prediction:  the Marines shown in a famous World War II photograph raising the U.S. flag on Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services\n",
      "True Answer: U.S. President-elect Barack Obama\n",
      "Exact match: False\n",
      "F1 score: 0.03\n",
      "\n",
      "Question: What kind of weapons are being discussed?\n",
      "Prediction: \n",
      "True Answer: nuclear\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who criticized Obama for saying nuclear weapon development is unaccaptable?\n",
      "Prediction: Jonathan Scharf\n",
      "True Answer: Iran's parliament speaker\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who spent nine years in prison?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Tim Masters,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who seeks a dismissal of Tim Masters murder case?\n",
      "Prediction:  U.S. Citizenship and Immigration Services\n",
      "True Answer: Colorado prosecutor\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What was Masters convicted of?\n",
      "Prediction:  father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S.\n",
      "True Answer: first-degree murder charge\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When was Masters released following the toss of his conviction?\n",
      "Prediction:  March 1, 1945,\n",
      "True Answer: Tuesday\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who seeks dismissal?\n",
      "Prediction: \n",
      "True Answer: Colorado prosecutor\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who was released on Tuesday?\n",
      "Prediction: One of the Marines\n",
      "True Answer: Tim Masters,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What was released?\n",
      "Prediction:  a famous World War II photograph\n",
      "True Answer: new DNA evidence\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What was Masters convicted of in 1999?\n",
      "Prediction:  citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island\n",
      "True Answer: first-degree murder\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What happened to the U.N. compound?\n",
      "Prediction:  awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank,\n",
      "True Answer: hit and set on fire,\n",
      "Exact match: False\n",
      "F1 score: 0.13\n",
      "\n",
      "Question: What does the lawmaker say?\n",
      "Prediction: \n",
      "True Answer: Israeli military action in Gaza is comparable to that of German soldiers during\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who are Israel being asked to talk to\n",
      "Prediction:  Marines\n",
      "True Answer: Hamas,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What has the UK PM called indefensible\n",
      "Prediction:  World War II photograph raising the U.S. flag on Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank\n",
      "True Answer: the shelling of the compound\n",
      "Exact match: False\n",
      "F1 score: 0.03\n",
      "\n",
      "Question: What type of choice has Hamas made\n",
      "Prediction:  U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "\n",
      "He hailed Strank as a true American hero and a wonderful example of the remarkable contribution and sacrifices that immigrants\n",
      "True Answer: step up attacks against innocent civilians.\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: The second shot hit what?\n",
      "Prediction:  Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island\n",
      "True Answer: struck Grant in the upper right arm,\n",
      "Exact match: False\n",
      "F1 score: 0.11\n",
      "\n",
      "Question: The man was rescued from what in northern Australia?\n",
      "Prediction:  Iwo Jima\n",
      "True Answer: the jaws of a crocodile\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: The men were collecting what on the river bank in the Northern Territory?\n",
      "Prediction:  a flag\n",
      "True Answer: crocodile eggs\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: where Man rescues co-worker?\n",
      "Prediction:  Michael Strank,\n",
      "True Answer: northern Australia\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did Giuliana Rancic do?\n",
      "Prediction:  killed in action on the island\n",
      "True Answer: undergoing a double mastectomy and reconstructive surgery,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What surgery did Rancic have?\n",
      "Prediction:  II photograph\n",
      "True Answer: double mastectomy and reconstructive\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: She says it feels great to be what?\n",
      "Prediction:  Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "\n",
      "He hailed Strank as a true American hero\n",
      "True Answer: back at work,\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Giuliana Rancic was back on the set where?\n",
      "Prediction: \n",
      "True Answer: \"E! News\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who was back on the set at E!?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Rancic\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where was Giuliana Rancic?\n",
      "Prediction:  Arlington, Virginia,\n",
      "True Answer: \"E! News\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did Rancic say about it?\n",
      "Prediction:  a true American hero and a wonderful example of the remarkable contribution and sacrifices that immigrants have made to our great republic throughout its history.\n",
      "True Answer: was a wonderful homecoming,\"\n",
      "Exact match: False\n",
      "F1 score: 0.08\n",
      "\n",
      "Question: Rancic, 37, had the surgery after lumpectomies failed to eradicate her what?\n",
      "Prediction: \n",
      "True Answer: breast cancer.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did she say?\n",
      "Prediction:  a true American hero and\n",
      "True Answer: \"Even though I moved a tad slower than usual today, everyone welcomed me back with open arms and it was a wonderful homecoming,\"\n",
      "Exact match: False\n",
      "F1 score: 0.08\n",
      "\n",
      "Question: Who was greeted in Seoul?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: the announcement\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What happened in 1994?\n",
      "Prediction:  father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men\n",
      "True Answer: Kim Il Sung died\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What was the reaction of South Korean military?\n",
      "Prediction:  the citizenship certificate\n",
      "True Answer: raising its alert level,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who died in 1994?\n",
      "Prediction: Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank\n",
      "True Answer: Kim Il Sung\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When did Kim II Sung die?\n",
      "Prediction:  March 1, 1945,\n",
      "True Answer: 1994\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who says most people in the south are calm about the situation?\n",
      "Prediction: Jonathan Scharfen,\n",
      "True Answer: Woosuk Ken Choi,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What level was raised?\n",
      "Prediction: \n",
      "True Answer: alert\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What was the reaction in Seoul?\n",
      "Prediction:  in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "True Answer: astonishment\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: There was general astonishment where?\n",
      "Prediction:  Arlington, Virginia,\n",
      "True Answer: Seoul,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did cast member A.J. Jewell's death cause?\n",
      "Prediction:  in action\n",
      "True Answer: of \"The Real Housewives of Atlanta\" reunion special,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is the former fiance of Kandi?\n",
      "Prediction:  Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "True Answer: Ashley \"A.J.\" Jewell,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who was the former fiance of Kandi Burruss?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Ashley \"A.J.\" Jewell,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What show was scheduled to tape its reunion special recently?\n",
      "Prediction: \n",
      "True Answer: \"The Real Housewives of Atlanta\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Whose death caused the postponement of taping?\n",
      "Prediction:  Michael Strank,\n",
      "True Answer: Kandi Burruss' former fianc√©, Ashley \"A.J.\" Jewell,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When was the reunion scheduled for?\n",
      "Prediction:  Tuesday\n",
      "True Answer: last week,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When was the \"Real Housewives of Atlanta\" scheduled to tape their reunion special?\n",
      "Prediction:  February 23, 1945.\n",
      "True Answer: last week,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where did the deadly earthquake happen?\n",
      "Prediction:  Arlington, Virginia,\n",
      "True Answer: Haiti.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where did he lead the effort?\n",
      "Prediction:  Arlington, Virginia,\n",
      "True Answer: in Haiti.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Is the cause of ibs known?\n",
      "Prediction:  recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "True Answer: remains unknown,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What percent of North Americans have ibs?\n",
      "Prediction:  North Americans have ibs?</s></s>One of the Marines shown in a famous World War II photograph raising the U.S. flag on Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt.\n",
      "True Answer: 10 to 15\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does the study analysis say works?\n",
      "Prediction:  U.S. Citizenship and Immigration Services\n",
      "True Answer: Peppermint oil, soluble fiber, and antispasmodic drugs can indeed help people with irritable bowel syndrome,\n",
      "Exact match: False\n",
      "F1 score: 0.1\n",
      "\n",
      "Question: Approximately how many people in North America have IBS?\n",
      "Prediction:  five others\n",
      "True Answer: 10 to 15 percent\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What treatments work for ibs?\n",
      "Prediction: \n",
      "True Answer: including fiber supplements, probiotics, antidepressants, behavioral-based therapies, psychotherapy, food modification, acupuncture, and laxatives.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who  has filed suit with international court?\n",
      "Prediction:  Strank's younger sister, Mary Pero.\n",
      "True Answer: Australian officials\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does the moratorium allow hunting whales for?\n",
      "Prediction:  World War II photograph raising the U.S. flag\n",
      "True Answer: scientific reasons.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who says decision to head to court \"regrettable\"?\n",
      "Prediction:  U.S. Citizenship and Immigration Services\n",
      "True Answer: Japanese Foreign Ministry spokesman Hidenobu Sobashima\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What has Austrailia filed  suit over?\n",
      "Prediction: \n",
      "True Answer: to stop Japan from exploiting the research loophole.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who allows hunting whales for scientific reasons?\n",
      "Prediction:  his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services\n",
      "True Answer: Japanese officials\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what resumes TNT?\n",
      "Prediction:  Michael Strank,\n",
      "True Answer: \"The Closer.\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what is a sag award\n",
      "Prediction:  a certificate of U.S. citizenship\n",
      "True Answer: Screen Actors Guild\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When does TNT resume the series?\n",
      "Prediction: \n",
      "True Answer: Monday night\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who did Superman battle in \"Clan of the Fiery Cross\"?\n",
      "Prediction: \n",
      "True Answer: Ku Klux\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What have affected people in real life?\n",
      "Prediction:  planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "\n",
      "He hailed Strank as a true American hero and a wonderful example of the remarkable contribution and sacrifices that immigrants\n",
      "True Answer: comic book characters\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What group did Superman battle in \"Clan of the Fiery Cross\"?\n",
      "Prediction:  Japanese and U.S. forces\n",
      "True Answer: the Ku Klux Klan,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who did Superman battle in the radio series?\n",
      "Prediction: \n",
      "True Answer: Ku Klux Klan,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who blocked a scientist from getting a patent?\n",
      "Prediction:  Strank\n",
      "True Answer: Donald Duck\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What cartoon character blocked a scientist from getting a patent?\n",
      "Prediction: Sgt.\n",
      "True Answer: Donald Duck\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where was the teenage boy shot?\n",
      "Prediction:  on the island\n",
      "True Answer: Athens,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many civilians were injured?\n",
      "Prediction: \n",
      "True Answer: 34\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What caused protests to explode?\n",
      "Prediction: \n",
      "True Answer: killing of a 15-year-old boy\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Authorities vow to do what in regards to the rioting?\n",
      "Prediction:  to do what in regards to the rioting?</s></s>One of the Marines shown in a famous World War II photograph raising the U.S. flag on Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935.\n",
      "True Answer: re-impose order\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where is the rioting happening?\n",
      "Prediction:  Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial\n",
      "True Answer: across Greece\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Number of civilians injure during riots?\n",
      "Prediction: \n",
      "True Answer: 34\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many civilians were injured in the rioting?\n",
      "Prediction:  civilians\n",
      "True Answer: 34\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where was the teenager shot at?\n",
      "Prediction:  on the island\n",
      "True Answer: Athens,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is rioting?\n",
      "Prediction:  Michael Strank\n",
      "True Answer: young self-styled anarchists\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What do authorities vow to re-impose?\n",
      "Prediction:  the flag-raising\n",
      "True Answer: order\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is suing the ICE?\n",
      "Prediction:  ICE?</s></s>One of the Marines shown in a famous World War II photograph raising the U.S. flag on Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank,\n",
      "True Answer: Raymond Soeoth of Indonesia and Amadou Diouf of Senegal in West Africa,\n",
      "Exact match: False\n",
      "F1 score: 0.11\n",
      "\n",
      "Question: WHAT HAVE 1073 DETAINEES HAD SINCE 2003?\n",
      "Prediction:  citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "True Answer: \"medical escorts\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Which senator vows to investigate the allegations?\n",
      "Prediction:  Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS,\n",
      "True Answer: Sen. Joe Lieberman,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who did the detainees sue\n",
      "Prediction:  Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "True Answer: the government.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What number of detainees had medical escorts since 2003?\n",
      "Prediction:  of detainees\n",
      "True Answer: 1,073\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did they tell CNN?\n",
      "Prediction: \n",
      "True Answer: were injected with the drugs against their will.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is ICE alleged to have done to detainees?\n",
      "Prediction: \n",
      "True Answer: forcibly injecting them with psychotropic drugs\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What news station interviewed the detainee\n",
      "Prediction:  Associated Press\n",
      "True Answer: CNN\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who did the detainees file a suit against?\n",
      "Prediction: \n",
      "True Answer: Immigration and Customs Enforcement\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what is the senator doing\n",
      "Prediction: \n",
      "True Answer: intends to follow up with ICE\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what were detainees allegedly injected with?\n",
      "Prediction:  an American flag\n",
      "True Answer: psychotropic drugs\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who is lieberman?\n",
      "Prediction:  the Marines\n",
      "True Answer: \"Senator\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who had medical escorts?\n",
      "Prediction:  Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "True Answer: 1,073 immigration detainees\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is Sen. Lieberman vowing?\n",
      "Prediction: \n",
      "True Answer: detainees are not drugged unless there\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What number of detainees have had \"medical escorts\" since 2003?\n",
      "Prediction:  five\n",
      "True Answer: 1,073\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the ICE suit about?\n",
      "Prediction:  U.S. Citizenship and Immigration Services\n",
      "True Answer: forcibly injecting them with psychotropic drugs while trying to shuttle them out of the country during their deportation.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who files suit?\n",
      "Prediction:  Strank's younger sister, Mary Pero.\n",
      "True Answer: American Civil Liberties Union\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what did he say\n",
      "Prediction: \n",
      "True Answer: was injected with drugs by ICE agents against his will.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What dog breed is described as \"active athletes\"?\n",
      "Prediction:  dog\n",
      "True Answer: Portuguese water\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What are Portugese water dogs like?\n",
      "Prediction:  water\n",
      "True Answer: are \"active athletes,\" far from couch potatoes,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is more likely to rip up the couch, than lounge on it?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Portuguese water dogs\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What issue is the breeders concerned about?\n",
      "Prediction:  his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "True Answer: a thorough understanding of the dogs' needs,\n",
      "Exact match: False\n",
      "F1 score: 0.04\n",
      "\n",
      "Question: Who is Bo the dog?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: gift to the Obama girls from Sen. Ted Kennedy.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is his name?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Jeffrey Jamaleldine\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where did he go to college?\n",
      "Prediction:  Czechoslovakia\n",
      "True Answer: in Missouri\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Soldier was one of more than 20,000 \"green-card warriors\"\n",
      "Prediction: \n",
      "True Answer: Jamaleldine\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What do people still believe?\n",
      "Prediction:  was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "True Answer: \"You can go from rags to riches\n",
      "Exact match: False\n",
      "F1 score: 0.03\n",
      "\n",
      "Question: What type of soldier is this?\n",
      "Prediction:  American\n",
      "True Answer: U.S. Army scout\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does his dad wonder?\n",
      "Prediction:  never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945,\n",
      "True Answer: Why he's more American than a German,\n",
      "Exact match: False\n",
      "F1 score: 0.03\n",
      "\n",
      "Question: what's the cyclist's Olympic record?\n",
      "Prediction: \n",
      "True Answer: fifth\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who was a three-time road race world champion by 1988?\n",
      "Prediction:  Michael Strank,\n",
      "True Answer: Longo-Ciprelli\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where did they win medals?\n",
      "Prediction:  the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia,\n",
      "True Answer: Atlanta,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who won the record\n",
      "Prediction:  Strank\n",
      "True Answer: Longo-Ciprelli\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is the president?\n",
      "Prediction: Jonathan Scharfen\n",
      "True Answer: Bush\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Will there be any restrictions on funding the wars\n",
      "Prediction:  derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "True Answer: without the\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What victory did bush get\n",
      "Prediction:  national icons\n",
      "True Answer: the bill\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the benefit for veterans?\n",
      "Prediction:  U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "True Answer: education\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What else does the bill contain?\n",
      "Prediction:  citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising\n",
      "True Answer: that expands education benefits for veterans who have served since the 9/11 attacks, provides a 13-week extension of unemployment benefits and more than $2 billion in disaster assistance for parts of the Midwest that have been hit by record floods.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What causes a victory for President Bush?\n",
      "Prediction: Jonathan Scharfen,\n",
      "True Answer: the bill\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what does legislation fund\n",
      "Prediction:  certificate of U.S. citizenship\n",
      "True Answer: wars in Iraq and Afghanistan\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what does bill contains\n",
      "Prediction:  the flag-raising\n",
      "True Answer: nearly $162 billion in war funding\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what did bush do\n",
      "Prediction: \n",
      "True Answer: signed a bill that will pay for the wars in Iraq and Afghanistan\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the budget for the spending bill?\n",
      "Prediction:  U.S. citizenship when his father was naturalized in 1935.\n",
      "True Answer: $162 billion\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did he sign?\n",
      "Prediction:  U.S. citizenship\n",
      "True Answer: a bill that will pay for the wars in Iraq and Afghanistan\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Can non European players be in the squad?\n",
      "Prediction: \n",
      "True Answer: frees up a place for another non-European Union\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is granted dual nationality?\n",
      "Prediction: \n",
      "True Answer: Ronaldinho\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does this move mean for the squad?\n",
      "Prediction: \n",
      "True Answer: frees up a place\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where was he given dual nationality?\n",
      "Prediction:  Arlington, Virginia,\n",
      "True Answer: Spain\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What has Robaldinho been granted by Spain?\n",
      "Prediction:  a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "True Answer: dual nationality\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who stars in \"The Da Vinci Code\"?\n",
      "Prediction: One of the Marines shown in a famous World War II photograph raising the U.S. flag on Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank,\n",
      "True Answer: Ewan McGregor\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the name of the other actor?\n",
      "Prediction: Jonathan Scharfen, the acting director of CIS,\n",
      "True Answer: Ayelet Zurer\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the name of the lead actor in the movie?\n",
      "Prediction: Jonathan Scharfen,\n",
      "True Answer: Tom Hanks\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: The president of which body said  \"It's all a lie\"?\n",
      "Prediction: \n",
      "True Answer: the Catholic League.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Which actor starred in both movies?\n",
      "Prediction: Sgt.\n",
      "True Answer: Tom Hanks\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is Tom's famous cast mate?\n",
      "Prediction: \n",
      "True Answer: Ewan McGregor\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What was siad by the Catholic League president?\n",
      "Prediction:  a certificate of U.S. citizenship\n",
      "True Answer: \"I have a strong objection to the genre of mixing fact with fiction,\"\n",
      "Exact match: False\n",
      "F1 score: 0.13\n",
      "\n",
      "Question: Angels & Demons was the sequel to which other film?\n",
      "Prediction: Angels & Demons\n",
      "True Answer: \"The Da Vinci Code\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What HBO show was he on?\n",
      "Prediction:  HBO\n",
      "True Answer: \"The Sopranos,\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who did he meet with to discuss the issue?\n",
      "Prediction:  Strank's younger sister, Mary Pero.\n",
      "True Answer: Obama and McCain camps\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who did he meet with?\n",
      "Prediction:  Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "\n",
      "He hailed Strank as a true American hero and a wonderful example of the remarkable contribution and sacrifices that immigrants have made to our great republic throughout its history.\n",
      "True Answer: Obama and McCain camps\n",
      "Exact match: False\n",
      "F1 score: 0.02\n",
      "\n",
      "Question: What does the adovocacy group promote?\n",
      "Prediction: \n",
      "True Answer: mental health and recovery.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is the co-founder of the advocacy group No Kidding, Me Too?\n",
      "Prediction: Jonathan Scharfen,\n",
      "True Answer: Joe Pantoliano\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did the actor act in before?\n",
      "Prediction:  famous World War II photograph\n",
      "True Answer: \"The Sopranos,\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who can Dublin rival?\n",
      "Prediction: \n",
      "True Answer: Silicon Valley.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where are headquartered Google and Facebook?\n",
      "Prediction:  Arlington, Virginia,\n",
      "True Answer: Dublin.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What companies have their headquaters in Ireland?\n",
      "Prediction:  headquaters in Ireland?</s></s>One of the Marines shown in a famous World War II photograph raising the U.S. flag on Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS\n",
      "True Answer: Facebook and Google,\n",
      "Exact match: False\n",
      "F1 score: 0.01\n",
      "\n",
      "Question: What provides Dogpatch Labs Europe?\n",
      "Prediction:  Dogpatch Labs Europe?</s></s>One of the Marines shown in a famous World War II photograph raising the U.S. flag on Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank,\n",
      "True Answer: a space for aspiring entrepreneurs to brainstorm with like-minded people.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who already has headquarters in Ireland?\n",
      "Prediction:  Michael Strank,\n",
      "True Answer: Facebook and Google,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who provides space for entrepreneurs?\n",
      "Prediction:  his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services\n",
      "True Answer: Dogpatch Labs\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did Twitter announce?\n",
      "Prediction:  was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935.\n",
      "True Answer: its intention to set up headquarters in Dublin.\n",
      "Exact match: False\n",
      "F1 score: 0.07\n",
      "\n",
      "Question: Which river has plunged to record low levels?\n",
      "Prediction: Which river\n",
      "True Answer: Tigris\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does Iraq want Turkey to do?\n",
      "Prediction: \n",
      "True Answer: to increase the flow of water passing through its network of dams.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What river has plunged to record lows?\n",
      "Prediction:  Mount Suribachi\n",
      "True Answer: Tigris\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What happened to the Tigris River?\n",
      "Prediction: \n",
      "True Answer: has plunged to record low levels,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What country does Iraq and Syria want to increase its water flow?\n",
      "Prediction:  Czechoslovakia\n",
      "True Answer: Turkey,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did Turkey do?\n",
      "Prediction: \n",
      "True Answer: provided Syria and Iraq 500 cubic meters of water a second,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What have they withdrawn from each others' capitals?\n",
      "Prediction:  each others' capitals?\n",
      "True Answer: ambassadors\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where is the rivers' source located?\n",
      "Prediction:  Arlington, Virginia,\n",
      "True Answer: Turkey,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Is pre-marital sex legal in Saudi Arabia?\n",
      "Prediction:  pre-marital sex\n",
      "True Answer: is illegal\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did Jawad talk about?\n",
      "Prediction:  famous World War II photograph\n",
      "True Answer: foreplay, sexual conquests and how he picks up women,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: For what reason did Mazen Abdul Jawad apologize?\n",
      "Prediction:  that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "True Answer: his comments while Saudi authorities discuss whether he should be charged with a crime,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What are Saudi authorities debating?\n",
      "Prediction:  U.S. flag\n",
      "True Answer: whether he should be charged with a crime,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did Mazen Abdul apologize for?\n",
      "Prediction: \n",
      "True Answer: bragging about his sex life on television\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What show was Jawad on?\n",
      "Prediction:  World War II photograph\n",
      "True Answer: \"Red Lines,\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: North Korea recently threatened to \"wipe out\" what country  if provoked?\n",
      "Prediction: North Korea\n",
      "True Answer: United States\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who does Japanese media reporte North Korea may fire a missile at?\n",
      "Prediction: \n",
      "True Answer: Hawaii.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who did North Korea threaten to \"wipe out\" if provoked?\n",
      "Prediction:  forces\n",
      "True Answer: the United States\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: U.S. does not believe who intends to launch long-range missile soon?\n",
      "Prediction:  Japanese and U.S.\n",
      "True Answer: North Korea\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who was warned to be clear due to \"military firing exercise\"?\n",
      "Prediction:  the Marines\n",
      "True Answer: mariners\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does US believe?\n",
      "Prediction:  a true American hero and\n",
      "True Answer: North Korea intends to launch a long-range missile in the near future,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did North Korea recently threaten?\n",
      "Prediction: \n",
      "True Answer: \"wipe out\" the United States if provoked.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What may North Korea due to Hawaii on July 4?\n",
      "Prediction:  citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "\n",
      "He hailed Strank as a true American hero and a wonderful example of the remarkable contribution and sacrifices that immigrants have made to our great republic throughout its history.\n",
      "True Answer: fire a missile toward\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When did the japanese media report?\n",
      "Prediction:  Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "True Answer: July 4.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who refuses to broadcast ad?\n",
      "Prediction:  U.S. Citizenship and Immigration Services\n",
      "True Answer: The BBC\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is BBC funded by?\n",
      "Prediction:  BBC\n",
      "True Answer: an obligatory license fee paid\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the name of the charity group?\n",
      "Prediction:  U.S. Citizenship and Immigration Services\n",
      "True Answer: British Red Cross, Oxfam, Save the Children and 10 other charities,\n",
      "Exact match: False\n",
      "F1 score: 0.13\n",
      "\n",
      "Question: What do the protesters occupy?\n",
      "Prediction:  U.S. flag on Iwo Jima\n",
      "True Answer: Glasgow office\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where did protestors occupy?\n",
      "Prediction:  Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island\n",
      "True Answer: the foyer of the BBC building in Glasgow, Scotland\n",
      "Exact match: False\n",
      "F1 score: 0.07\n",
      "\n",
      "Question: what is the ad about?\n",
      "Prediction:  World War II photograph raising the U.S. flag on Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising\n",
      "True Answer: aid to Gaza,\n",
      "Exact match: False\n",
      "F1 score: 0.02\n",
      "\n",
      "Question: on which date disasters emergency committee will launch appeal?\n",
      "Prediction:  Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "True Answer: Monday.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Carter's sentencing was postponed so he could get what?\n",
      "Prediction:  certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship\n",
      "True Answer: some dental work done,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Rapper Dwayne Carter will be sentenced for what kind of conviction?\n",
      "Prediction: \n",
      "True Answer: gun\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does the work?\n",
      "Prediction: \n",
      "True Answer: removal of his diamond-studded braces.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who will be condemned?\n",
      "Prediction: \n",
      "True Answer: Rapper Lil Wayne\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Work includes removal of diamond-encrusted what?\n",
      "Prediction: \n",
      "True Answer: braces.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Which gene did the ALS association discover?\n",
      "Prediction: \n",
      "True Answer: ALS6,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many people does Lou Gehrig's disease effect?\n",
      "Prediction: \n",
      "True Answer: 5,600\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What number of people get ALS each year?\n",
      "Prediction: Sgt.\n",
      "True Answer: 5,600\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does the ALS call the gene discovery?\n",
      "Prediction:  U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services\n",
      "True Answer: \"momentous discovery\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many people has ALS or Lou Gehrig's disease?\n",
      "Prediction: \n",
      "True Answer: 5,600\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How much was seized?\n",
      "Prediction: \n",
      "True Answer: of methamphetamine and $7.8 million in cash\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How long was the investigation?\n",
      "Prediction:  less than a month\n",
      "True Answer: 15-month\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where there any other drugs recovered?\n",
      "Prediction:  citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial\n",
      "True Answer: 123 pounds of cocaine and 4.5 pounds of heroin,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What else did the authorities recover?\n",
      "Prediction:  citizenship papers.\n",
      "True Answer: 123 pounds of cocaine and 4.5 pounds of heroin,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How much cash did the authorities seize?\n",
      "Prediction:  a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less\n",
      "True Answer: $7.8 million\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many pounds of marijuana?\n",
      "Prediction:  of marijuana?</s></s>One of the Marines shown in a famous World War II photograph raising the U.S. flag on Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt.\n",
      "True Answer: 650\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What was the investigation dubbed?\n",
      "Prediction:  U.S. Citizenship and Immigration Services\n",
      "True Answer: \"Operation Crank Call,\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where was the overcrowded ferry?\n",
      "Prediction:  Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia,\n",
      "True Answer: Bangladesh,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where were the people traveling to?\n",
      "Prediction:  United States\n",
      "True Answer: Bhola\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many died in ferry capsize?\n",
      "Prediction:  five\n",
      "True Answer: 28\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where did the ferry depart from?\n",
      "Prediction:  Czechoslovakia\n",
      "True Answer: Dhaka,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: By how many was the boat overcrowded?\n",
      "Prediction:  five others\n",
      "True Answer: 2,000 people,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many people were on board the ferry?\n",
      "Prediction:  five others\n",
      "True Answer: 2,000\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What was conveyed?\n",
      "Prediction: \n",
      "True Answer: our sincerity\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did the president of Toyota say he takes full responsibility for?\n",
      "Prediction: \n",
      "True Answer: cars\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did the president of Toyota say?\n",
      "Prediction:  hailed Strank\n",
      "True Answer: he takes full responsibility for safety issues in the company's\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who should be held responsible?\n",
      "Prediction:  immigrants\n",
      "True Answer: Akio Toyoda\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who should be responsible?\n",
      "Prediction:  Strank's younger sister, Mary Pero.\n",
      "True Answer: the chief executive officer,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did Toyota's president say?\n",
      "Prediction:  the citizenship certificate\n",
      "True Answer: he takes full responsibility for safety issues in the company's\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What are the safety issues?\n",
      "Prediction: \n",
      "True Answer: related to sudden acceleration.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When was it isolated?\n",
      "Prediction:  March 1, 1945,\n",
      "True Answer: 1983\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who pioneered lithium treatment?\n",
      "Prediction: Jonathan Scharfen,\n",
      "True Answer: Dr. Cade\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When was HIV isolated?\n",
      "Prediction:  March 1, 1945,\n",
      "True Answer: 1983\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Many gay rights activists applaud Obama's what?\n",
      "Prediction:  a certificate of U.S. citizenship\n",
      "True Answer: on supporting full marriage equality,\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what did the president do\n",
      "Prediction:  was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "\n",
      "He hailed Strank\n",
      "True Answer: He acknowledged \"we have more work to do,\" including on the issue of bullying.\n",
      "Exact match: False\n",
      "F1 score: 0.05\n",
      "\n",
      "Question: what will obama do\n",
      "Prediction:  awarded a certificate of U.S. citizenship\n",
      "True Answer: \"we have more work to do,\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who do the gay rights activists applaud\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: the administration's progress,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who says he will continue to advocate for equality?\n",
      "Prediction:  Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "\n",
      "He hailed Strank as a true American hero and a wonderful example of the remarkable contribution and sacrifices that immigrants have made to our great republic throughout its history.\n",
      "True Answer: President Barack Obama,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who will continue to advocate for equality\n",
      "Prediction: \n",
      "True Answer: President Barack Obama,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What are hot spots for drug use?\n",
      "Prediction: \n",
      "True Answer: clubs and bars in Hong Kong and Shenzhen,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where else are drug hotspots?\n",
      "Prediction: \n",
      "True Answer: public toilets and playgrounds.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what types of drugs consumed\n",
      "Prediction:  drugs\n",
      "True Answer: ketamine.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is ketamine?\n",
      "Prediction:  Czechoslovakia\n",
      "True Answer: an animal tranquilizer,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the top drug choice in Hong Kong?\n",
      "Prediction:  Hong Kong?</s></s>One of the Marines shown in a famous World War II photograph\n",
      "True Answer: ketamine,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what includes an ice sculpture of the Grinch?\n",
      "Prediction: The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank,\n",
      "True Answer: frozen world located in the Gaslight Theater.\n",
      "Exact match: False\n",
      "F1 score: 0.08\n",
      "\n",
      "Question: What offers great shopping?\n",
      "Prediction:  Marine Corps War Memorial\n",
      "True Answer: Opry Mills,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where is the show ICE! being held?\n",
      "Prediction:  Arlington, Virginia,\n",
      "True Answer: Gaslight Theater.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: when did the decorations start going up?\n",
      "Prediction:  February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "\n",
      "He hailed Strank as a true American hero and a wonderful example of the remarkable contribution and sacrifices that immigrants have made to our great republic throughout its history.\n",
      "True Answer: in July\n",
      "Exact match: False\n",
      "F1 score: 0.03\n",
      "\n",
      "Question: When did the decorations begin to go up?\n",
      "Prediction:  February 23, 1945.\n",
      "True Answer: July\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When do the decorations go up?\n",
      "Prediction:  February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945,\n",
      "True Answer: July\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: length of time pilots to be treated\n",
      "Prediction: \n",
      "True Answer: at least 12 months.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: how long is the treatment\n",
      "Prediction:  less than a month\n",
      "True Answer: 12 months.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what does faa say\n",
      "Prediction: \n",
      "True Answer: the new policy will improve safety by bringing to the surface pilots who either ignore signs of depression or lie about their use of medication for fear of losing their licenses to fly.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: pilots must have been treated for at least 12 months for what reason?\n",
      "Prediction: \n",
      "True Answer: mild to moderate depression\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does de FAA says about the policy?\n",
      "Prediction: \n",
      "True Answer: \"absolutely\" improve safety,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what has become a way in which to emphasize ideas on Twitter?\n",
      "Prediction:  U.S. flag on Iwo Jima\n",
      "True Answer: the hashtag\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What we can use to tag a name on twitter or Facebook?\n",
      "Prediction: \n",
      "True Answer: \"@\"\n",
      "Exact match: True\n",
      "F1 score: 1\n",
      "\n",
      "Question: how many were wounded\n",
      "Prediction:  Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank\n",
      "True Answer: Four other people\n",
      "Exact match: False\n",
      "F1 score: 0.02\n",
      "\n",
      "Question: What was the number of suicide bombers?\n",
      "Prediction:  Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1,\n",
      "True Answer: two\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who said 2 suicide bombers carried out the attack\n",
      "Prediction:  U.S. Citizenship and Immigration Services\n",
      "True Answer: NATO's International Security Assistance Force\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did Afghan authorities describe?\n",
      "Prediction:  World War II\n",
      "True Answer: The attacker hid the explosive device inside his turban,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: how many bombers were there\n",
      "Prediction:  five\n",
      "True Answer: two\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who said the face of the peace initiative has been attacked\n",
      "Prediction: Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "True Answer: Gen. John R. Allen, commander of ISAF,\n",
      "Exact match: False\n",
      "F1 score: 0.03\n",
      "\n",
      "Question: What did the commander say?\n",
      "Prediction: \n",
      "True Answer: \"face of the peace initiative has been attacked.\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who was attacked\n",
      "Prediction: Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank\n",
      "True Answer: Burhanuddin Rabbani,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: how many people were wounded in the attack\n",
      "Prediction: how many people\n",
      "True Answer: Four\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What wa sthe name of the suspect?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Adam Yahiye Gadahn,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what does he say about his citizenship\n",
      "Prediction: \n",
      "True Answer: of America, the symbol of oppression and tyranny and advocate of terror in the world?\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who is in the video?\n",
      "Prediction: \n",
      "True Answer: Adam Yahiye Gadahn, also known as Azzam the American,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who criticizes obama\n",
      "Prediction: Jonathan Scharfen,\n",
      "True Answer: Adam Yahiye Gadahn,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did Adam Yahiye Gadahn say?\n",
      "Prediction:  a true American hero and\n",
      "True Answer: \"Let me here tell you something about myself and my biography, in which there is a benefit and a lesson,\"\n",
      "Exact match: False\n",
      "F1 score: 0.09\n",
      "\n",
      "Question: Which area was the convert from?\n",
      "Prediction:  United States\n",
      "True Answer: rural California,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who says the man denied wife liberty of coming and going with face uncovered?\n",
      "Prediction: \n",
      "True Answer: Immigration Minister Eric Besson\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did the man deny his wife of?\n",
      "Prediction: \n",
      "True Answer: liberty to come and go with her face uncovered,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Which country is scheduled to vote on whether to ban full veils?\n",
      "Prediction:  whether to ban full veils?</s></s>One of the Marines shown in a famous World War II photograph raising the U.S. flag on Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia\n",
      "True Answer: France\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did France deny a woman?\n",
      "Prediction:  citizenship papers.\n",
      "True Answer: naturalization request\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Which country denied a Moroccan woman's naturalization request?\n",
      "Prediction:  United States\n",
      "True Answer: France\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what was denied\n",
      "Prediction:  citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "True Answer: citizenship\n",
      "Exact match: False\n",
      "F1 score: 0.07\n",
      "\n",
      "Question: what was man denied\n",
      "Prediction:  citizenship papers.\n",
      "True Answer: citizenship\n",
      "Exact match: False\n",
      "F1 score: 0.67\n",
      "\n",
      "Question: Who defeated Froch?\n",
      "Prediction: \n",
      "True Answer: Mikkel\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who defeated Carl Froch?\n",
      "Prediction:  was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces\n",
      "True Answer: Mikkel\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many rounds were there in the match?\n",
      "Prediction: How many rounds were there in the match?</s></s>One of the Marines shown in a famous World War II photograph raising the U.S. flag on Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five\n",
      "True Answer: 12\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who defeats Carl to win?\n",
      "Prediction:  Japanese and U.S. forces\n",
      "True Answer: Mikkel\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Kessler secures unanimous points decision after what number of rounds?\n",
      "Prediction: Jonathan Scharfen, the acting director of CIS\n",
      "True Answer: 12\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where are the Thai soldiers accused of crossing into?\n",
      "Prediction:  Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island\n",
      "True Answer: Cambodian territory\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who claimed the soldiers had crossed into the area?\n",
      "Prediction: \n",
      "True Answer: Cambodian officials\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What temple is at the center of the debate?\n",
      "Prediction:  Mount Suribachi\n",
      "True Answer: Preah Vihear\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: The Thai army said what?\n",
      "Prediction:  Strank\n",
      "True Answer: soldiers had not gone anywhere they were not permitted to be.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What century is the Preah Vihear temple from?\n",
      "Prediction:  II\n",
      "True Answer: 11th\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did Thai soldiers cross into?\n",
      "Prediction:  citizenship papers.\n",
      "True Answer: Cambodian territory\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is arsenal manager?\n",
      "Prediction: \n",
      "True Answer: Arsene Wenger\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is the Arsenal manager?\n",
      "Prediction: Jonathan Scharfen,\n",
      "True Answer: Arsene Wenger\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does Luka Modric suffer from?\n",
      "Prediction: \n",
      "True Answer: a fracture to his right fibula,\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Which team beat Arsenal 2-1?\n",
      "Prediction: \n",
      "True Answer: Manchester United.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did Wenger kick?\n",
      "Prediction:  in action on the island\n",
      "True Answer: an empty water bottle\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who does Luka Modric play for?\n",
      "Prediction:  Czechoslovakia\n",
      "True Answer: Croatia\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who will receive an apology?\n",
      "Prediction:  Strank's younger sister, Mary Pero.\n",
      "True Answer: Arsenal\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what was traveling?\n",
      "Prediction:  Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island\n",
      "True Answer: 2,000 people\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many people were on board?\n",
      "Prediction: \n",
      "True Answer: about 2,000\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How much was the ferry capable of carrying?\n",
      "Prediction:  a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others\n",
      "True Answer: 1,500\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what did authorities recover\n",
      "Prediction:  U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "\n",
      "He hailed Strank as a true American hero and a wonderful example of the remarkable contribution and sacrifices that immigrants have made to our great republic throughout its history.\n",
      "True Answer: 54 bodies\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what is eid al adha?\n",
      "Prediction: \n",
      "True Answer: Muslim festival\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what capacity did the boat have?\n",
      "Prediction:  five other men\n",
      "True Answer: 1,500\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what was the boat capacity?\n",
      "Prediction: \n",
      "True Answer: 1,500\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many bodies were recovered?\n",
      "Prediction:  five\n",
      "True Answer: 54\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where was the ferry headed?\n",
      "Prediction:  Czechoslovakia\n",
      "True Answer: Bhola\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Taliban plans what month offensive?\n",
      "Prediction: Str\n",
      "True Answer: January\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When do they feel strong\n",
      "Prediction: \n",
      "True Answer: during the snowing season,\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is the Taliban leader?\n",
      "Prediction: Jonathan Scharfen\n",
      "True Answer: Hakeemullah Mehsud\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What day was the blast in Peshawar?\n",
      "Prediction:  March 1, 1945,\n",
      "True Answer: Monday's\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many people were killed in Monday's blast?\n",
      "Prediction:  five\n",
      "True Answer: Eleven\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many were killed  in Peshawar?\n",
      "Prediction:  five other men\n",
      "True Answer: people\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Do you know how many were killed\n",
      "Prediction:  five other men\n",
      "True Answer: Eleven\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who led 2-0 at halftime?\n",
      "Prediction: \n",
      "True Answer: Ghana\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who are the four-time champions?\n",
      "Prediction: Strank and five other men\n",
      "True Answer: Brazil\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who beat Costa Rica?\n",
      "Prediction:  Costa Rica?</s></s>One of the Marines shown in a famous World War II photograph raising the U.S. flag on Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men\n",
      "True Answer: Brazil\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where is the under-20 World Cup being held?\n",
      "Prediction:  Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia,\n",
      "True Answer: Egypt.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What was the score of Brazil vs Costa Rica?\n",
      "Prediction:  of Brazil vs Costa Rica?</s></s>One of the Marines shown in a famous World War II photograph raising the U.S. flag on Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "True Answer: 5-0,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who did Ghana beat?\n",
      "Prediction:  Japanese and U.S. forces\n",
      "True Answer: Hungary\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who faces the 4 times champion?\n",
      "Prediction:  Michael Strank,\n",
      "True Answer: Los Ticos\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the price of the Large Hadron Collider?\n",
      "Prediction: Sgt. Michael Strank, who was born in Czechoslovakia\n",
      "True Answer: $10 billion\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: how much are the cost of particle accelerator?\n",
      "Prediction: Sgt.\n",
      "True Answer: $10 billion\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the Hadron Collider?\n",
      "Prediction:  World War II photograph raising the U.S. flag on Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial\n",
      "True Answer: the world's largest particle\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does the machine do?\n",
      "Prediction:  planting\n",
      "True Answer: look at how the universe formed by analyzing particle collisions.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who worked alongside Tom Cruise?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Scott Altman\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who did Altman work with in this film?\n",
      "Prediction:  five others\n",
      "True Answer: Tom\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What profession did Scott Altman work for?\n",
      "Prediction:  Marines\n",
      "True Answer: retired Navy F-14 fighter pilot\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Was Scott Altman a fighter pilot?\n",
      "Prediction:  a famous World War II photograph\n",
      "True Answer: retired Navy F-14\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Which 1986 hit film did Altman perform a stunt double in?\n",
      "Prediction:  II photograph\n",
      "True Answer: \"Top Gun\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: in which hospital the child receives tracheotomy?\n",
      "Prediction:  Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial\n",
      "True Answer: SSM Cardinal Glennon Children's Medical Center in St. Louis.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: where is the hospital?\n",
      "Prediction:  Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia,\n",
      "True Answer: St. Louis, Missouri.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: when baby joseph died?\n",
      "Prediction:  March 1, 1945,\n",
      "True Answer: Tuesday afternoon.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: that suffers Joseph Maraachli?\n",
      "Prediction:  Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island\n",
      "True Answer: a progressive neurological disease\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: The infant received a tracheotomy at\n",
      "Prediction: \n",
      "True Answer: a children's hospital in St. Louis, Missouri.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who suffered from a neurological disease?\n",
      "Prediction: Sgt.\n",
      "True Answer: Joseph\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the reason that women have mammograms?\n",
      "Prediction:  mammograms?</s></s>One of the Marines shown in a famous World War II photograph raising the U.S. flag on Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "True Answer: cancer.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what does the cycle have to do with it?\n",
      "Prediction:  the flag-raising -- in Arlington, Virginia,\n",
      "True Answer: \"The best time of your cycle to do a mammogram is going to be when your period is over, maybe the week after your period is done when the breasts are not going to be tender.\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did the man have a degree in?\n",
      "Prediction:  Czechoslovakia\n",
      "True Answer: MBA in finance\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did the man get stuck in?\n",
      "Prediction:  on the island\n",
      "True Answer: a rabbit hole,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What was the man's name?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Karthik Rajaram\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did the father have an MBA in\n",
      "Prediction: \n",
      "True Answer: finance\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who was a fulbright schokar\n",
      "Prediction: \n",
      "True Answer: Krishna Rajaram,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where was one of the son's attending school?\n",
      "Prediction:  Czechoslovakia\n",
      "True Answer: UCLA.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many letters did the man leave\n",
      "Prediction: \n",
      "True Answer: three\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What are government troops and rebels battling for?\n",
      "Prediction:  citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces\n",
      "True Answer: strongholds in the north of Sri Lanka,\n",
      "Exact match: False\n",
      "F1 score: 0.05\n",
      "\n",
      "Question: Number of civilians that are trapped according to aid groups?\n",
      "Prediction:  of civilians\n",
      "True Answer: many as 250,000\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many civilians are trapped?\n",
      "Prediction:  five\n",
      "True Answer: as 250,000\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many civilians are trapped, according to aid groups?\n",
      "Prediction: \n",
      "True Answer: 250,000\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What country is being discussed here?\n",
      "Prediction:  U.S. flag on Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia,\n",
      "True Answer: Sri Lanka,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How long have the Ethnic Tamil minority been fighting?\n",
      "Prediction:  a month\n",
      "True Answer: since 1983.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many civilians are trapped due to this conflict?\n",
      "Prediction:  civilians\n",
      "True Answer: as\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What do the Tamils want?\n",
      "Prediction: \n",
      "True Answer: an independent homeland\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What happend to the last functioning medical facility in the zone?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: The fighting has forced the closure of Pudukkudiyiruppu hospital in the Vanni region,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Year the ethnic Tamil minority have been fighting since?\n",
      "Prediction:  1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "\n",
      "He hailed Strank as a true American hero and a wonderful example of the remarkable contribution and sacrifices that immigrants have made to our great republic throughout its history.\n",
      "True Answer: 1983.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How long have the Tamil minority been fighting for independence?\n",
      "Prediction:  March 1,\n",
      "True Answer: an independent homeland since 1983.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the date of the case?\n",
      "Prediction:  March 1, 1945,\n",
      "True Answer: March 22,\n",
      "Exact match: False\n",
      "F1 score: 0.4\n",
      "\n",
      "Question: Who will it be argued before?\n",
      "Prediction:  Strank's younger sister, Mary Pero.\n",
      "True Answer: a federal judge in Mississippi\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Which group filed a motion?\n",
      "Prediction: \n",
      "True Answer: The American Civil Liberties Union\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When will the case take place?\n",
      "Prediction:  Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945,\n",
      "True Answer: March 22,\n",
      "Exact match: False\n",
      "F1 score: 0.02\n",
      "\n",
      "Question: Who files against the school district?\n",
      "Prediction:  the school district?</s></s>One of the Marines shown in a famous World War II photograph raising the U.S. flag on Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank,\n",
      "True Answer: rights group\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Which school district is subject to the injunction?\n",
      "Prediction:  Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen\n",
      "True Answer: Mississippi\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did the group file with the court?\n",
      "Prediction:  a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "\n",
      "He hailed Strank as a true American hero and a wonderful example of the remarkable contribution and sacrifices that immigrants have made to our great republic throughout its history.\n",
      "True Answer: a motion for a preliminary injunction against a Mississippi school district and high school\n",
      "Exact match: False\n",
      "F1 score: 0.01\n",
      "\n",
      "Question: Who made passengers remove nipple rings?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Transportation Security Administration\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did the agency say?\n",
      "Prediction:  Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "\n",
      "He hailed Strank as a true American hero and a wonderful example of the remarkable contribution and sacrifices that immigrants have made to our great republic throughout its history.\n",
      "True Answer: airport appear to have properly followed procedures when they allegedly forced a woman to remove her nipple rings -- one with pliers -- but acknowledged the procedures should be changed.\n",
      "Exact match: False\n",
      "F1 score: 0.05\n",
      "\n",
      "Question: What backs officers who made passenger remove nipple rings?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Transportation Security Administration\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who found piercings at airport?\n",
      "Prediction:  U.S. Citizenship and Immigration Services\n",
      "True Answer: Mandi Hamlin\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who says she heard male agents snicker?\n",
      "Prediction: \n",
      "True Answer: Mandi Hamlin\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did the woman say?\n",
      "Prediction:  a true American hero and a wonderful example of the remarkable contribution and sacrifices that immigrants have made to our great republic throughout its history.\n",
      "True Answer: she was humiliated\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who said procedures need to be changed?\n",
      "Prediction:  U.S. Citizenship and Immigration Services\n",
      "True Answer: Transportation Security Administration\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What needs to be changed?\n",
      "Prediction:  citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "True Answer: the procedures\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who is suspended\n",
      "Prediction: Strank\n",
      "True Answer: in a campus library,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What was mocked at the party?\n",
      "Prediction: \n",
      "True Answer: Black History Month\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What was the rally opposing?\n",
      "Prediction:  Japanese and U.S. forces\n",
      "True Answer: racial intolerance.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: how many people involved\n",
      "Prediction:  five\n",
      "True Answer: hundreds\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: when is the incident\n",
      "Prediction:  March 1, 1945,\n",
      "True Answer: Thursday\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: was he punished\n",
      "Prediction: \n",
      "True Answer: suspended\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where did the student hang the noose?\n",
      "Prediction:  Arlington, Virginia,\n",
      "True Answer: campus library,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what was student attempting to accomplish\n",
      "Prediction: \n",
      "True Answer: intent to terrorize\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did Bryan ride for more than 11,000 miles?\n",
      "Prediction: \n",
      "True Answer: a motor scooter\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did the activist's route form?\n",
      "Prediction:  planting an American flag on top of Mount Suribachi\n",
      "True Answer: a peace sign.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did the activist want to do?\n",
      "Prediction: \n",
      "True Answer: meeting with the president to discuss her son.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many miles did Bryan ride his scooter?\n",
      "Prediction: \n",
      "True Answer: 11,000\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When was the tribunal established?\n",
      "Prediction: \n",
      "True Answer: in late 1994.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: when The United Nations established the genocide tribunal?\n",
      "Prediction:  Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945,\n",
      "True Answer: in late 1994.\n",
      "Exact match: False\n",
      "F1 score: 0.02\n",
      "\n",
      "Question: bagosora is charged with which crime?\n",
      "Prediction: \n",
      "True Answer: genocide,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is the mastermind?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Theoneste Bagosora,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When did massacres occur?\n",
      "Prediction:  March 1, 1945,\n",
      "True Answer: 1994\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: how many people dead?\n",
      "Prediction:  five\n",
      "True Answer: 800,000\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what country did massacre occur?\n",
      "Prediction: \n",
      "True Answer: Rwanda\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what position did bagosora hold?\n",
      "Prediction: \n",
      "True Answer: a colonel in the Rwandan army,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What have beer drinkers left?\n",
      "Prediction:  U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "\n",
      "He hailed Strank as a true American hero and a wonderful example of the remarkable contribution and sacrifices that immigrants have made to our great republic throughout its history.\n",
      "True Answer: glass shards\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What kind of dogs are wearing protective shoes?\n",
      "Prediction:  dogs\n",
      "True Answer: police\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What do dog shoes cost?\n",
      "Prediction:  dog shoes cost?</s></s>One of the Marines shown in a famous World War II photograph raising the U.S. flag on Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate\n",
      "True Answer: 60 euros\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What are police dogs now wearing?\n",
      "Prediction: \n",
      "True Answer: protective shoes\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What was the reason that the dogs wear protective shoes?\n",
      "Prediction: \n",
      "True Answer: Too many glass shards left by beer drinkers in the city center,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the reason for giving them shoes?\n",
      "Prediction: \n",
      "True Answer: Too many glass shards left by beer drinkers in the city center,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What do the shoes cost?\n",
      "Prediction:  Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services\n",
      "True Answer: $89\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What are police dogs wearing?\n",
      "Prediction: \n",
      "True Answer: protective shoes\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who also wore dog shoes?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: walk\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who says America needs a leader who understands the future we seek?\n",
      "Prediction: \n",
      "True Answer: former Virginia Gov. Mark Warner\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What political party is Mark Warner associated with?\n",
      "Prediction: S\n",
      "True Answer: Democratic\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Bush never asked Americans to do what?\n",
      "Prediction: \n",
      "True Answer: step up.\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Much of Warner's address focused on what?\n",
      "Prediction:  War II photograph\n",
      "True Answer: bipartisan rhetoric Obama has espoused\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Warner says America need what?\n",
      "Prediction: \n",
      "True Answer: a president who understands the world today, the future we seek and the change we\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who, specifically, did Mark Warner criticize?\n",
      "Prediction: \n",
      "True Answer: President Bush\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What type of rhetoric did Warner focus on?\n",
      "Prediction:  flag-raising\n",
      "True Answer: bipartisan\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Bush never asked what?\n",
      "Prediction: \n",
      "True Answer: us to step up.\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did most of Warner's address focus about?\n",
      "Prediction: \n",
      "True Answer: the kind of bipartisan rhetoric Obama has espoused on the campaign trail.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What kind of rhetoric did Mark Warner allude to frequently?\n",
      "Prediction:  national icons\n",
      "True Answer: bipartisan\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did Bush never ask Americans to do?\n",
      "Prediction: \n",
      "True Answer: step up.\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who does One Laptop per Child target?\n",
      "Prediction: \n",
      "True Answer: the world's poorest children.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What can laptops do?\n",
      "Prediction:  do?</s></s>One of the Marines shown in a famous World War II photograph raising the U.S. flag on Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons\n",
      "True Answer: allow students to engage in learning differently, enjoy a customized approach and hone critical thinking skills,\n",
      "Exact match: False\n",
      "F1 score: 0.05\n",
      "\n",
      "Question: Who knows how preachy and awkward the movies get?\n",
      "Prediction: Jonathan Scharfen,\n",
      "True Answer: Tripplehorn,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who directed the vignettes?\n",
      "Prediction: Jonathan Scharfen,\n",
      "True Answer: Aniston, Demi Moore and Alicia Keys\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does Jeanne Tripplehorn know about the movies?\n",
      "Prediction: \n",
      "True Answer: can get.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What story does \"Five\" tell?\n",
      "Prediction: \n",
      "True Answer: stories of different women coping with breast cancer\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who each directed a vignette?\n",
      "Prediction: Jonathan Scharfen,\n",
      "True Answer: Aniston, Demi Moore and Alicia Keys\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who directed a Vignette?\n",
      "Prediction: Jonathan Scharfen,\n",
      "True Answer: Aniston, Demi Moore and Alicia Keys\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the subject of the vignettes?\n",
      "Prediction:  World War II photograph raising the U.S. flag on Iwo Jima\n",
      "True Answer: different women coping with breast cancer\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is \"Five\" about?\n",
      "Prediction:  raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons\n",
      "True Answer: tells stories of different women coping with breast cancer in five vignettes.\n",
      "Exact match: False\n",
      "F1 score: 0.07\n",
      "\n",
      "Question: What stories does Five tell?\n",
      "Prediction:  them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "True Answer: different women coping with breast cancer in\n",
      "Exact match: False\n",
      "F1 score: 0.04\n",
      "\n",
      "Question: What would the news outlet say happened?\n",
      "Prediction: \n",
      "True Answer: series of summer concerts at the O2.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What musician has scheduled a new conference?\n",
      "Prediction: \n",
      "True Answer: Michael Jackson\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: In which season will the concerts be held?\n",
      "Prediction:  Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "True Answer: summer\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What has ben the subject of rumors?\n",
      "Prediction: \n",
      "True Answer: Pop star Michael Jackson\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where was the news conference held?\n",
      "Prediction:  Arlington, Virginia,\n",
      "True Answer: London's O2 arena,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What news outlet says he will hold a series of summer concerts?\n",
      "Prediction:  CIS\n",
      "True Answer: Britain's Sky\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: In what city is the O2 arena?\n",
      "Prediction:  Arlington, Virginia,\n",
      "True Answer: London's\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What has been the subject of rumors?\n",
      "Prediction: \n",
      "True Answer: Michael Jackson\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What do new materials do?\n",
      "Prediction: \n",
      "True Answer: truck safer,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did the Ford expert say?\n",
      "Prediction: \n",
      "True Answer: \"The extensive use of advanced technologies and materials in the 2009 F-150 required us to develop new, specific procedures and repair recommendations,\" said Gerry Bonanni,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What will insurers do?\n",
      "Prediction:  presented the citizenship certificate\n",
      "True Answer: try and reduce the cost of auto repairs and insurance premiums for consumers\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What makes the vehicles safer?\n",
      "Prediction:  Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank,\n",
      "True Answer: ultra-high-strength steel and boron\n",
      "Exact match: False\n",
      "F1 score: 0.08\n",
      "\n",
      "Question: What does the Ford expert say?\n",
      "Prediction: \n",
      "True Answer: \"The extensive use of advanced technologies and materials in the 2009 F-150 required us to develop new, specific procedures and repair recommendations,\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who did UAE deny visa to?\n",
      "Prediction:  his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank\n",
      "True Answer: Israeli tennis player Shahar Peer\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What company is no longer sponsoring the tournament?\n",
      "Prediction: \n",
      "True Answer: The Wall Street Journal Europe\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the sport the player is active in?\n",
      "Prediction: \n",
      "True Answer: tennis\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what sport did he play\n",
      "Prediction:  II\n",
      "True Answer: tennis\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: where did this happen\n",
      "Prediction:  Arlington, Virginia,\n",
      "True Answer: Dubai\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many tips did the worker call in?\n",
      "Prediction:  five\n",
      "True Answer: three\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What child's remains were found?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Caylee Anthony\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How long ago did the friend tell police to check the area?\n",
      "Prediction:  February 23, 1945.\n",
      "True Answer: month before the meter\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who went missing?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Caylee Anthony,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What was found in the search area?\n",
      "Prediction:  an American flag\n",
      "True Answer: \"significant skeletal remains\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who called in several tips?\n",
      "Prediction:  U.S. Citizenship and Immigration Services\n",
      "True Answer: meter reader\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who told police to check the area five months ago?\n",
      "Prediction:  U.S.\n",
      "True Answer: KioMarie Cruz,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What news station reported on this case?\n",
      "Prediction:  Associated\n",
      "True Answer: CNN\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who crossed the Atlantic in a micro yacht?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Hugo Vihlen\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who made evolutionary discoveries?\n",
      "Prediction:  U.S. Citizenship and Immigration Services\n",
      "True Answer: Naturalist Charles Darwin\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What happened in 1620?\n",
      "Prediction: \n",
      "True Answer: Pilgrims sail to Plymouth Rock\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When did Darwin make his discoveries?\n",
      "Prediction:  March 1, 1945,\n",
      "True Answer: 1831\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where did the Pilgrim's voyage to?\n",
      "Prediction:  United States\n",
      "True Answer: Plymouth Rock\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When was the Beagle voyage?\n",
      "Prediction:  March 1, 1945,\n",
      "True Answer: 1831\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When was the Mayflower voyage?\n",
      "Prediction:  1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945,\n",
      "True Answer: 1620\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who did they back?\n",
      "Prediction:  Strank's younger sister, Mary Pero.\n",
      "True Answer: the Dalai Lama\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who wnated Tibet's independence?\n",
      "Prediction: \n",
      "True Answer: small minority\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does Lama seek?\n",
      "Prediction:  citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "True Answer: autonomy.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does a small minority demand for Tibet?\n",
      "Prediction:  the U.S. flag\n",
      "True Answer: independence,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What do Tibetian leaders back?\n",
      "Prediction:  a famous World War II photograph raising the U.S. flag on Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers\n",
      "True Answer: the Dalai Lama's current \"middle way approach,\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does the Dalai Lama seek from Bejing?\n",
      "Prediction:  certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers\n",
      "True Answer: autonomy.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does Dalai Lama seeks?\n",
      "Prediction: \n",
      "True Answer: autonomy.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does the minority want?\n",
      "Prediction: \n",
      "True Answer: Tibet's independence,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What do the Tibetan exile leaders back?\n",
      "Prediction:  a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "True Answer: \"middle way approach,\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who designed courses in Eastern Europe?\n",
      "Prediction:  designed courses in Eastern Europe?</s></s>One of the Marines shown in a famous World War II photograph raising the U.S. flag on Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank,\n",
      "True Answer: Gary Player\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: where has 6 courses opened\n",
      "Prediction:  Arlington, Virginia,\n",
      "True Answer: on the Black Sea coast in Bulgaria.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what has Player designed\n",
      "Prediction: \n",
      "True Answer: two courses on the Black Sea coast in Bulgaria.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who has been designer for Golf in several Eastern European countries?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Gary Player\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many courses have opened?\n",
      "Prediction:  five\n",
      "True Answer: six\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what has made progress\n",
      "Prediction:  remarkable contribution and sacrifices that immigrants\n",
      "True Answer: development of two courses on the Black Sea coast in Bulgaria.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: All together, how many Golf developments Bulgaria has?\n",
      "Prediction: \n",
      "True Answer: two\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is the  Aztecas street gang affiliated with?\n",
      "Prediction:  Czechoslovakia\n",
      "True Answer: cartel.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is the suspect?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Ricardo Valles de la Rosa,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is Ricardo Valles de la Rosa's age?\n",
      "Prediction:  Czechoslovakia\n",
      "True Answer: 42 years old\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who arrested a suspect on Friday?\n",
      "Prediction:  U.S. Citizenship and Immigration Services\n",
      "True Answer: Mexican military\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who arrested the suspect Friday?\n",
      "Prediction: Jonathan Scharfen\n",
      "True Answer: Mexican military\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is the new suspect sought in connection with the slaying?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Ricardo Valles de la Rosa,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When was the Mexican military arrested?\n",
      "Prediction:  March 1, 1945,\n",
      "True Answer: Monday.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what is Bin's first name?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Omar\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Omar bin Laden split with whom in 2000?\n",
      "Prediction:  U.S\n",
      "True Answer: al Qaeda.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is Bin Laden?\n",
      "Prediction: \n",
      "True Answer: the most-wanted man in the world\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When did he split from his father?\n",
      "Prediction: \n",
      "True Answer: in 2000\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the name of Bin Laden's son?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Omar bin Laden\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Bin Laden's son was named who?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Omar\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: when was 9/11?\n",
      "Prediction:  March 1, 1945,\n",
      "True Answer: September 11, 2001.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is CNN's Baghdad correspondent?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Michael Ware\n",
      "Exact match: False\n",
      "F1 score: 0.4\n",
      "\n",
      "Question: Who is the leader of the Iranian-backed militia?\n",
      "Prediction:  Michael Strank,\n",
      "True Answer: Muqtada al-Sadr,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what did CNN Baghdad say?\n",
      "Prediction: \n",
      "True Answer: correspondent Michael Ware cast doubt on Woodward's assertion\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What team does Bob Woodward credit?\n",
      "Prediction:  II\n",
      "True Answer: \"fusion teams,\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What day will the winners be announced?\n",
      "Prediction:  Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935.\n",
      "True Answer: Friday,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: when is going to be announced the winner?\n",
      "Prediction:  Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935.\n",
      "True Answer: Friday,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: whats the name of the colombian senator?\n",
      "Prediction: \n",
      "True Answer: Piedad Cordoba,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: how many contenders are for nobel peace prize?\n",
      "Prediction:  five\n",
      "True Answer: three\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When is the winner going to be announced?\n",
      "Prediction:  February 23, 1945.\n",
      "True Answer: Friday,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many top contenders are there for the Nobel Peace Prize?\n",
      "Prediction:  many top contenders are there for the Nobel Peace Prize?</s></s>One of the Marines shown in a famous World War II photograph raising the U.S. flag on Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five\n",
      "True Answer: three\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many years until another hearing for Atkins?\n",
      "Prediction:  March 1, 1945,\n",
      "True Answer: three\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who held Sharon Tate down and stabbed her 16 times?\n",
      "Prediction: Sgt.\n",
      "True Answer: Susan Atkins,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When did the panel set another hearing for?\n",
      "Prediction:  Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday\n",
      "True Answer: in three years,\n",
      "Exact match: False\n",
      "F1 score: 0.03\n",
      "\n",
      "Question: What happened at Susan Atkins parole hearing?\n",
      "Prediction:  was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "True Answer: denied\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Did Sharon Tate's murderer get parole?\n",
      "Prediction: \n",
      "True Answer: denied\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many times was Sharon Tate stabbed?\n",
      "Prediction:  3\n",
      "True Answer: 16\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is Atkins battling?\n",
      "Prediction:  Japanese and U.S. forces there\n",
      "True Answer: terminal brain cancer.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where did the dogs detect her scent?\n",
      "Prediction:  the dogs\n",
      "True Answer: near the George Washington Bridge,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the disappeared woman's name?\n",
      "Prediction:  Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "True Answer: Liza Murphy\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What number do you call if you have information?\n",
      "Prediction: Sgt.\n",
      "True Answer: 201-262-2800.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When did the woman disappear?\n",
      "Prediction:  March 1, 1945,\n",
      "True Answer: August 19, 2007.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did the husband try after wards?\n",
      "Prediction:  an American flag\n",
      "True Answer: to take his own life\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: where was a man killed ?\n",
      "Prediction:  on the island\n",
      "True Answer: Port-au-Prince, Haiti\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What was the cause of death?\n",
      "Prediction: \n",
      "True Answer: \"The people kill him with the blocks,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did the photos show?\n",
      "Prediction:  raising the U.S. flag\n",
      "True Answer: the man facing up, with his arms out to the side.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what did the man say?\n",
      "Prediction: \n",
      "True Answer: \"This is robbery. He went to rob the people. He went to steal money -- American dollars,\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what showed the gruesome scene?\n",
      "Prediction:  World War II photograph raising the U.S. flag\n",
      "True Answer: photos\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who provides humanitarian aid to the Somalis?\n",
      "Prediction:  Somalis\n",
      "True Answer: Daryeel Bulasho Guud\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the DBG agency and where do they operate?\n",
      "Prediction: \n",
      "True Answer: Nairobi, Kenya,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What nationalities were the aid workers?\n",
      "Prediction:  Czechoslovakia\n",
      "True Answer: Somalis\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What aid agency suspended operations?\n",
      "Prediction:  Citizenship and Immigration Services\n",
      "True Answer: DBG,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What are some reasons why Somalis depend on humanitarian aid?\n",
      "Prediction: \n",
      "True Answer: severe famine\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What has the aid agency done?\n",
      "Prediction: \n",
      "True Answer: suspend all\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Since when do Somalis depend on humanitarian aid?\n",
      "Prediction:  Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "True Answer: 1991-1993,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who was shot in Somalia?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Three aid workers\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How long have Robinson and Bridges been dating?\n",
      "Prediction:  a month before the battle between Japanese and U.S. forces there ended.\n",
      "True Answer: two years,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: how long have they been dating\n",
      "Prediction:  1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "\n",
      "He hailed Strank as a true American hero and a wonderful example of the remarkable contribution and sacrifices that immigrants have made to our great republic throughout its history.\n",
      "True Answer: two years,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: how many children does he have\n",
      "Prediction:  five\n",
      "True Answer: second child\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is having a baby?\n",
      "Prediction: \n",
      "True Answer: Chris Robinson and girlfriend Allison Bridges\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How long has Robinson and girlfriend been dating?\n",
      "Prediction:  1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less\n",
      "True Answer: two years,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is Robinson's ex-girlfriend?\n",
      "Prediction:  Mary Pero.\n",
      "True Answer: Kate\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the 5 1/2 year old's name?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Ryder Russell,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the name of her show ?\n",
      "Prediction:  a famous World War II photograph\n",
      "True Answer: \"The Rosie Show,\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What O`Donnell said about her love life?\n",
      "Prediction: \n",
      "True Answer: \"I think if I had known that she was gay, I wouldn't have been brave enough to talk to her,\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did O'Donnell say recently?\n",
      "Prediction:  who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "\n",
      "He hailed Strank as a true American hero and a wonderful example of the remarkable contribution and sacrifices that immigrants have made to our great republic throughout its history.\n",
      "True Answer: \"I think if I had known that she was gay, I wouldn't have been brave enough to talk to her,\"\n",
      "Exact match: False\n",
      "F1 score: 0.05\n",
      "\n",
      "Question: Where  was O'Donnell  attracted to Rounds  ?\n",
      "Prediction:  Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia\n",
      "True Answer: Starbucks\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who recently said she's in love ?\n",
      "Prediction:  Michael Strank,\n",
      "True Answer: O'Donnell,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the name of her new show?\n",
      "Prediction:  Strank's younger sister, Mary Pero.\n",
      "True Answer: \"The Rosie Show,\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: On what O`Donnell has been attracted to?\n",
      "Prediction: \n",
      "True Answer: Michelle Rounds\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When did the Rosie Show debut on the OWN network?\n",
      "Prediction:  February 23, 1945.\n",
      "True Answer: Monday\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who was O'donnel attracted to after seeing her in starbucks?\n",
      "Prediction:  O\n",
      "True Answer: Michelle Rounds\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who was absent from the trial?\n",
      "Prediction: \n",
      "True Answer: Gov. Rod Blagojevich\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who is absent from trial?\n",
      "Prediction: \n",
      "True Answer: Gov. Rod Blagojevich\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who heard the recorded phone calls?\n",
      "Prediction: \n",
      "True Answer: state senators\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is holding interviews?\n",
      "Prediction: \n",
      "True Answer: Gov. Rod Blagojevich\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who did the senators hear from?\n",
      "Prediction:  citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "\n",
      "He hailed Strank\n",
      "True Answer: Rod Blagojevich\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: From who was the testimony?\n",
      "Prediction:  Strank's younger sister, Mary Pero.\n",
      "True Answer: FBI Special Agent Daniel Cain,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who heard recorded phone calls?\n",
      "Prediction:  Michael Strank,\n",
      "True Answer: state senators\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did commentators focus on?\n",
      "Prediction:  citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men\n",
      "True Answer: \"bystander effect\":\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what are commentators focused on?\n",
      "Prediction: \n",
      "True Answer: \"bystander effect\":\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what do the studies show?\n",
      "Prediction:  famous World War II photograph raising the U.S. flag\n",
      "True Answer: students often know ahead of time when and where violence will flare up on campus.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What crime do students not report?\n",
      "Prediction: \n",
      "True Answer: gang rape\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What do studies show?\n",
      "Prediction: \n",
      "True Answer: that students often know ahead of time when and where violence will flare up on campus.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is Robert Kimmitt?\n",
      "Prediction: \n",
      "True Answer: Deputy Treasury Secretary\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does MME talk to UAE's Minister of Foreign Trade about?\n",
      "Prediction: \n",
      "True Answer: Sovereign Wealth Funds\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Name of the Deputy Treasury Secretary?\n",
      "Prediction: \n",
      "True Answer: Robert Kimmitt.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is the U.A.E.'s Minister of Foreign Trade?\n",
      "Prediction:  U.A.E.'s\n",
      "True Answer: Sheikha Lubna Al Qasimi,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What was the purpose of Brown's Gulf tour?\n",
      "Prediction: \n",
      "True Answer: in an attempt to secure more funds from the region.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where did UK Prime Minister Gordon Brown tour?\n",
      "Prediction:  Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia,\n",
      "True Answer: the Gulf\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did Robert Kimmitt discuss?\n",
      "Prediction:  famous World War II photograph\n",
      "True Answer: Sovereign Wealth Funds\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Name of the prime minister of the UK?\n",
      "Prediction:  UK?</s></s>One of the Marines shown in a famous World War II photograph raising the U.S. flag on Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services\n",
      "True Answer: Gordon Brown\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What position does Robert Kimmitt hold?\n",
      "Prediction: \n",
      "True Answer: Deputy Treasury Secretary\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what is missing?\n",
      "Prediction:  Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "\n",
      "He hailed Strank as a true American hero and a wonderful example of the remarkable contribution and sacrifices that immigrants have made to our great republic throughout its history.\n",
      "True Answer: A Pablo Picasso sketchbook with 33 pencil drawings\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what does it look like?\n",
      "Prediction:  the flag-raising\n",
      "True Answer: The sketchbook has a red varnished cover with the word \"Album\" inscribed on\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: how many drawings missing\n",
      "Prediction:  U.S. Citizenship and Immigration Services recently discovered that Strank\n",
      "True Answer: 33 pencil\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: The artist used the sketchbook when?\n",
      "Prediction:  February 23, 1945.\n",
      "True Answer: between 1917 and 1924\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: when was the notebook used?\n",
      "Prediction:  February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "\n",
      "He hailed Strank as a true American hero and a wonderful example of the remarkable contribution and sacrifices that immigrants have made to our great republic throughout its history.\n",
      "True Answer: between 1917 and 1924\n",
      "Exact match: False\n",
      "F1 score: 0.06\n",
      "\n",
      "Question: What color is the cover?\n",
      "Prediction:  II\n",
      "True Answer: red\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who was killed in the hijacking?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Reggae legend Lucky Dube,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who has killed in the attempted hijacking?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Reggae legend Lucky Dube,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Does this affect the upcoming World Cup?\n",
      "Prediction: \n",
      "True Answer: his death cast a shadow over festivities\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where did the hijacking take place?\n",
      "Prediction:  Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia,\n",
      "True Answer: Johannesburg\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did the hijacker try to steal?\n",
      "Prediction: \n",
      "True Answer: car,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: In what city was Dube killed?\n",
      "Prediction:  Czechoslovakia\n",
      "True Answer: Johannesburg\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When was Dube killed?\n",
      "Prediction:  March 1, 1945,\n",
      "True Answer: around 8 p.m. local time Thursday\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Did hijacker try to steal a car?\n",
      "Prediction:  certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "True Answer: his\n",
      "Exact match: False\n",
      "F1 score: 0.01\n",
      "\n",
      "Question: What country was reggae legend Lucky Dube from?\n",
      "Prediction:  Czechoslovakia\n",
      "True Answer: South Africa's\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who was killed in an attempted hijacking?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Lucky Dube,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Did he during an attempted carjacking\n",
      "Prediction:  on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "True Answer: Dube, 43, was killed\n",
      "Exact match: False\n",
      "F1 score: 0.03\n",
      "\n",
      "Question: Where did 1 million gather?\n",
      "Prediction:  Arlington, Virginia,\n",
      "True Answer: in Angola\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the name of the Pope?\n",
      "Prediction: \n",
      "True Answer: Benedict XVI\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Amount of people gathered to hear the Pope?\n",
      "Prediction:  of people gathered to hear the Pope?</s></s>One of the Marines shown in a famous World War II photograph raising the U.S. flag on Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others\n",
      "True Answer: 1 million\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: where was the event\n",
      "Prediction:  Arlington, Virginia,\n",
      "True Answer: Angola\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does the Pope express?\n",
      "Prediction:  a true American hero\n",
      "True Answer: Benedict also expressed \"deep sorrow\" at the death of two women killed in a stampede at one of his events in Angola on Saturday,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: how many dead?\n",
      "Prediction:  five other men\n",
      "True Answer: two\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where was Mass?\n",
      "Prediction:  Arlington, Virginia,\n",
      "True Answer: in Angola,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What continent is the Pope visiting?\n",
      "Prediction: oslovakia\n",
      "True Answer: Africa.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Which airline has lighting that subtly shifts throughout the day?\n",
      "Prediction: Sgt. Michael Strank\n",
      "True Answer: Virgin America\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Which airline has in-cabin lighting?\n",
      "Prediction: \n",
      "True Answer: Virgin America\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many driverless pods were there?\n",
      "Prediction:  five\n",
      "True Answer: 18\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where are the driverless pods being tested\n",
      "Prediction:  the Marine Corps Memorial\n",
      "True Answer: at airports\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many pods are being tested?\n",
      "Prediction:  many\n",
      "True Answer: are\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What shifts throughout the day?\n",
      "Prediction:  the flag-raising\n",
      "True Answer: in-cabin lighting system\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: In which country is Madonna helping orphans?\n",
      "Prediction:  Czechoslovakia\n",
      "True Answer: Malawi,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who discovers the passion of the Angolan footballers?\n",
      "Prediction:  U.S. Citizenship and Immigration Services\n",
      "True Answer: David McKenzie\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who discovered the passion of the Angolan football squad?\n",
      "Prediction:  U.S. Citizenship and Immigration Services\n",
      "True Answer: David McKenzie\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Alina Cho speaks to who about her efforts to help other Malawi's orphans?\n",
      "Prediction: Alina Cho\n",
      "True Answer: Madonna\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is helping Malawian orphans?\n",
      "Prediction: \n",
      "True Answer: Madonna\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What sport does the World Cup legend play?\n",
      "Prediction:  II\n",
      "True Answer: football\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the name of the World Cup legend?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Diego Maradona\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Inside Africa catches up with a World Cup legend spreading football excitement in which country?\n",
      "Prediction:  Czechoslovakia\n",
      "True Answer: South\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who speaks to Madonna?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Alina Cho\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where did he go?\n",
      "Prediction:  Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island\n",
      "True Answer: drove to a gym\n",
      "Exact match: False\n",
      "F1 score: 0.04\n",
      "\n",
      "Question: where were the bodies found?\n",
      "Prediction:  Arlington, Virginia,\n",
      "True Answer: the bedrooms of their two-floor home\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what city did slayings occur in?\n",
      "Prediction:  Arlington, Virginia,\n",
      "True Answer: St. Louis,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What was on the walls of the home?\n",
      "Prediction: \n",
      "True Answer: threatening messages\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who will decide whether to file charges?\n",
      "Prediction:  U.S. Citizenship and Immigration Services\n",
      "True Answer: state's attorney\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where did the husband say he was?\n",
      "Prediction:  Czechoslovakia\n",
      "True Answer: gym to work out.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who made the documentary?\n",
      "Prediction:  Associated Press\n",
      "True Answer: Sabina Guzzanti\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Documentary was screened where?\n",
      "Prediction: \n",
      "True Answer: Cannes Film Festival,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What was the name of the documentary?\n",
      "Prediction:  World War II photograph raising the U.S. flag on Iwo Jima\n",
      "True Answer: \"Draquila\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who boycotted the film?\n",
      "Prediction:  Michael Strank,\n",
      "True Answer: Sandro Bondi\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: The film, \"Draquila,\" takes issue with the way the prime minister handled what earthquake?\n",
      "Prediction:  \"Draquila,\" takes issue with the way the prime minister handled what earthquake?</s></s>One of the Marines shown in a famous World War II\n",
      "True Answer: L'Aquila\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Italian Culture Minister boycotted Cannes  why?\n",
      "Prediction:  his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "True Answer: Bondi issued a statement, dismissing the documentary as \"propaganda\" and saying it \"offends the truth and all of the\n",
      "Exact match: False\n",
      "F1 score: 0.06\n",
      "\n",
      "Question: What is the documentary about?\n",
      "Prediction:  World War II photograph\n",
      "True Answer: prime minister's handling of the L'Aquila earthquake,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the filmmaker's name?\n",
      "Prediction: \n",
      "True Answer: Sabina Guzzanti\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who was the documentary about?\n",
      "Prediction: \n",
      "True Answer: Silvio Berlusconi.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is open 24/7?\n",
      "Prediction: \n",
      "True Answer: The control room\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what year was H1N1\n",
      "Prediction:  1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1,\n",
      "True Answer: 2009\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who needs to use caution?\n",
      "Prediction: Sgt. Michael Strank\n",
      "True Answer: those traveling near the Somali coast\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where will passengers fly?\n",
      "Prediction:  Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island\n",
      "True Answer: over the\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where do passengers have to fly to continue their journey?\n",
      "Prediction:  United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia,\n",
      "True Answer: Dubai\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who issued a travel warning?\n",
      "Prediction:  U.S. Citizenship and Immigration Services\n",
      "True Answer: German Foreign Ministry,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many passengers were mentioned as travelling in this situation?\n",
      "Prediction:  five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five\n",
      "True Answer: 246\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Which countries advise extreme caution while traveling near the Somali coast?\n",
      "Prediction:  Czechoslovakia\n",
      "True Answer: U.S. State Department and British Foreign Office\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where will passengers fly to continue their journey?\n",
      "Prediction:  Czechoslovakia\n",
      "True Answer: Dubai\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many passengers does it involve?\n",
      "Prediction:  five\n",
      "True Answer: 246\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Which ship was the incident aboard?\n",
      "Prediction:  U.S. flag on Iwo Jima\n",
      "True Answer: MS Columbus,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where did the US and UK say that people should avoid?\n",
      "Prediction:  that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island\n",
      "True Answer: the Somali coast\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where was Omar previously denied?\n",
      "Prediction:  citizenship papers.\n",
      "True Answer: asylum in Britain.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Did Spain give a reason for turning down the asylum?\n",
      "Prediction:  citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "\n",
      "He hailed Strank as a true American hero and a wonderful example of the remarkable contribution and sacrifices that immigrants have made to our great republic throughout its history.\n",
      "True Answer: was given\n",
      "Exact match: False\n",
      "F1 score: 0.02\n",
      "\n",
      "Question: Who was denied asylum in Britain?\n",
      "Prediction:  Michael Strank,\n",
      "True Answer: Omar bin Laden\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where did Omar bin Laden first try to flee to?\n",
      "Prediction:  Czechoslovakia\n",
      "True Answer: Britain.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who did Spain deny asylum to?\n",
      "Prediction:  Marines\n",
      "True Answer: Omar bin Laden,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who was denied asylum?\n",
      "Prediction: \n",
      "True Answer: Omar bin Laden\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who didn't give a reason?\n",
      "Prediction: Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank\n",
      "True Answer: U.N. High Commissioner for Refugees\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What family member of Omar bin Laden was associated with terrorism?\n",
      "Prediction: Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "True Answer: his father\n",
      "Exact match: False\n",
      "F1 score: 0.04\n",
      "\n",
      "Question: What did he want his father to abandon?\n",
      "Prediction:  the U.S. flag\n",
      "True Answer: terrorism.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Mugabe and Tsvangirai have signed agreement paving way for what?\n",
      "Prediction:  agreement paving way for what?</s></s>One of the Marines shown in a famous World War II photograph raising the U.S. flag on Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "True Answer: power-sharing talks to take place in the next few weeks.\n",
      "Exact match: False\n",
      "F1 score: 0.04\n",
      "\n",
      "Question: Whose government was called to be illegitimate?\n",
      "Prediction:  his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services\n",
      "True Answer: Zimbabwean\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does order expand?\n",
      "Prediction: \n",
      "True Answer: U.S. sanctions against\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did Bush calls Robert Mugabe's government?\n",
      "Prediction:  was killed in action on the island on March 1,\n",
      "True Answer: \"illegitimate.\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who signed the order?\n",
      "Prediction: \n",
      "True Answer: President Bush\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What two countries vetoed UN resolutions relevant to this situation?\n",
      "Prediction:  Czechoslovakia\n",
      "True Answer: Russia and China\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is Tsvangirai?\n",
      "Prediction:  Czechoslovakia and came to the United States when he was 3,\n",
      "True Answer: opposition candidate\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did President Bush sign order to expand?\n",
      "Prediction:  U.S.\n",
      "True Answer: U.S. sanctions against\n",
      "Exact match: False\n",
      "F1 score: 0.5\n",
      "\n",
      "Question: Who signed an agreement paving way for power-sharing talks?\n",
      "Prediction:  U.S. Citizenship and Immigration Services\n",
      "True Answer: Mugabe and Tsvangirai\n",
      "Exact match: False\n",
      "F1 score: 0.25\n",
      "\n",
      "Question: What country does Robert Mugabe lead?\n",
      "Prediction:  Czechoslovakia\n",
      "True Answer: Zimbabwe,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What country are the sanctions against?\n",
      "Prediction:  Czechoslovakia\n",
      "True Answer: Zimbabwe,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who vetoed the UN resolution?\n",
      "Prediction:  Scharfen\n",
      "True Answer: Russia and China\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did tthe CEO say?\n",
      "Prediction: He hailed Strank as\n",
      "True Answer: the Airbus A330-200 encountered heavy turbulence about 02:15 a.m. local time Monday\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What encountered heavy turbulence?\n",
      "Prediction: \n",
      "True Answer: the Airbus A330-200\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did the A330-200 encounter?\n",
      "Prediction:  citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces\n",
      "True Answer: heavy turbulence\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did the Airbus A330-200 encounter\n",
      "Prediction:  citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces\n",
      "True Answer: heavy turbulence\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many people were on the flight?\n",
      "Prediction: Sgt.\n",
      "True Answer: 228\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many people where onboard the flight\n",
      "Prediction:  five\n",
      "True Answer: 228\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did some consider to be a possible cause\n",
      "Prediction:  immigrants\n",
      "True Answer: lightning strike\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did Obama said?\n",
      "Prediction:  Strank\n",
      "True Answer: \"We've seen Washington launch policy after policy, yet our dependence on foreign oil has only grown, even as the world's resources are disappearing,\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How much will be invested?\n",
      "Prediction: \n",
      "True Answer: $150 billion\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How much would Obama invest?\n",
      "Prediction:  citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "True Answer: $150 billion over 10 years\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who was named secretary of energy?\n",
      "Prediction: Jonathan Scharfen,\n",
      "True Answer: Steven Chu\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who named Secretary of energy?\n",
      "Prediction:  Secretary of energy?</s></s>One of the Marines shown in a famous World War II photograph raising the U.S. flag on Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank,\n",
      "True Answer: Steven Chu\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who created climate policy?\n",
      "Prediction:  his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen,\n",
      "True Answer: Carol Browner\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who hosted \"Star Search\"?\n",
      "Prediction: \n",
      "True Answer: Ed McMahon\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who hosted \"The Tonight Show\"?\n",
      "Prediction: Jonathan Scharfen,\n",
      "True Answer: Carson\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What tv shows hosted McMahon?\n",
      "Prediction:  tv shows hosted McMahon?</s></s>One of the Marines shown in a famous World War II photograph raising the U.S. flag on Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps\n",
      "True Answer: \"TV's Bloopers and Practical Jokes,\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is McMahon most famous for?\n",
      "Prediction: \n",
      "True Answer: longtime pitchman and Johnny Carson sidekick\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what health problems suffered McMahon?\n",
      "Prediction:  II photograph raising the U.S. flag on Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed\n",
      "True Answer: pneumonia and other medical\n",
      "Exact match: False\n",
      "F1 score: 0.03\n",
      "\n",
      "Question: Which TV programs did McMahon host?\n",
      "Prediction: \n",
      "True Answer: \"TV's Bloopers and Practical Jokes,\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: New book quotes Reid discussing what?\n",
      "Prediction:  famous World War II photograph\n",
      "True Answer: saying privately in 2008 that Obama could be successful as a black candidate in part because of his \"light-skinned\" appearance and speaking patterns \"with no Negro dialect, unless he wanted to have one.\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does Congressional Black Caucus reject?\n",
      "Prediction: \n",
      "True Answer: calls for Reid's dismissal.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does GOP chair say about the Senate majority leader's language?\n",
      "Prediction:  was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "\n",
      "He hailed Strank as a true American hero and a wonderful example of the remarkable contribution and sacrifices that immigrants have made to our great republic throughout its history.\n",
      "True Answer: \"embarrassing and racially insensitive,\"\n",
      "Exact match: False\n",
      "F1 score: 0.03\n",
      "\n",
      "Question: Congressional Black Caucus rejects calls for?\n",
      "Prediction: \n",
      "True Answer: him to step down as majority leader.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What are training dogs for?\n",
      "Prediction:  U.S. flag\n",
      "True Answer: to sniff out cell phones.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What are dogs trained to do?\n",
      "Prediction:  planting\n",
      "True Answer: sniff out cell phones.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What clears Texas Legislature?\n",
      "Prediction:  a famous World War II photograph\n",
      "True Answer: that would crack down on convicts caught with phones and allow prison systems to monitor and detect cell signals.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does the bill crack down on?\n",
      "Prediction: \n",
      "True Answer: convicts caught with phones\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What are sniffer dogs in Texas prisons looking for?\n",
      "Prediction:  citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "\n",
      "He hailed Strank as a true American hero and a wonderful example of the remarkable contribution and sacrifices that immigrants have made to our great republic throughout its history.\n",
      "True Answer: sniff out cell phones.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What do inmates in Texas have?\n",
      "Prediction:  certificate\n",
      "True Answer: cell phones\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is texas?\n",
      "Prediction: \n",
      "True Answer: state\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does the book include?\n",
      "Prediction:  the flag-raising\n",
      "True Answer: a paragraph about the king and crown prince\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who was arrested?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Harry Nicolaides,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what did authorities consider it as\n",
      "Prediction:  never was given citizenship papers\n",
      "True Answer: illegal to defame, insult or threaten the crown.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When was Harry Nicolaides arrested over his 2005 book?\n",
      "Prediction:  February 23, 1945.\n",
      "True Answer: August 31.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what age was Harry Nicolaides\n",
      "Prediction:  3,\n",
      "True Answer: 41,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what does book include?\n",
      "Prediction:  World War II photograph\n",
      "True Answer: a paragraph about the king and crown prince\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what did Man's lawyer say\n",
      "Prediction: \n",
      "True Answer: he was released Friday and taken to the Australian embassy\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what did authorites deem?\n",
      "Prediction:  Strank\n",
      "True Answer: a violation of a law that makes it illegal to defame, insult or threaten the crown.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where does the husband tweet from?\n",
      "Prediction:  Czechoslovakia\n",
      "True Answer: Haiti\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: from where Troy issues tweets?\n",
      "Prediction:  Czechoslovakia\n",
      "True Answer: Haiti\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who are missionary couple with houseful of children?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Tara and Troy Livesay\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When did wife blog?\n",
      "Prediction:  February 23, 1945.\n",
      "True Answer: Wednesday morning.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where does Troy issue tweets from?\n",
      "Prediction:  Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island\n",
      "True Answer: Haiti\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What are the couple?\n",
      "Prediction: \n",
      "True Answer: country directors\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who is reputed leader of klan?\n",
      "Prediction: \n",
      "True Answer: Raymond \"Chuck\" Foster\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where is Cynthia Lynch from?\n",
      "Prediction:  Czechoslovakia\n",
      "True Answer: Tulsa, Oklahoma.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the KKK?\n",
      "Prediction:  KKK?</s></s>One of the Marines shown in a famous World War II photograph raising the U.S. flag on Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen,\n",
      "True Answer: Ku Klux Klan\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What are Americans doing as a way to cope with tough economic times?\n",
      "Prediction: \n",
      "True Answer: bartering\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When does bartering rise dramatically?\n",
      "Prediction: \n",
      "True Answer: \"When the economy turns unfriendly,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is the barter network president?\n",
      "Prediction: Jonathan Scharfen,\n",
      "True Answer: Michael Krane,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does bartering involve?\n",
      "Prediction:  the flag-raising\n",
      "True Answer: trading goods and services without exchanging money\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does barterng involve?\n",
      "Prediction:  raising the U.S. flag on Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia,\n",
      "True Answer: trading goods and services without exchanging money\n",
      "Exact match: False\n",
      "F1 score: 0.04\n",
      "\n",
      "Question: What are Americans doing to cope with tough economic times?\n",
      "Prediction: What are Americans doing to cope with tough economic times?</s></s>One of the Marines shown in a famous World War II photograph raising the U.S. flag on Iwo Jima\n",
      "True Answer: bartering\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who is not  considered an African-American woman for high court?\n",
      "Prediction: Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister,\n",
      "True Answer: Elena Kagan\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what are they ticked off ab out\n",
      "Prediction:  never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "True Answer: nomination of Elena Kagan to fill the seat of retiring Supreme Court Justice John Paul\n",
      "Exact match: False\n",
      "F1 score: 0.04\n",
      "\n",
      "Question: What has ticked off a number of leaders?\n",
      "Prediction: Jonathan Scharfen,\n",
      "True Answer: nomination of Elena Kagan to fill the seat of retiring Supreme Court Justice John Paul\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what woman has he considered\n",
      "Prediction: \n",
      "True Answer: Elena Kagan\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does the draft report say about Iran?\n",
      "Prediction: \n",
      "True Answer: could be secretly working on a nuclear weapon\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What could Iran be secretly working on?\n",
      "Prediction: \n",
      "True Answer: nuclear weapon\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who said the report is a major development?\n",
      "Prediction:  U.S. Citizenship and Immigration Services\n",
      "True Answer: Fareed Zakaria.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what did zakaria say\n",
      "Prediction: \n",
      "True Answer: \"To be casually talking about military action because we're getting frustrated seems to me somewhat dangerous,\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is not on the side of the Iranian regime?\n",
      "Prediction:  Iranian regime?</s></s>One of the Marines shown in a famous World War II photograph raising the U.S. flag on Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "True Answer: Iran's Green Movement of protesters\n",
      "Exact match: False\n",
      "F1 score: 0.01\n",
      "\n",
      "Question: What does he talk of against Iran?\n",
      "Prediction: \n",
      "True Answer: military strike\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what was the report about\n",
      "Prediction:  Strank's younger sister, Mary Pero.\n",
      "True Answer: Iran could be secretly working on a nuclear weapon\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When was Ma Khin Khin Leh sentenced to life in prison?\n",
      "Prediction:  March 1, 1945,\n",
      "True Answer: July 1999,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many prisoners were freed from Myanmar?\n",
      "Prediction:  five\n",
      "True Answer: Nineteen\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where was leader Aung San Suu Kyi confined to?\n",
      "Prediction:  Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935.\n",
      "True Answer: in her home\n",
      "Exact match: False\n",
      "F1 score: 0.06\n",
      "\n",
      "Question: Number of politicl prisoners freed in Myanmar?\n",
      "Prediction: Number of politicl prisoners freed in Myanmar?</s></s>One of the Marines shown in a famous World War II photograph raising the U.S. flag on Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank,\n",
      "True Answer: Nineteen\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where were the school teacher and 18 other political prisoners freed from?\n",
      "Prediction:  Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "True Answer: Myanmar\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is still confined to home?\n",
      "Prediction: \n",
      "True Answer: Aung San Suu Kyi\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who was confined to his home?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Aung San Suu Kyi\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Why was Ma Khin Leh sentenced to life?\n",
      "Prediction:  U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "\n",
      "He hailed Strank as a true American hero and a wonderful example of the remarkable contribution and sacrifices that immigrants have made to our great republic throughout its history.\n",
      "True Answer: because her husband, a student activist, had helped plan a protest demonstration in Bago in July 1999,\n",
      "Exact match: False\n",
      "F1 score: 0.01\n",
      "\n",
      "Question: Who is the pro-democracy leader confined to her home in Myanmar?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Aung San Suu Kyi\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who wa sentenced to life?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Ma Khin Khin Leh,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who was sentenced to life?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Ma Khin Khin Leh,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is among the 19 prisoners?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Ma Khin Khin Leh,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is being planned?\n",
      "Prediction:  was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia,\n",
      "True Answer: tallest building,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What will reach over a kilometer in height?\n",
      "Prediction:  a kilometer\n",
      "True Answer: tallest building,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: a new generation of what?\n",
      "Prediction: \n",
      "True Answer: innovative, exciting skyscrapers\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Which towers have good designs in the US?\n",
      "Prediction: Which towers\n",
      "True Answer: highlight of the rebuilt World Trade Center.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What height will Kingdom City be?\n",
      "Prediction: \n",
      "True Answer: over a kilometer (3,281 feet)\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What will the height of Kingdom City be?\n",
      "Prediction: \n",
      "True Answer: over a kilometer (3,281 feet) high.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many hostages did the Farc hold?\n",
      "Prediction:  between Japanese and U.S. forces\n",
      "True Answer: 750\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who was kidnapped August 4?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Oscar Tulio Lizcano\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does FARC stand for?\n",
      "Prediction:  Czechoslovakia\n",
      "True Answer: Revolutionary Armed Forces of Colombia,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who kidnapped the ex-congressman?\n",
      "Prediction:  an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen,\n",
      "True Answer: leftist rebels\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: In what country are the estimated 750 hostages being held?\n",
      "Prediction:  Czechoslovakia\n",
      "True Answer: Colombia.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When was Lizcano kidnapped?\n",
      "Prediction:  March 1, 1945,\n",
      "True Answer: August 4, 2000\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many hostages does the FARC hold?\n",
      "Prediction:  was killed in action on the island on March 1,\n",
      "True Answer: 750\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Lizcano fled how long ago?\n",
      "Prediction:  March 1, 1945,\n",
      "True Answer: about three days\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who was kidnapped on August 4, 2000?\n",
      "Prediction: Strank\n",
      "True Answer: Oscar Tulio Lizcano\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What was the value of the fortune Daniel Radcliffe received when he turned 18?\n",
      "Prediction:  U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "\n",
      "He hailed Strank as a true American hero and a wonderful example of the remarkable contribution and sacrifices that immigrants have made to our great republic throughout its history.\n",
      "True Answer: ¬£20 million\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What age is Daniel?\n",
      "Prediction:  3,\n",
      "True Answer: 18,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where was the money held?\n",
      "Prediction:  Arlington, Virginia,\n",
      "True Answer: in a trust fund\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is he going to do?\n",
      "Prediction:  was posthumously awarded a certificate of U.S. citizenship\n",
      "True Answer: have some sort of party,\"\n",
      "Exact match: False\n",
      "F1 score: 0.17\n",
      "\n",
      "Question: What happens when Daniel Radcliffe turns 18?\n",
      "Prediction:  was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "True Answer: gains access to a reported ¬£20 million\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Which earnings were held?\n",
      "Prediction: \n",
      "True Answer: ¬£20 million\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who stars in Harry Potter?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Daniel Radcliffe\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What day is Daniel Radcliffe's birthday?\n",
      "Prediction:  Tuesday\n",
      "True Answer: Monday,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who inherits ¬£20M on Monday?\n",
      "Prediction: \n",
      "True Answer: Daniel Radcliffe\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is he saving for?\n",
      "Prediction:  father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "True Answer: books\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What happened to all the money Radcliffe made from the Harry Potter movies?\n",
      "Prediction:  a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "True Answer: held in a trust fund\n",
      "Exact match: False\n",
      "F1 score: 0.01\n",
      "\n",
      "Question: Where did the money in the trust fund come from?\n",
      "Prediction:  Czechoslovakia\n",
      "True Answer: the first five Potter films\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is Radcliffe's net worth?\n",
      "Prediction:  Czechoslovakia\n",
      "True Answer: ¬£20 million\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What are his plans with the money?\n",
      "Prediction:  was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "True Answer: books\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is the star of Harry Potter?\n",
      "Prediction: \n",
      "True Answer: Daniel Radcliffe\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What amount of money is Daniel getting?\n",
      "Prediction:  a certificate of U.S. citizenship\n",
      "True Answer: ¬£20 million\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what did the young actor say?\n",
      "Prediction: \n",
      "True Answer: \"I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is search of the temple expected to turn up?\n",
      "Prediction: \n",
      "True Answer: possible victims of physical and sexual abuse.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where is Jeffs in jail awaiting trial?\n",
      "Prediction:  Czechoslovakia\n",
      "True Answer: Texas\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the number of children removed from ranch?\n",
      "Prediction: \n",
      "True Answer: 137\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: In which state is Jeffs jailed while awaiting trial?\n",
      "Prediction:  Virginia\n",
      "True Answer: Arizona\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many children were removed?\n",
      "Prediction:  five\n",
      "True Answer: 137\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the number of people removed from ranch?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: 183\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many children were removed from the ranch?\n",
      "Prediction:  five\n",
      "True Answer: 137\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many people were removed?\n",
      "Prediction: \n",
      "True Answer: 183\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many years was Jeffs sentenced to prison last year?\n",
      "Prediction: \n",
      "True Answer: 10\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What state jail is Jeffs waiting trial in?\n",
      "Prediction:  CIS\n",
      "True Answer: Utah\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What instrument does she play?\n",
      "Prediction: \n",
      "True Answer: piano\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Whose new album is she producing?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: \"Quiet Nights,\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is Diana Krall's new album?\n",
      "Prediction: \n",
      "True Answer: \"Quiet Nights,\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what bridge did Monet draw?\n",
      "Prediction: \n",
      "True Answer: London's Waterloo\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what room did Monet stay in at the Savoy?\n",
      "Prediction:  Savoy?</s></s>One of the Marines shown in a famous World War II photograph raising the U.S. flag on Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935.\n",
      "True Answer: 618\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: From which room was Monet's Waterloo Bridge painted?\n",
      "Prediction: \n",
      "True Answer: 618\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what hotel did Monet stay at?\n",
      "Prediction:  the Marine Corps Memorial\n",
      "True Answer: Savoy\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Whose drawing is on display at the Savoy?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Claude Monet\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When did the crash happen?\n",
      "Prediction:  March 1, 1945,\n",
      "True Answer: Monday\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what is the number of passengers?\n",
      "Prediction:  five\n",
      "True Answer: 82\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what amount of bodies have been found so far?\n",
      "Prediction:  five\n",
      "True Answer: 14\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where was black box located?\n",
      "Prediction:  Arlington, Virginia,\n",
      "True Answer: at a depth of about 1,300 meters in the Mediterranean Sea.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What was found?\n",
      "Prediction:  Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "\n",
      "He hailed Strank as a true American hero and a wonderful example of the remarkable contribution and sacrifices that immigrants have made to our great republic throughout its history.\n",
      "True Answer: flight data recorder from an Ethiopian Airlines plane\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many bodies were recovered?\n",
      "Prediction:  five\n",
      "True Answer: 14\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where was the flight going?\n",
      "Prediction:  Arlington, Virginia,\n",
      "True Answer: Addis Ababa,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: where was the back box found?\n",
      "Prediction:  U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia,\n",
      "True Answer: at a depth of about 1,300 meters in the Mediterranean Sea.\n",
      "Exact match: False\n",
      "F1 score: 0.11\n",
      "\n",
      "Question: What did 26 year old return from doing?\n",
      "Prediction:  U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them\n",
      "True Answer: after giving birth to baby daughter Jada,\n",
      "Exact match: False\n",
      "F1 score: 0.03\n",
      "\n",
      "Question: How long did the final last and what was the score\n",
      "Prediction:  less than a month\n",
      "True Answer: 6-2 6-1\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did Belgian defeat American in?\n",
      "Prediction:  World War II photograph\n",
      "True Answer: final of the Sony Ericsson Open\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who did Kim demolish?\n",
      "Prediction: \n",
      "True Answer: Venus Williams\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who beat williams\n",
      "Prediction: Strank and five other men\n",
      "True Answer: Kim\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what was the score\n",
      "Prediction:  certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "True Answer: 6-2 6-1\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many titles has the winner of the tournament collected on the tour\n",
      "Prediction:  five others\n",
      "True Answer: third\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Which player did Venue Williams lose to in the Miami final\n",
      "Prediction:  U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island\n",
      "True Answer: Kim\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who had a baby\n",
      "Prediction:  Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "True Answer: Kim\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who is al-moayad\n",
      "Prediction:  shown in a famous World War II photograph\n",
      "True Answer: a Yemeni cleric\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did the court say?\n",
      "Prediction:  the citizenship certificate Tuesday.\n",
      "\n",
      "He hailed Strank as a true American hero and\n",
      "True Answer: not receive a fair trial.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who did al-Moayad boast about giving money to?\n",
      "Prediction: \n",
      "True Answer: al Qaeda leader Osama bin Laden.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who supported terrorism?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Sheik Mohammed Ali al-Moayad and Mohammed Mohsen Zayed,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who gave money to Osama bin Laden?\n",
      "Prediction: Jonathan Scharf\n",
      "True Answer: al-Moayad\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What denied the pair a fair trial?\n",
      "Prediction:  Japanese and U.S. forces\n",
      "True Answer: prosecutors\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what does court say\n",
      "Prediction:  that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "True Answer: they did not receive a fair trial.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who was convicted of supporting terrorism?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Sheik Mohammed Ali al-Moayad and Mohammed Mohsen Zayed,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who is Khalid Sheikh Mohammed ?\n",
      "Prediction: \n",
      "True Answer: the mastermind behind the September 11, 2001, terrorist attacks on the United States.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What happened to Khalid Sheikh Mohammed?\n",
      "Prediction:  was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "True Answer: was waterboarded 183 times in a month,\n",
      "Exact match: False\n",
      "F1 score: 0.05\n",
      "\n",
      "Question: What did former CIA officer say?\n",
      "Prediction: He hailed Strank as a true American hero\n",
      "True Answer: Kiriakou said Zubaydah had been waterboarded for \"about 30 seconds, 35 seconds\" and agreed to cooperate with interrogators the following day.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who denounced the decision?\n",
      "Prediction:  U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen,\n",
      "True Answer: Hayden\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who did denounces decision to release memos?\n",
      "Prediction: \n",
      "True Answer: Hayden\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where did the passenger purchase airline tickets from?\n",
      "Prediction:  Czechoslovakia\n",
      "True Answer: Expedia.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did the passenger purchase?\n",
      "Prediction:  U.S. flag\n",
      "True Answer: two tickets to Italy\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What travel company issued a corrected ticket?\n",
      "Prediction:  U.S. Citizenship and Immigration Services\n",
      "True Answer: Expedia.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What was misspelled when received the tickets?\n",
      "Prediction:  citizenship papers.\n",
      "True Answer: his wife's name,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who faces 22 felony counts in connection with sex tape?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Chester Arthur Stiles,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who discovered the tape?\n",
      "Prediction:  U.S. Citizenship and Immigration Services\n",
      "True Answer: Darrin Tuck,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the evidence that shows a girl under 3 being assaulted?\n",
      "Prediction: \n",
      "True Answer: videotape\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many felony counts does Stiles face?\n",
      "Prediction:  five\n",
      "True Answer: 22\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What group of people know about the case and hae strong feelings about it?\n",
      "Prediction:  U.S. Citizenship and Immigration Services\n",
      "True Answer: a jury\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did the tape show?\n",
      "Prediction:  U.S. flag\n",
      "True Answer: images of the small girl being sexually assaulted.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many felony counts is Arthur Stiles facing?\n",
      "Prediction:  five\n",
      "True Answer: 22\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who was charged with slaying the woman?\n",
      "Prediction:  Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "\n",
      "He hailed Strank\n",
      "True Answer: Philip Markoff, a pre-med student at Boston University\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who makes public plea?\n",
      "Prediction:  Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "\n",
      "He hailed Strank as a true American hero and a wonderful example of the remarkable contribution and sacrifices that immigrants have made to our great republic throughout its history.\n",
      "True Answer: Boston Police Department,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who was charged\n",
      "Prediction:  Japanese and U.S. forces there\n",
      "True Answer: Philip Markoff,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what was he charged with\n",
      "Prediction:  came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "True Answer: murder\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What else was the suspect charged with?\n",
      "Prediction:  a certificate of U.S. citizenship on Tuesday.\n",
      "True Answer: the armed robbery and kidnapping of another victim,\n",
      "Exact match: False\n",
      "F1 score: 0.15\n",
      "\n",
      "Question: what did the police say\n",
      "Prediction:  was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "True Answer: said. \"It appears that there was a struggle between the victim and the suspect in the threshold of the hotel room immediately prior to the shooting,\"\n",
      "Exact match: False\n",
      "F1 score: 0.09\n",
      "\n",
      "Question: whom did police charge?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Philip Markoff,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: The suspect is how old?\n",
      "Prediction:  3,\n",
      "True Answer: 22-year-old\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does the columnist describe draper as?\n",
      "Prediction: \n",
      "True Answer: complicated man\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did columnist describe Draper as?\n",
      "Prediction: \n",
      "True Answer: sexy.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what is frustrating?\n",
      "Prediction: \n",
      "True Answer: the refusal or inability to \"turn it off\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What year was Bhutto's father hanged?\n",
      "Prediction:  1945\n",
      "True Answer: 1979\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What happened to Bhutto in October?\n",
      "Prediction: \n",
      "True Answer: narrowly escaped injury\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What happened to her father?\n",
      "Prediction:  was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "True Answer: hanged in 1979\n",
      "Exact match: False\n",
      "F1 score: 0.09\n",
      "\n",
      "Question: Who attempted to assassinate Bhutto in October?\n",
      "Prediction:  Marines\n",
      "True Answer: suicide bombing\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who was the first female prime minister of a Muslim country?\n",
      "Prediction:  female prime minister of a Muslim country?</s></s>One of the Marines shown in a famous World War II photograph raising the U.S. flag on Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank,\n",
      "True Answer: Bhutto,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who was hanged in 1979?\n",
      "Prediction: Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank\n",
      "True Answer: Zulfikar Ali Bhutto,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did Bhutto refuse to allow?\n",
      "Prediction:  citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "\n",
      "He hailed Strank as a true American hero and a wonderful example of the remarkable contribution and sacrifices that immigrants have made to our great republic throughout its history.\n",
      "True Answer: assassins\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is Georgia supposed to join?\n",
      "Prediction: \n",
      "True Answer: NATO\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what do european countries do\n",
      "Prediction: \n",
      "True Answer: expressed concerns about the missile defense system.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what did bush say\n",
      "Prediction: \n",
      "True Answer: A planned missile defense system in Eastern Europe poses no threat to Russia,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What do European countries share?\n",
      "Prediction: \n",
      "True Answer: Russian concerns that the defensive shield could be used for offensive aims.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: to where they carried the injured?\n",
      "Prediction:  Arlington, Virginia,\n",
      "True Answer: Nasser Medical Institute in Cairo,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many tourists are injured?\n",
      "Prediction:  five\n",
      "True Answer: 19\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is \"A Lion Among Men,\" about?\n",
      "Prediction: \n",
      "True Answer: -- the motherless cub defended by Elphaba in \"Wicked.\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did Maguire write for 14 years?\n",
      "Prediction: \n",
      "True Answer: children's books\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many Maguire books of Wicked has he sold?\n",
      "Prediction: \n",
      "True Answer: 2.5 million copies,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How long has Maguire been writing childrens's books?\n",
      "Prediction:  Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "True Answer: 14 years\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the name of Gregory's new book?\n",
      "Prediction:  U.S.\n",
      "True Answer: \"A Lion Among Men,\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many years did Maguire write children's books?\n",
      "Prediction:  Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "True Answer: 14\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many copies has \"Wicked\" sold?\n",
      "Prediction:  five\n",
      "True Answer: 2.5 million\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many times did the novel 'Wicked' sell?\n",
      "Prediction:  five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "True Answer: \"Wicked,\" has sold more than 2.5 million copies,\n",
      "Exact match: False\n",
      "F1 score: 0.03\n",
      "\n",
      "Question: What is Gregory Maguire's new book about?\n",
      "Prediction: \n",
      "True Answer: tells the story of the Cowardly Lion\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is wanted for questioning in the death of a two-year-old girl?\n",
      "Prediction:  Michael Strank,\n",
      "True Answer: Arthur\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: where was the child's body found?\n",
      "Prediction:  on the island\n",
      "True Answer: in a stream in Shark River Park in Monmouth County\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is wanted for questioning in the death of a two year old girl?\n",
      "Prediction: \n",
      "True Answer: Arthur\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is he wanted for\n",
      "Prediction: \n",
      "True Answer: questioning in the death of a two-year-old girl,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where was the body discovered\n",
      "Prediction:  on the island\n",
      "True Answer: in a stream in Shark River Park\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: The child was last seen with who?\n",
      "Prediction: \n",
      "True Answer: Arthur\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who is victor?\n",
      "Prediction:  American\n",
      "True Answer: Manuel Mejia Munera was a drug lord with ties to paramilitary groups,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who did the defense minister issue a warning to ?\n",
      "Prediction:  his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "True Answer: drug traffickers\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What was found with a dead drug lord ?\n",
      "Prediction:  a certificate\n",
      "True Answer: short- and long-range weapons\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where was Manuel Munera on the wanted list ?\n",
      "Prediction:  Czechoslovakia\n",
      "True Answer: Colombia's\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did the government find?\n",
      "Prediction: \n",
      "True Answer: identity documents\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What title did Eikenberry hold whilst he served time in Kabul\n",
      "Prediction:  of U.S. citizenship\n",
      "True Answer: U.S. security coordinator\n",
      "Exact match: False\n",
      "F1 score: 0.33\n",
      "\n",
      "Question: When did Eikenberry advise the secretary of defense?\n",
      "Prediction:  March 1, 1945,\n",
      "True Answer: for strategy, plans and policy on the Army staff.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: After how many years service did Eikenberry retire from the army\n",
      "Prediction: \n",
      "True Answer: nearly 40\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did Eikenberry do?\n",
      "Prediction:  derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945,\n",
      "True Answer: sent private cables to Obama last week,\n",
      "Exact match: False\n",
      "F1 score: 0.02\n",
      "\n",
      "Question: what did he advise US secretey of defense\n",
      "Prediction:  citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "True Answer: China, Taiwan, Hong Kong and Mongolia,\n",
      "Exact match: False\n",
      "F1 score: 0.02\n",
      "\n",
      "Question: Name the four countries that he advised the US secretary of defense on\n",
      "Prediction:  four\n",
      "True Answer: China, Taiwan, Hong Kong and Mongolia,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who retired from the army\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Eikenberry\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is Apples iphone 4s voice assistant feature called?\n",
      "Prediction:  iphone 4s voice assistant\n",
      "True Answer: Siri.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What was siri based on?\n",
      "Prediction:  World War II photograph raising the U.S. flag\n",
      "True Answer: artificial intelligence.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is siri based on\n",
      "Prediction:  World War II photograph\n",
      "True Answer: onstage demos.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When did apple acquire siri\n",
      "Prediction: \n",
      "True Answer: April 2010.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the name of the voice-assistant?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Siri\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What station did the train leave from?\n",
      "Prediction:  Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia,\n",
      "True Answer: Liverpool Street\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What number of suspects are in the videos?\n",
      "Prediction: \n",
      "True Answer: three\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where does the train leave?\n",
      "Prediction:  on the island\n",
      "True Answer: Liverpool Street Station\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did the jurors see?\n",
      "Prediction:  in a famous World War II photograph raising the U.S. flag on Iwo Jima\n",
      "True Answer: transit bombings\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did videos also show?\n",
      "Prediction:  U.S. flag\n",
      "True Answer: the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does the video show?\n",
      "Prediction:  U.S. flag\n",
      "True Answer: transit bombings\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What will the jurors see?\n",
      "Prediction: \n",
      "True Answer: Videos of the chaos and horrified reactions after the July 7, 2005, London\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What number of men were charged with conspiracy?\n",
      "Prediction:  five\n",
      "True Answer: three\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does the painting show?\n",
      "Prediction:  the flag-raising\n",
      "True Answer: Picasso's muse and mistress, Marie-Therese Walter.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What was the previous record?\n",
      "Prediction: \n",
      "True Answer: $104,327,006\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How much did the painting sell for?\n",
      "Prediction:  World War II\n",
      "True Answer: $106,482,500\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What painting sold for $106.482,500?\n",
      "Prediction: Sgt. Michael Strank\n",
      "True Answer: \"Nude, Green Leaves and Bust\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who was Picasso's muse?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Marie-Therese Walter.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where is the facility?\n",
      "Prediction:  Arlington, Virginia,\n",
      "True Answer: in Salt Lake City,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did FAA say about the situation?\n",
      "Prediction: \n",
      "True Answer: \"The\n",
      "Exact match: True\n",
      "F1 score: 1\n",
      "\n",
      "Question: where is the problem\n",
      "Prediction:  never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "True Answer: a Federal Aviation Administration facility,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: where is all flight-plan information processed\n",
      "Prediction:  Arlington, Virginia,\n",
      "True Answer: through a facility in Salt Lake City, Utah,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what does faa say\n",
      "Prediction: \n",
      "True Answer: \"The\n",
      "Exact match: True\n",
      "F1 score: 1\n",
      "\n",
      "Question: Where is flight plan information processed?\n",
      "Prediction:  Arlington, Virginia,\n",
      "True Answer: Salt Lake City, Utah,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the problem the facility is having?\n",
      "Prediction: \n",
      "True Answer: processing data,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is there an unknown number of?\n",
      "Prediction:  U.S. Citizenship and Immigration Services recently discovered that Strank\n",
      "True Answer: flights affected\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What problem were FAA having?\n",
      "Prediction:  never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "\n",
      "He hailed Strank as a true American hero and a wonderful example of the remarkable contribution and sacrifices that immigrants have made to our great republic throughout its history.\n",
      "True Answer: communications breakdown\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What has deteriorated this year?\n",
      "Prediction: Strank was killed in action on the island\n",
      "True Answer: the peace with Israel\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: In what year was the president murder?\n",
      "Prediction: \n",
      "True Answer: 1981,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who was assasinated?\n",
      "Prediction: \n",
      "True Answer: President Mohamed Anwar al-Sadat\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What happened in October 1981?\n",
      "Prediction: Strank\n",
      "True Answer: assassination of\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who has Sadt's daughter implicated?\n",
      "Prediction: \n",
      "True Answer: Mubarak,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What amount many members of Zoe's Ark are under arrest?\n",
      "Prediction: \n",
      "True Answer: Six\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What president wants the journalists and flight crew released\n",
      "Prediction: \n",
      "True Answer: Chadian\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: WHAT DOES THE CHADIAN PRESIDENT WANT?\n",
      "Prediction:  was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "True Answer: journalists and the flight crew will be freed,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who was arrested in Chad?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Six members of Zoe's Ark\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is under arrest in Chad?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: But the four women and three men are\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is happening in Chad?\n",
      "Prediction: \n",
      "True Answer: Hundreds of women protest\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the Chadian president's reaction?\n",
      "Prediction:  the battle between Japanese and U.S. forces there ended.\n",
      "True Answer: Idriss Deby hopes the journalists and the flight crew\n",
      "Exact match: False\n",
      "F1 score: 0.13\n",
      "\n",
      "Question: Where are the children from?\n",
      "Prediction:  Czechoslovakia\n",
      "True Answer: Chad\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who wants journalists, flight crew released?\n",
      "Prediction: \n",
      "True Answer: Chadian President Idriss Deby\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What number were arrested in Chad\n",
      "Prediction:  five\n",
      "True Answer: Six members\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: few were arrested in chad?\n",
      "Prediction: Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank\n",
      "True Answer: Six members of Zoe's Ark\n",
      "Exact match: False\n",
      "F1 score: 0.02\n",
      "\n",
      "Question: What is the Chadian president asking for?\n",
      "Prediction:  a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "\n",
      "He hailed Strank as a true American hero and a wonderful example of the remarkable contribution and sacrifices that immigrants have made to our great republic throughout its history.\n",
      "True Answer: journalists and the flight crew will be freed,\n",
      "Exact match: False\n",
      "F1 score: 0.02\n",
      "\n",
      "Question: Where did they try to fly from\n",
      "Prediction:  Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia,\n",
      "True Answer: Chad\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who interviewed the children that tried to fly out of Chad?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: The Red Cross, UNHCR and UNICEF\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does the president want?\n",
      "Prediction:  want?</s></s>One of the Marines shown in a famous World War II photograph raising the U.S. flag on Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "True Answer: hopes the journalists and the flight crew will be freed,\n",
      "Exact match: False\n",
      "F1 score: 0.02\n",
      "\n",
      "Question: Which charity organisations have been interviewing children?\n",
      "Prediction: \n",
      "True Answer: Red Cross, UNHCR and UNICEF\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: WHO ARE UNDER ARREST IN CHAD?\n",
      "Prediction:  Strank's younger sister, Mary Pero.\n",
      "True Answer: Three French journalists, a seven-member Spanish flight crew and one Belgian\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: where was the blast\n",
      "Prediction:  Arlington, Virginia,\n",
      "True Answer: a municipal building in Baghdad's Sadr City,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who was killed in the Mosul suicide bomb?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: two soldiers and two civilians from the Defense and State departments\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many americans died\n",
      "Prediction:  five other men\n",
      "True Answer: Four\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many were killed in the bombing?\n",
      "Prediction:  five\n",
      "True Answer: Four\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who was killed in Mosul?\n",
      "Prediction: Strank\n",
      "True Answer: two soldiers and two civilians from the Defense and State departments\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who did the victim work for?\n",
      "Prediction:  Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS,\n",
      "True Answer: U.S. Defense Department\n",
      "Exact match: False\n",
      "F1 score: 0.03\n",
      "\n",
      "Question: Who was shot in California?\n",
      "Prediction: Strank\n",
      "True Answer: Samuel Herr,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where is Daniel Wozniak held without bail?\n",
      "Prediction:  the Marine Corps Memorial\n",
      "True Answer: California\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Police says he shot Herr at California training base for what?\n",
      "Prediction:  raising the U.S. flag\n",
      "True Answer: financial gain,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is charged with murder?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Daniel Wozniak,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is charged with two counts of murder?\n",
      "Prediction:  Michael Strank,\n",
      "True Answer: Daniel Wozniak,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is Daniel Wozniak charged with?\n",
      "Prediction: \n",
      "True Answer: two counts of murder.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Wozniak is being held where?\n",
      "Prediction:  Arlington, Virginia,\n",
      "True Answer: Costa Mesa Police Department\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where did Daniel Wozniak shoot Samuel Herr?\n",
      "Prediction:  on the island\n",
      "True Answer: Los Alamitos Joint Forces Training Base\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who got engaged to Ryan Adams?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Mandy Moore\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is Moore famous for?\n",
      "Prediction:  World War II photograph\n",
      "True Answer: role as a bride in the 2007 movie \"License to Wed\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is Moore better known for?\n",
      "Prediction:  World War II photograph raising the U.S. flag on Iwo Jima\n",
      "True Answer: success as a recording artist\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is she engaged to?\n",
      "Prediction:  Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "\n",
      "He hailed Strank as a true American hero and a wonderful example of the remarkable contribution and sacrifices that immigrants have made to our great republic throughout its history.\n",
      "True Answer: Ryan Adams.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: where Site raised $17,000 before crashing on Tuesday due to high volume?\n",
      "Prediction:  Arlington, Virginia,\n",
      "True Answer: City of Los Angeles' Web\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many police were on hand?\n",
      "Prediction:  five\n",
      "True Answer: Three thousand\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Costs include what?\n",
      "Prediction:  Czechoslovakia\n",
      "True Answer: putting extra police on the streets, trash pickup, sanitation, traffic control and more for the Tuesday event,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many cops were at the event?\n",
      "Prediction:  five\n",
      "True Answer: Three thousand\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did costs include?\n",
      "Prediction:  the flag-raising\n",
      "True Answer: putting extra police on the streets, trash pickup, sanitation, traffic control and more\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Site raised how much money before crashing?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: $17,000\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what City set up Web page asking Jackson fans to donate money?\n",
      "Prediction: \n",
      "True Answer: Los Angeles'\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How much money was raised?\n",
      "Prediction:  U.S. flag on Iwo Jima\n",
      "True Answer: $17,000\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Which political party did Nepal belong to?\n",
      "Prediction:  Czechoslov\n",
      "True Answer: Communist\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What was Nepal's old job?\n",
      "Prediction:  Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces\n",
      "True Answer: former general secretary of the Communist Party,\n",
      "Exact match: False\n",
      "F1 score: 0.02\n",
      "\n",
      "Question: When did Pushpa Kamal Dahal resign ?\n",
      "Prediction:  March 1, 1945,\n",
      "True Answer: May 4\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is Nepal's age?\n",
      "Prediction:  Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935.\n",
      "True Answer: 56,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who was the only candidate?\n",
      "Prediction: Sgt. Michael Strank, who\n",
      "True Answer: Madhav Kumar Nepal\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the age of Madhav  Kumar Nepal?\n",
      "Prediction:  3\n",
      "True Answer: 56,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who resigned as Prime Minister?\n",
      "Prediction: Jonathan Scharfen,\n",
      "True Answer: Pushpa Kamal Dahal, the Maoist chairman,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When did the Maoist chairman resign?\n",
      "Prediction:  February 23, 1945.\n",
      "True Answer: May 4\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What age is Madhav Nepal?\n",
      "Prediction:  3,\n",
      "True Answer: 56,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the Palm Jumeirah?\n",
      "Prediction:  World War II photograph raising the U.S. flag on Iwo Jima\n",
      "True Answer: A huge man-made island\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where is Palm Jumeirah island?\n",
      "Prediction:  Czechoslovakia\n",
      "True Answer: off the coast of Dubai\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who will be at the opening party?\n",
      "Prediction: \n",
      "True Answer: Oprah Winfrey, Michael Jordan, Robert De Niro, Janet Jackson\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When was the opening party?\n",
      "Prediction:  Tuesday\n",
      "True Answer: Thursday night.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where is Dubai?\n",
      "Prediction:  Czechoslovakia\n",
      "True Answer: Arab Emirates\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How much has the addition of the man-made island increased the Dubai coastline?\n",
      "Prediction: The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "True Answer: 100 percent\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the name of the man-made island?\n",
      "Prediction:  Iwo Jima\n",
      "True Answer: Palm Jumeirah\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where is the island located?\n",
      "Prediction:  Arlington, Virginia,\n",
      "True Answer: off the coast of Dubai\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many died in mall shooting?\n",
      "Prediction:  Michael Strank,\n",
      "True Answer: eight.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What are malls expected to assess?\n",
      "Prediction:  the flag-raising\n",
      "True Answer: their emergency plans\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many people did the gunman kill?\n",
      "Prediction:  five\n",
      "True Answer: eight.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What day did the shooting occur?\n",
      "Prediction:  March 1, 1945,\n",
      "True Answer: Wednesday.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many people did the gunman shoot?\n",
      "Prediction: \n",
      "True Answer: eight.\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What do security experts say abut such incidents?\n",
      "Prediction: \n",
      "True Answer: it's a matter of money.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is impossible to anticipate?\n",
      "Prediction: \n",
      "True Answer: murderous rampage\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where was the shooting?\n",
      "Prediction:  the island\n",
      "True Answer: Westroads Mall in Omaha, Nebraska,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What are malls expected to do in wake of Wednesday shooting?\n",
      "Prediction: \n",
      "True Answer: review their emergency plans and consider additional security measures\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did a gunman and Omaha Nebraska mall do?\n",
      "Prediction:  raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons\n",
      "True Answer: murderous rampage\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who opened civil rights investigation?\n",
      "Prediction:  U.S. Citizenship and Immigration Services\n",
      "True Answer: FBI's Baltimore field office\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is being held for the death of a police officer?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Ronnie White,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What type of investigation did the FBI open?\n",
      "Prediction:  U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "True Answer: civil rights\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who has been held following the death of police officer?\n",
      "Prediction: \n",
      "True Answer: Ronnie White,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did Ronnie White die of?\n",
      "Prediction:  in action on the island\n",
      "True Answer: strangulation and asphyxiation\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where was Ronnie White?\n",
      "Prediction:  on the island\n",
      "True Answer: Prince George's County Correctional Center,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where was White being held?\n",
      "Prediction: \n",
      "True Answer: Prince George's County Correctional Center,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the name of the defendant?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Casey Anthony,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What doesn't help the mother's defense?\n",
      "Prediction:  citizenship papers\n",
      "True Answer: her alibi\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What could hinder the prosecution?\n",
      "Prediction: \n",
      "True Answer: lack of a cause of death and the absence of any soft tissue on the toddler's skeletal remains\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When did the toddler vanish?\n",
      "Prediction:  March 1, 1945,\n",
      "True Answer: last summer.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the cause of death?\n",
      "Prediction:  March 1, 1945,\n",
      "True Answer: no evidence\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who vanished last summer?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Caylee Anthony's\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f1=0\n",
    "for contexts, question, answer in zip(valid_contexts[:], valid_questions[:], valid_str_ans[:]):\n",
    "    f1 += question_answer(context, question, answer)\n",
    "avg_f1_score=f1/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "78a47788",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T22:18:52.588471Z",
     "iopub.status.busy": "2023-10-20T22:18:52.587973Z",
     "iopub.status.idle": "2023-10-20T22:18:52.593077Z",
     "shell.execute_reply": "2023-10-20T22:18:52.592228Z"
    },
    "papermill": {
     "duration": 10.103032,
     "end_time": "2023-10-20T22:18:52.595548",
     "exception": false,
     "start_time": "2023-10-20T22:18:42.492516",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 score=0.009529999999999995\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average F1 score={avg_f1_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d14d0c",
   "metadata": {
    "papermill": {
     "duration": 10.005658,
     "end_time": "2023-10-20T22:19:12.341900",
     "exception": false,
     "start_time": "2023-10-20T22:19:02.336242",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 13780.428647,
   "end_time": "2023-10-20T22:19:25.401123",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-20T18:29:44.972476",
   "version": "2.4.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "01a26d79e55d421fb43f6defa4c5f8bd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "04dc0ba166c44ae294b061ad50ad854d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "05d402d8e3a8473cbfdfbc50ccb2bdfe": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0b6f42295d4e418a95d4a20c5c51eb48": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0bbbad4af628425b8f4a384e49574214": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_9fa50fec931e44b3b1eddb1cf7d98deb",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_fe61ccfc4d254c959971eb5fa72e0fe8",
       "value": " 496M/496M [00:01&lt;00:00, 308MB/s]"
      }
     },
     "0d923558eb55431b94b9ea4d9549282d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "0f40bdf541a74372a07a3b443d527921": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "123c11bd6acb4ef4a2de4ec312764ec8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "126b9a594f74489a966bcb73090675d2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "12c9e76379244ff6b6ee489a536c1ae2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_9273324df1e140b3902eb34212f65b4a",
        "IPY_MODEL_793e87ce32f542a1bfc1113a2fa6ad16",
        "IPY_MODEL_50f9c75683b243be822c545423a9036f"
       ],
       "layout": "IPY_MODEL_d11edc885acf4c539e2ed65d6851760d"
      }
     },
     "1387ea9a6ab945a8bcb4269dc8c1a3d8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "14deffdd01a34d45817d37174fd26061": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f338015bf8f54916b4b1d519c7decbb1",
       "max": 456318.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_d29f1acac95c4a9dbdb5a0c6c4d83cdd",
       "value": 456318.0
      }
     },
     "153c7294cbf548aaa524e016d0c8105f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "16521a802f02406fb74972f886cdeaf1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "171c939e1e0647e08ed6112e1685337f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "17fb8eaff327492f943410a9edeb126e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "198d0fa59d5d4f6492a6c7ed1a300277": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1c6b32ea44f244b6bc203031e11ac141": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1e27d9782abd4f30beadf39f96bd2800": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_0b6f42295d4e418a95d4a20c5c51eb48",
       "max": 2.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_909fefb9c3fe4d2988be0dc926411b31",
       "value": 2.0
      }
     },
     "2121a1e89cdc4c28bf77d38b9782bcb0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_e308c91b352b422a84d6dce7f758cad8",
        "IPY_MODEL_9b74997551934071923ea833dbf0e7d7",
        "IPY_MODEL_76b6aa9f133d4e68b73d7d87c38e141b"
       ],
       "layout": "IPY_MODEL_7af7558ae85d49ca9b77253f8bacfed6"
      }
     },
     "21708070cae543339e9b0073846b4445": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "21cd1cd8c8dc466aa271affd1c53bef2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c48a2c4fad914ef8b113cc80546beca7",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_aeda6fb4303f4a8e831dde55f1563fa9",
       "value": " 1.63M/1.63M [00:00&lt;00:00, 4.54MB/s]"
      }
     },
     "23e30e622f4c40cdb01f19722eb5c5d5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_71038424226b4da181c836aad3e73325",
        "IPY_MODEL_ffa206a5878a46d6a97005b694a70932",
        "IPY_MODEL_8962df28178849a483d2aeca06f5999c"
       ],
       "layout": "IPY_MODEL_8f05879e879049e3835a3e71b5a68188"
      }
     },
     "36a0e7010033414aa60b32a50f7781a5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "389334d98cc447ad9323fbff5c4c2a81": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "39b218af2ce14a7a8b3d9d7f292cd053": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3b02c29e1a904593a060d4ca7fdb2664": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3c3764cba50244a4bc5fa876d847d317": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3d7aaaef9f674e519c909d7ba0dc3309": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_8afbfad051fb4c3d944318f365e583eb",
       "max": 2.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_1387ea9a6ab945a8bcb4269dc8c1a3d8",
       "value": 2.0
      }
     },
     "4133d3569a024b02b04cd4c5e7d4e676": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_a77eed877376470c88703772003b0384",
        "IPY_MODEL_1e27d9782abd4f30beadf39f96bd2800",
        "IPY_MODEL_e535472424a941cbb440e4d791eb5604"
       ],
       "layout": "IPY_MODEL_1c6b32ea44f244b6bc203031e11ac141"
      }
     },
     "4299b95a82ec4fa99c75612788295e47": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "4632ab6aec6f4a79805b16fd68382a83": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4a67a0ed5e3e4edfb4cbcc4153c29d86": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_dea107d26ebe4635b54675db8fe96110",
        "IPY_MODEL_f1558ba4b0c2413bb3dd5e990b319c41",
        "IPY_MODEL_f838d6997c1a4d53b8fe7e04992f90a4"
       ],
       "layout": "IPY_MODEL_4632ab6aec6f4a79805b16fd68382a83"
      }
     },
     "4ccd3cabcb714e7f8abe08335788cfdd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_05d402d8e3a8473cbfdfbc50ccb2bdfe",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_943d0288de1046bc84ddf91795bacdeb",
       "value": "Downloading (‚Ä¶)olve/main/merges.txt: 100%"
      }
     },
     "50f9c75683b243be822c545423a9036f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b08482322fe246d9a1d8b61a9ad1d80a",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_f0fbae40fe6046a490651a0bd7d6449f",
       "value": " 2/2 [00:00&lt;00:00, 110.52it/s]"
      }
     },
     "510ecd1ea0764bb6a60d80e6365f99c1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_17fb8eaff327492f943410a9edeb126e",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_7cd1fb4863654f20bd9e194751bda5ad",
       "value": "Downloading data files: 100%"
      }
     },
     "54a3f7f6498b4aa2bac38fbb96680c19": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "590ac3cd20a7414ab525fc6f4a0bb71a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_16521a802f02406fb74972f886cdeaf1",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_ed198e91870a48eabc4f953317c3cb37",
       "value": " 2/2 [00:02&lt;00:00,  1.04s/it]"
      }
     },
     "5978e552f248457aad61e85c3db54192": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_7a64a4c4caf34ee685d7eaa932a84ea6",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_b9a41f0c90b44293941b767c97afa348",
       "value": "Downloading model.safetensors: 100%"
      }
     },
     "5f6093f02ba645c58788f21e176ce607": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_4ccd3cabcb714e7f8abe08335788cfdd",
        "IPY_MODEL_14deffdd01a34d45817d37174fd26061",
        "IPY_MODEL_c70d01270c754578b415d350daf04432"
       ],
       "layout": "IPY_MODEL_94e194c282624e9181f6720dd37c16e2"
      }
     },
     "62681c5fecbe448482d5d32ac35607b9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "628b5f4258864095aedc9c73034a6ca5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_964f18e2a28942509284f41f4bdc882a",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_4299b95a82ec4fa99c75612788295e47",
       "value": " 772/772 [00:00&lt;00:00, 66.9kB/s]"
      }
     },
     "65d48b18c3354580929b1f1c0cf3df82": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "66966c57ff6c4ac2abece01c554f2779": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "6b0cb758d31c489cba05d54c74a9ef81": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_198d0fa59d5d4f6492a6c7ed1a300277",
       "max": 496254442.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_66966c57ff6c4ac2abece01c554f2779",
       "value": 496254442.0
      }
     },
     "6cf10018de434693b365c822100b3830": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3b02c29e1a904593a060d4ca7fdb2664",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_21708070cae543339e9b0073846b4445",
       "value": "Downloading (‚Ä¶)cial_tokens_map.json: 100%"
      }
     },
     "6ea8c878f5cb4cdaa5cdeb9cb5edb853": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_0f40bdf541a74372a07a3b443d527921",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_c62468d86ef34cfebcd7bf18655ae069",
       "value": "Downloading (‚Ä¶)okenizer_config.json: 100%"
      }
     },
     "71038424226b4da181c836aad3e73325": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f52ea1c0b98c4819b2357e488d948b42",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_7c862f23db064a17bad2580021bf3d1b",
       "value": "Downloading data: 100%"
      }
     },
     "7357e9e05821415bbed16d1ad846ab06": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "76b6aa9f133d4e68b73d7d87c38e141b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_65d48b18c3354580929b1f1c0cf3df82",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_db6a105729294c869000e61efcea8383",
       "value": " 899k/899k [00:00&lt;00:00, 4.66MB/s]"
      }
     },
     "76f780247f4d43759cc01829bbfb31c9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_510ecd1ea0764bb6a60d80e6365f99c1",
        "IPY_MODEL_3d7aaaef9f674e519c909d7ba0dc3309",
        "IPY_MODEL_590ac3cd20a7414ab525fc6f4a0bb71a"
       ],
       "layout": "IPY_MODEL_cd8acf4961304e2a9358e7471589b319"
      }
     },
     "793e87ce32f542a1bfc1113a2fa6ad16": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_04dc0ba166c44ae294b061ad50ad854d",
       "max": 2.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_171c939e1e0647e08ed6112e1685337f",
       "value": 2.0
      }
     },
     "7a64a4c4caf34ee685d7eaa932a84ea6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7af7558ae85d49ca9b77253f8bacfed6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7b6c3d075f9d4faf967c4c50bb2a6e31": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_6cf10018de434693b365c822100b3830",
        "IPY_MODEL_d2f285489b3a4a8192a75a607326d08c",
        "IPY_MODEL_628b5f4258864095aedc9c73034a6ca5"
       ],
       "layout": "IPY_MODEL_89fb64f95a3d438b9665f329403085e7"
      }
     },
     "7c862f23db064a17bad2580021bf3d1b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "7cd1fb4863654f20bd9e194751bda5ad": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "80bf269273ba4e05b762d29d6ac13788": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "8962df28178849a483d2aeca06f5999c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_be7b567957ba4ba7a763754555fb5e83",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_0d923558eb55431b94b9ea4d9549282d",
       "value": " 29.7M/29.7M [00:00&lt;00:00, 64.9MB/s]"
      }
     },
     "898cec359c49498188cf2e824a90e202": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "89fb64f95a3d438b9665f329403085e7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8afbfad051fb4c3d944318f365e583eb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8b011aa71e9844c89a648d20281ef3bd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_e858b161b6024f0e995bc417288fa191",
        "IPY_MODEL_dd9f12c209624a8f9e28d991b984cdb7",
        "IPY_MODEL_21cd1cd8c8dc466aa271affd1c53bef2"
       ],
       "layout": "IPY_MODEL_e3cdedc72e1c41cf8255e2f9a0d014e7"
      }
     },
     "8f05879e879049e3835a3e71b5a68188": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "909fefb9c3fe4d2988be0dc926411b31": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "9273324df1e140b3902eb34212f65b4a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_389334d98cc447ad9323fbff5c4c2a81",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_d5c268d27e2642e4bf8b5539f4c357f8",
       "value": "100%"
      }
     },
     "943d0288de1046bc84ddf91795bacdeb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "94e194c282624e9181f6720dd37c16e2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "964f18e2a28942509284f41f4bdc882a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9b74997551934071923ea833dbf0e7d7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_62681c5fecbe448482d5d32ac35607b9",
       "max": 898822.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_153c7294cbf548aaa524e016d0c8105f",
       "value": 898822.0
      }
     },
     "9fa50fec931e44b3b1eddb1cf7d98deb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a77eed877376470c88703772003b0384": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_db53a454ba0a45a19433e1eaaa7af87a",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_ff620766e30644f8b9a1dec0f4766025",
       "value": "Extracting data files: 100%"
      }
     },
     "a9f5d1431ba94b3cb6fb9406742a56b3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "aa910dccb0044b69a8e0d246293a07e8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "aeda6fb4303f4a8e831dde55f1563fa9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b08482322fe246d9a1d8b61a9ad1d80a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b9a41f0c90b44293941b767c97afa348": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "ba0e6f215e304de3b418e9328b274044": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "be472781b4f944ab8c6ad0b07ece237c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "be7b567957ba4ba7a763754555fb5e83": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c39743ac00a84d8aa6efb9dde1c8ad6b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d7bfac5e8c734c2bb406a165ed95abb9",
       "max": 79.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_eaa37cff36294a0699274a521a6b5564",
       "value": 79.0
      }
     },
     "c3c8000900dc41c9921f3496d868c141": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_6ea8c878f5cb4cdaa5cdeb9cb5edb853",
        "IPY_MODEL_c39743ac00a84d8aa6efb9dde1c8ad6b",
        "IPY_MODEL_f2189181359c4541a24f0a7bdbcf1316"
       ],
       "layout": "IPY_MODEL_d66ea56d12b94842a02342c9d7287016"
      }
     },
     "c4379747299f4e1c960cbf51045b7c3d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "c48a2c4fad914ef8b113cc80546beca7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c62468d86ef34cfebcd7bf18655ae069": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "c70d01270c754578b415d350daf04432": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3c3764cba50244a4bc5fa876d847d317",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_54a3f7f6498b4aa2bac38fbb96680c19",
       "value": " 456k/456k [00:00&lt;00:00, 35.8MB/s]"
      }
     },
     "c93628f61bf44954819632dde43570b6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "cd8acf4961304e2a9358e7471589b319": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d11edc885acf4c539e2ed65d6851760d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d29f1acac95c4a9dbdb5a0c6c4d83cdd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "d2f285489b3a4a8192a75a607326d08c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e0d5a1fb23dc474b8a39a2da26b8da56",
       "max": 772.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_aa910dccb0044b69a8e0d246293a07e8",
       "value": 772.0
      }
     },
     "d5c268d27e2642e4bf8b5539f4c357f8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "d66ea56d12b94842a02342c9d7287016": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d7bfac5e8c734c2bb406a165ed95abb9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "dae527cf78ba4a29824aaaa0a73d2f65": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "db53a454ba0a45a19433e1eaaa7af87a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "db6a105729294c869000e61efcea8383": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "dd9f12c209624a8f9e28d991b984cdb7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_36a0e7010033414aa60b32a50f7781a5",
       "max": 1633893.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_f916bca182a5409f96a8b8924fcb7fdc",
       "value": 1633893.0
      }
     },
     "dea107d26ebe4635b54675db8fe96110": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f837c19ccb794426ab2f7d35b2aed898",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_01a26d79e55d421fb43f6defa4c5f8bd",
       "value": "Downloading (‚Ä¶)lve/main/config.json: 100%"
      }
     },
     "e0d5a1fb23dc474b8a39a2da26b8da56": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e308c91b352b422a84d6dce7f758cad8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_123c11bd6acb4ef4a2de4ec312764ec8",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_c4379747299f4e1c960cbf51045b7c3d",
       "value": "Downloading (‚Ä¶)olve/main/vocab.json: 100%"
      }
     },
     "e3cdedc72e1c41cf8255e2f9a0d014e7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e4775580f1cf41bc92cc21abeff3e667": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e535472424a941cbb440e4d791eb5604": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_7357e9e05821415bbed16d1ad846ab06",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_c93628f61bf44954819632dde43570b6",
       "value": " 2/2 [00:00&lt;00:00, 142.81it/s]"
      }
     },
     "e858b161b6024f0e995bc417288fa191": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a9f5d1431ba94b3cb6fb9406742a56b3",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_be472781b4f944ab8c6ad0b07ece237c",
       "value": "Downloading data: 100%"
      }
     },
     "eaa37cff36294a0699274a521a6b5564": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "eb3e3229430b47a2a87e3de2f60c487c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ed198e91870a48eabc4f953317c3cb37": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "f0fbae40fe6046a490651a0bd7d6449f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "f1558ba4b0c2413bb3dd5e990b319c41": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e4775580f1cf41bc92cc21abeff3e667",
       "max": 571.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_126b9a594f74489a966bcb73090675d2",
       "value": 571.0
      }
     },
     "f1b2137e88224b4ab61bac774bf7053a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "f2189181359c4541a24f0a7bdbcf1316": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ba0e6f215e304de3b418e9328b274044",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_f1b2137e88224b4ab61bac774bf7053a",
       "value": " 79.0/79.0 [00:00&lt;00:00, 7.21kB/s]"
      }
     },
     "f338015bf8f54916b4b1d519c7decbb1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f52ea1c0b98c4819b2357e488d948b42": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f647949d462a4593b2c97fff91effc10": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_5978e552f248457aad61e85c3db54192",
        "IPY_MODEL_6b0cb758d31c489cba05d54c74a9ef81",
        "IPY_MODEL_0bbbad4af628425b8f4a384e49574214"
       ],
       "layout": "IPY_MODEL_39b218af2ce14a7a8b3d9d7f292cd053"
      }
     },
     "f837c19ccb794426ab2f7d35b2aed898": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f838d6997c1a4d53b8fe7e04992f90a4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_898cec359c49498188cf2e824a90e202",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_dae527cf78ba4a29824aaaa0a73d2f65",
       "value": " 571/571 [00:00&lt;00:00, 51.1kB/s]"
      }
     },
     "f916bca182a5409f96a8b8924fcb7fdc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "fe61ccfc4d254c959971eb5fa72e0fe8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "ff620766e30644f8b9a1dec0f4766025": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "ffa206a5878a46d6a97005b694a70932": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_eb3e3229430b47a2a87e3de2f60c487c",
       "max": 29694916.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_80bf269273ba4e05b762d29d6ac13788",
       "value": 29694916.0
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
