{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e917a29d",
   "metadata": {
    "id": "mI1zNQQ5WaGW",
    "papermill": {
     "duration": 0.011485,
     "end_time": "2023-10-22T09:41:23.221153",
     "exception": false,
     "start_time": "2023-10-22T09:41:23.209668",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Question Answering❓**\n",
    "with fine-tuned BERT on newsQA.  \n",
    "\n",
    "Question answering comes in many forms. We’ll look at the particular type of extractive QA that involves answering a question about a passage by highlighting the segment of the passage that answers the question. This involves fine-tuning a model which predicts a start position and an end position in the passage. More specifically, we will fine tune the [bert-base-uncased](https://huggingface.co/bert-base-uncased) model on the [NewsQA](https://huggingface.co/datasets/lucadiliello/newsqa) dataset.\n",
    "\n",
    "I have followed [this tutorial](https://github.com/angelosps/Question-Answering) from the for how to fine tune BERT on SQuAD 2.0 which in our case is a custom newsQA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e42d1c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-22T09:41:23.244240Z",
     "iopub.status.busy": "2023-10-22T09:41:23.243865Z",
     "iopub.status.idle": "2023-10-22T09:41:36.215817Z",
     "shell.execute_reply": "2023-10-22T09:41:36.214607Z"
    },
    "id": "NLAz1cug_MSc",
    "outputId": "62884f19-0941-4706-8777-876f09e53f3a",
    "papermill": {
     "duration": 12.986257,
     "end_time": "2023-10-22T09:41:36.218331",
     "exception": false,
     "start_time": "2023-10-22T09:41:23.232074",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.33.0)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.16.4)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.6.3)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\r\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\r\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.3.3)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.9.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.6.3)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.1.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d7f037a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-22T09:41:36.242557Z",
     "iopub.status.busy": "2023-10-22T09:41:36.241585Z",
     "iopub.status.idle": "2023-10-22T09:41:49.343990Z",
     "shell.execute_reply": "2023-10-22T09:41:49.343102Z"
    },
    "id": "NbYipki39qLw",
    "papermill": {
     "duration": 13.116739,
     "end_time": "2023-10-22T09:41:49.346491",
     "exception": false,
     "start_time": "2023-10-22T09:41:36.229752",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21c0d98d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-22T09:41:49.369767Z",
     "iopub.status.busy": "2023-10-22T09:41:49.368712Z",
     "iopub.status.idle": "2023-10-22T09:42:01.243903Z",
     "shell.execute_reply": "2023-10-22T09:42:01.242880Z"
    },
    "id": "mlYX0bLL_YcI",
    "outputId": "f648648f-5cdc-42ce-9495-da9cc41490ed",
    "papermill": {
     "duration": 11.889035,
     "end_time": "2023-10-22T09:42:01.246425",
     "exception": false,
     "start_time": "2023-10-22T09:41:49.357390",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.23.5)\r\n",
      "Requirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\r\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.7)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.0.2)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\r\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\r\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.3.0)\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.15)\r\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2023.9.0)\r\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.4)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.16.4)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\r\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.18.0)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (3.1.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.12.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.6.3)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.15)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.7.22)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70cc6bb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-22T09:42:01.270340Z",
     "iopub.status.busy": "2023-10-22T09:42:01.269692Z",
     "iopub.status.idle": "2023-10-22T09:42:04.473313Z",
     "shell.execute_reply": "2023-10-22T09:42:04.472417Z"
    },
    "id": "rxbbQQ-j9x6x",
    "outputId": "9fa4e963-6491-4461-bc22-ca8c5c3d1a2c",
    "papermill": {
     "duration": 3.217637,
     "end_time": "2023-10-22T09:42:04.475435",
     "exception": false,
     "start_time": "2023-10-22T09:42:01.257798",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset parquet/lucadiliello--newsqa to /root/.cache/huggingface/datasets/parquet/lucadiliello--newsqa-206550e86bcc3ded/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a490c040bbf148488b060c59652ca804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fce10c543fac44a5ba3f91ebe9353541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/29.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e35057ce8c454f43ac04f6021ceae519",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.63M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5be0000e71454caab0434066e700f6b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/parquet/lucadiliello--newsqa-206550e86bcc3ded/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fa4f8d52988469c8f1806c19c37bb15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the NewsQA dataset\n",
    "from datasets import load_dataset\n",
    "newsqa_dataset = load_dataset('lucadiliello/newsqa')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcf9a5e",
   "metadata": {
    "id": "65heqpOoXiRs",
    "papermill": {
     "duration": 0.012026,
     "end_time": "2023-10-22T09:42:04.500089",
     "exception": false,
     "start_time": "2023-10-22T09:42:04.488063",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### **Get data 📁**\n",
    "\n",
    "Let's extract our data and store them into some data structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8e4f0a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-22T09:42:04.526358Z",
     "iopub.status.busy": "2023-10-22T09:42:04.525732Z",
     "iopub.status.idle": "2023-10-22T09:42:04.532858Z",
     "shell.execute_reply": "2023-10-22T09:42:04.532040Z"
    },
    "id": "lks0zjIo90PS",
    "papermill": {
     "duration": 0.022783,
     "end_time": "2023-10-22T09:42:04.534956",
     "exception": false,
     "start_time": "2023-10-22T09:42:04.512173",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_newsqa_data(dataset):\n",
    "    contexts = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "    string_ans = []\n",
    "\n",
    "    for item in dataset:\n",
    "        context = item['context']\n",
    "        question = item['question']\n",
    "        answer = {'answer_start': item['labels'][0]['start'][0], 'answer_end': item['labels'][0]['end'][0]}  # Assuming there's only one answer\n",
    "        string_answer = item['answers'][0]\n",
    "        \n",
    "        contexts.append(context)\n",
    "        questions.append(question)\n",
    "        answers.append(answer)\n",
    "        string_ans.append(string_answer)\n",
    "    return contexts, questions, answers, string_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "831be5fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-22T09:42:04.559487Z",
     "iopub.status.busy": "2023-10-22T09:42:04.559231Z",
     "iopub.status.idle": "2023-10-22T09:42:05.466145Z",
     "shell.execute_reply": "2023-10-22T09:42:05.465278Z"
    },
    "id": "YEWSoAaM91rV",
    "papermill": {
     "duration": 0.922091,
     "end_time": "2023-10-22T09:42:05.468681",
     "exception": false,
     "start_time": "2023-10-22T09:42:04.546590",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_contexts, train_questions, train_answers, train_str_ans = read_newsqa_data(newsqa_dataset['train'].select(list(range(5000))))\n",
    "valid_contexts, valid_questions, valid_answers, valid_str_ans = read_newsqa_data(newsqa_dataset['validation'].select(list(range(1000))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "daa4a6c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-22T09:42:05.494515Z",
     "iopub.status.busy": "2023-10-22T09:42:05.493911Z",
     "iopub.status.idle": "2023-10-22T09:42:05.500525Z",
     "shell.execute_reply": "2023-10-22T09:42:05.499661Z"
    },
    "papermill": {
     "duration": 0.021318,
     "end_time": "2023-10-22T09:42:05.502408",
     "exception": false,
     "start_time": "2023-10-22T09:42:05.481090",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['19',\n",
       " 'February.',\n",
       " 'rape and murder',\n",
       " 'Moninder Singh Pandher',\n",
       " 'Moninder Singh Pandher']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_str_ans[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fb0400",
   "metadata": {
    "id": "UWQhWWW3Xs8-",
    "papermill": {
     "duration": 0.011837,
     "end_time": "2023-10-22T09:42:05.526462",
     "exception": false,
     "start_time": "2023-10-22T09:42:05.514625",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### **Tokenization 🔢**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "377b9859",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-22T09:42:05.552477Z",
     "iopub.status.busy": "2023-10-22T09:42:05.551856Z",
     "iopub.status.idle": "2023-10-22T09:42:11.874953Z",
     "shell.execute_reply": "2023-10-22T09:42:11.874084Z"
    },
    "id": "2M0x8hjV-ELe",
    "papermill": {
     "duration": 6.338645,
     "end_time": "2023-10-22T09:42:11.877336",
     "exception": false,
     "start_time": "2023-10-22T09:42:05.538691",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a30e2610c9e412195360c09675ea027",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/79.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79a4a243ef7147569294c7f6dfbcac39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6629b1ccaa474b328278f34cafe00b6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f7435b9137e4437a2ae15890e94fa97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c35677d9968e470ab214b589cf021ecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the RoBERTa tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('deepset/roberta-base-squad2')\n",
    "train_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True)\n",
    "valid_encodings = tokenizer(valid_contexts, valid_questions, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ccf571",
   "metadata": {
    "id": "EvTnonkjX7lj",
    "papermill": {
     "duration": 0.013171,
     "end_time": "2023-10-22T09:42:11.904018",
     "exception": false,
     "start_time": "2023-10-22T09:42:11.890847",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Next we need to convert our character start/end positions to token start/end positions. Why is that? Because our words converted into tokens, so the answer start/end needs to show the index of start/end token which contains the answer and not the specific characters in the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3c1283f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-22T09:42:11.931432Z",
     "iopub.status.busy": "2023-10-22T09:42:11.931088Z",
     "iopub.status.idle": "2023-10-22T09:42:11.937980Z",
     "shell.execute_reply": "2023-10-22T09:42:11.937123Z"
    },
    "id": "YBY8gSt8-MV3",
    "papermill": {
     "duration": 0.022819,
     "end_time": "2023-10-22T09:42:11.939817",
     "exception": false,
     "start_time": "2023-10-22T09:42:11.916998",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert character start/end positions to token start/end positions\n",
    "def add_token_positions(encodings, answers):\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    for i in range(len(answers)):\n",
    "        char_start = answers[i]['answer_start']\n",
    "        char_end = answers[i]['answer_end']\n",
    "\n",
    "        token_start = encodings.char_to_token(i, char_start)\n",
    "        token_end = encodings.char_to_token(i, char_end)\n",
    "\n",
    "        start_positions.append(token_start)\n",
    "        end_positions.append(token_end)\n",
    "\n",
    "        if token_start is None:\n",
    "            start_positions[-1] = tokenizer.model_max_length\n",
    "        if token_end is None:\n",
    "            end_positions[-1] = tokenizer.model_max_length\n",
    "\n",
    "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66e5ab70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-22T09:42:11.966688Z",
     "iopub.status.busy": "2023-10-22T09:42:11.966426Z",
     "iopub.status.idle": "2023-10-22T09:42:11.986862Z",
     "shell.execute_reply": "2023-10-22T09:42:11.985982Z"
    },
    "id": "Cfvg6_K8DG1F",
    "papermill": {
     "duration": 0.035849,
     "end_time": "2023-10-22T09:42:11.988727",
     "exception": false,
     "start_time": "2023-10-22T09:42:11.952878",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "add_token_positions(train_encodings, train_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6963f766",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-22T09:42:12.015120Z",
     "iopub.status.busy": "2023-10-22T09:42:12.014822Z",
     "iopub.status.idle": "2023-10-22T09:42:12.022174Z",
     "shell.execute_reply": "2023-10-22T09:42:12.021359Z"
    },
    "id": "2okra58q-Sew",
    "papermill": {
     "duration": 0.022733,
     "end_time": "2023-10-22T09:42:12.024016",
     "exception": false,
     "start_time": "2023-10-22T09:42:12.001283",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "add_token_positions(valid_encodings, valid_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "549b7a00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-22T09:42:12.051430Z",
     "iopub.status.busy": "2023-10-22T09:42:12.051169Z",
     "iopub.status.idle": "2023-10-22T09:42:12.056700Z",
     "shell.execute_reply": "2023-10-22T09:42:12.055774Z"
    },
    "id": "CxfKEWKe-VrA",
    "papermill": {
     "duration": 0.021711,
     "end_time": "2023-10-22T09:42:12.058589",
     "exception": false,
     "start_time": "2023-10-22T09:42:12.036878",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NewsQA_Dataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f608c0dc",
   "metadata": {
    "id": "oCbSpm-4X-qs",
    "papermill": {
     "duration": 0.013791,
     "end_time": "2023-10-22T09:42:12.129769",
     "exception": false,
     "start_time": "2023-10-22T09:42:12.115978",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Creating the dataset using the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d519da1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-22T09:42:12.158373Z",
     "iopub.status.busy": "2023-10-22T09:42:12.157532Z",
     "iopub.status.idle": "2023-10-22T09:42:12.162063Z",
     "shell.execute_reply": "2023-10-22T09:42:12.161117Z"
    },
    "id": "NONwsopL9oVe",
    "papermill": {
     "duration": 0.020392,
     "end_time": "2023-10-22T09:42:12.163944",
     "exception": false,
     "start_time": "2023-10-22T09:42:12.143552",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = NewsQA_Dataset(train_encodings)\n",
    "valid_dataset = NewsQA_Dataset(valid_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad2388c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-22T09:42:12.191545Z",
     "iopub.status.busy": "2023-10-22T09:42:12.191270Z",
     "iopub.status.idle": "2023-10-22T09:42:12.196286Z",
     "shell.execute_reply": "2023-10-22T09:42:12.195300Z"
    },
    "id": "8iSTLk80IQhj",
    "papermill": {
     "duration": 0.021419,
     "end_time": "2023-10-22T09:42:12.198393",
     "exception": false,
     "start_time": "2023-10-22T09:42:12.176974",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create dataloaders for training and validation\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab92c2d",
   "metadata": {
    "id": "vb1REJlnYLDI",
    "papermill": {
     "duration": 0.013394,
     "end_time": "2023-10-22T09:42:12.225222",
     "exception": false,
     "start_time": "2023-10-22T09:42:12.211828",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Importing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0f0ead9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-22T09:42:12.253969Z",
     "iopub.status.busy": "2023-10-22T09:42:12.253216Z",
     "iopub.status.idle": "2023-10-22T09:42:29.920718Z",
     "shell.execute_reply": "2023-10-22T09:42:29.919919Z"
    },
    "id": "Q9p5C52IGRwI",
    "papermill": {
     "duration": 17.684126,
     "end_time": "2023-10-22T09:42:29.923028",
     "exception": false,
     "start_time": "2023-10-22T09:42:12.238902",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02638defc2094b3b93c9ff04ffb14543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/496M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the RoBERTa model for question answering\n",
    "model = AutoModelForQuestionAnswering.from_pretrained('deepset/roberta-base-squad2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5868094a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-22T09:42:29.950745Z",
     "iopub.status.busy": "2023-10-22T09:42:29.950471Z",
     "iopub.status.idle": "2023-10-22T09:42:29.954982Z",
     "shell.execute_reply": "2023-10-22T09:42:29.954150Z"
    },
    "id": "QHlMqyHXGTRI",
    "outputId": "406ff893-c6b7-46b9-d111-ff75259cbb11",
    "papermill": {
     "duration": 0.020638,
     "end_time": "2023-10-22T09:42:29.957124",
     "exception": false,
     "start_time": "2023-10-22T09:42:29.936486",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 12\n"
     ]
    }
   ],
   "source": [
    "num_layers = model.config.num_hidden_layers\n",
    "print(f\"Number of layers: {num_layers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ed49f5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-22T09:42:29.984175Z",
     "iopub.status.busy": "2023-10-22T09:42:29.983886Z",
     "iopub.status.idle": "2023-10-22T09:42:29.989221Z",
     "shell.execute_reply": "2023-10-22T09:42:29.988361Z"
    },
    "id": "iwLrBwi-GWam",
    "papermill": {
     "duration": 0.020632,
     "end_time": "2023-10-22T09:42:29.990962",
     "exception": false,
     "start_time": "2023-10-22T09:42:29.970330",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_layers_to_freeze = 6\n",
    "for param in model.roberta.embeddings.parameters():\n",
    "    param.requires_grad = False\n",
    "for layer in model.roberta.encoder.layer[:num_layers_to_freeze]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15e09f55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-22T09:42:30.017716Z",
     "iopub.status.busy": "2023-10-22T09:42:30.017431Z",
     "iopub.status.idle": "2023-10-22T09:42:35.446407Z",
     "shell.execute_reply": "2023-10-22T09:42:35.445533Z"
    },
    "id": "ScpO0TtC-Z-V",
    "outputId": "dc15296a-c9d0-4100-a05c-d677ca7030b8",
    "papermill": {
     "duration": 5.44493,
     "end_time": "2023-10-22T09:42:35.448585",
     "exception": false,
     "start_time": "2023-10-22T09:42:30.003655",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForQuestionAnswering(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if GPU is available and move the model accordingly\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea196ac",
   "metadata": {
    "id": "C5ztdFA1YUav",
    "papermill": {
     "duration": 0.013091,
     "end_time": "2023-10-22T09:42:35.475190",
     "exception": false,
     "start_time": "2023-10-22T09:42:35.462099",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "570c949b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-22T09:42:35.503056Z",
     "iopub.status.busy": "2023-10-22T09:42:35.502470Z",
     "iopub.status.idle": "2023-10-22T09:42:35.512126Z",
     "shell.execute_reply": "2023-10-22T09:42:35.511110Z"
    },
    "id": "_cnwNywP-dce",
    "outputId": "d2da7e25-a123-4d69-ffb3-335508d1e09c",
    "papermill": {
     "duration": 0.026177,
     "end_time": "2023-10-22T09:42:35.514591",
     "exception": false,
     "start_time": "2023-10-22T09:42:35.488414",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "# Training loop\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede604a3",
   "metadata": {
    "id": "cCMxvgZVYZFY",
    "papermill": {
     "duration": 0.01273,
     "end_time": "2023-10-22T09:42:35.541502",
     "exception": false,
     "start_time": "2023-10-22T09:42:35.528772",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb00752a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-22T09:42:35.569002Z",
     "iopub.status.busy": "2023-10-22T09:42:35.568448Z",
     "iopub.status.idle": "2023-10-22T14:32:19.856054Z",
     "shell.execute_reply": "2023-10-22T14:32:19.854899Z"
    },
    "id": "Qxfu7pff-gXN",
    "outputId": "5b79d805-3671-4fa2-e44c-456b4fd46beb",
    "papermill": {
     "duration": 17384.304091,
     "end_time": "2023-10-22T14:32:19.858685",
     "exception": false,
     "start_time": "2023-10-22T09:42:35.554594",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 313/313 [02:54<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 2.3683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 313/313 [02:53<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Avg Loss: 1.6652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Avg Loss: 1.3087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Avg Loss: 1.0093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Avg Loss: 0.7864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Avg Loss: 0.6383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Avg Loss: 0.5240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Avg Loss: 0.4586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Avg Loss: 0.4072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Avg Loss: 0.3764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 - Avg Loss: 0.3328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 - Avg Loss: 0.3267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 - Avg Loss: 0.2928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 - Avg Loss: 0.2733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 - Avg Loss: 0.2682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 - Avg Loss: 0.2472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 - Avg Loss: 0.2488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 - Avg Loss: 0.2349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 - Avg Loss: 0.2119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 - Avg Loss: 0.2154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 - Avg Loss: 0.2141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 - Avg Loss: 0.2040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 - Avg Loss: 0.2064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 - Avg Loss: 0.1972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 - Avg Loss: 0.1967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 313/313 [02:54<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 - Avg Loss: 0.2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 - Avg Loss: 0.1755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 - Avg Loss: 0.1799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 - Avg Loss: 0.1979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 - Avg Loss: 0.1800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 - Avg Loss: 0.1682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 - Avg Loss: 0.1650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 - Avg Loss: 0.1477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 - Avg Loss: 0.1589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 - Avg Loss: 0.1588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 - Avg Loss: 0.1644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37 - Avg Loss: 0.1656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 - Avg Loss: 0.1628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 - Avg Loss: 0.1612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 - Avg Loss: 0.1479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41 - Avg Loss: 0.1464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42 - Avg Loss: 0.1583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 - Avg Loss: 0.1428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44 - Avg Loss: 0.1507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45 - Avg Loss: 0.1443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46 - Avg Loss: 0.1502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47 - Avg Loss: 0.1395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48 - Avg Loss: 0.1349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 - Avg Loss: 0.1431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 - Avg Loss: 0.1311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51 - Avg Loss: 0.1412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52 - Avg Loss: 0.1394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53 - Avg Loss: 0.1348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54 - Avg Loss: 0.1330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55 - Avg Loss: 0.1378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56 - Avg Loss: 0.1286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57 - Avg Loss: 0.1211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58 - Avg Loss: 0.1196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59 - Avg Loss: 0.1286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60 - Avg Loss: 0.1452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61 - Avg Loss: 0.1323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62 - Avg Loss: 0.1142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63 - Avg Loss: 0.1117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64 - Avg Loss: 0.1270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65 - Avg Loss: 0.1257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66 - Avg Loss: 0.1300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67 - Avg Loss: 0.1190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68 - Avg Loss: 0.1231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69 - Avg Loss: 0.1189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70 - Avg Loss: 0.1220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71 - Avg Loss: 0.1286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72 - Avg Loss: 0.1095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73 - Avg Loss: 0.1231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74 - Avg Loss: 0.1256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75 - Avg Loss: 0.1213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76 - Avg Loss: 0.1095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 77: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77 - Avg Loss: 0.1279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 78: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78 - Avg Loss: 0.1153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 79: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79 - Avg Loss: 0.1196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 80: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80 - Avg Loss: 0.1151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 81: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81 - Avg Loss: 0.1055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 82: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82 - Avg Loss: 0.1119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 83: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83 - Avg Loss: 0.1254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 84: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84 - Avg Loss: 0.1030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 85: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85 - Avg Loss: 0.1110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 86: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86 - Avg Loss: 0.1130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 87: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87 - Avg Loss: 0.1184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 88: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88 - Avg Loss: 0.1005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 89: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89 - Avg Loss: 0.1134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 90: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90 - Avg Loss: 0.1186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 91: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91 - Avg Loss: 0.1100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 92: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92 - Avg Loss: 0.1034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 93: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93 - Avg Loss: 0.1128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 94: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94 - Avg Loss: 0.1114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 95: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95 - Avg Loss: 0.1025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 96: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96 - Avg Loss: 0.1095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 97: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97 - Avg Loss: 0.1183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 98: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98 - Avg Loss: 0.1041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99 - Avg Loss: 0.1100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 100: 100%|██████████| 313/313 [02:53<00:00,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100 - Avg Loss: 0.1037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f'Epoch {epoch + 1}', dynamic_ncols=True):\n",
    "        inputs = {key: value.to(device) for key, value in batch.items()}\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # Calculate and print the average loss for this epoch\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch + 1} - Avg Loss: {avg_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf501bf",
   "metadata": {
    "id": "3bhW5mqWYcZV",
    "papermill": {
     "duration": 2.530739,
     "end_time": "2023-10-22T14:32:24.816957",
     "exception": false,
     "start_time": "2023-10-22T14:32:22.286218",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6436343",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-22T14:32:29.952412Z",
     "iopub.status.busy": "2023-10-22T14:32:29.951557Z",
     "iopub.status.idle": "2023-10-22T14:32:30.963720Z",
     "shell.execute_reply": "2023-10-22T14:32:30.962836Z"
    },
    "id": "-iz3t_u3SG0f",
    "outputId": "091372a7-3130-4f1d-def6-d3f1985a4229",
    "papermill": {
     "duration": 3.46045,
     "end_time": "2023-10-22T14:32:30.965742",
     "exception": false,
     "start_time": "2023-10-22T14:32:27.505292",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('local_fine_tuned_roberta_on_newsqa/tokenizer_config.json',\n",
       " 'local_fine_tuned_roberta_on_newsqa/special_tokens_map.json',\n",
       " 'local_fine_tuned_roberta_on_newsqa/vocab.json',\n",
       " 'local_fine_tuned_roberta_on_newsqa/merges.txt',\n",
       " 'local_fine_tuned_roberta_on_newsqa/added_tokens.json',\n",
       " 'local_fine_tuned_roberta_on_newsqa/tokenizer.json')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the fine-tuned model if needed\n",
    "model.save_pretrained('local_fine_tuned_roberta_on_newsqa')\n",
    "tokenizer.save_pretrained('local_fine_tuned_roberta_on_newsqa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de3c917c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-22T14:32:36.046729Z",
     "iopub.status.busy": "2023-10-22T14:32:36.045969Z",
     "iopub.status.idle": "2023-10-22T14:32:37.527120Z",
     "shell.execute_reply": "2023-10-22T14:32:37.526307Z"
    },
    "id": "-Padf5ds-9fm",
    "outputId": "b6b58675-5766-432d-bfcc-3b8d679647f5",
    "papermill": {
     "duration": 4.039355,
     "end_time": "2023-10-22T14:32:37.529533",
     "exception": false,
     "start_time": "2023-10-22T14:32:33.490178",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    " # Initialize the tokenizer and model\n",
    "fine_tuned_tokenizer = AutoTokenizer.from_pretrained('local_fine_tuned_roberta_on_newsqa')\n",
    "fine_tuned_model = AutoModelForQuestionAnswering.from_pretrained('local_fine_tuned_roberta_on_newsqa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f77fb920",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-22T14:32:42.560064Z",
     "iopub.status.busy": "2023-10-22T14:32:42.559711Z",
     "iopub.status.idle": "2023-10-22T14:32:42.696336Z",
     "shell.execute_reply": "2023-10-22T14:32:42.695371Z"
    },
    "id": "mbFcaKKNTUx8",
    "outputId": "2cc83057-9c85-454b-a750-0ddcd00daa1b",
    "papermill": {
     "duration": 2.611441,
     "end_time": "2023-10-22T14:32:42.698693",
     "exception": false,
     "start_time": "2023-10-22T14:32:40.087252",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForQuestionAnswering(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tuned_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada022cf",
   "metadata": {
    "id": "RbRz-K-SYgvd",
    "papermill": {
     "duration": 2.586849,
     "end_time": "2023-10-22T14:32:47.805869",
     "exception": false,
     "start_time": "2023-10-22T14:32:45.219020",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "665d14b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-22T14:32:52.845682Z",
     "iopub.status.busy": "2023-10-22T14:32:52.845324Z",
     "iopub.status.idle": "2023-10-22T14:32:52.850653Z",
     "shell.execute_reply": "2023-10-22T14:32:52.849788Z"
    },
    "id": "nUMaTWVC_CfB",
    "papermill": {
     "duration": 2.480281,
     "end_time": "2023-10-22T14:32:52.852566",
     "exception": false,
     "start_time": "2023-10-22T14:32:50.372285",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform inference\n",
    "question = \"What war was the Iwo Jima battle a part of?\"\n",
    "context = \"One of the Marines shown in a famous World War II photograph raising the U.S. flag on Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\\n\\nThe Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\\n\\nSgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\\n\\nAt a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank\\'s younger sister, Mary Pero.\\n\\nStrank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\\n\\nStrank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\\n\\nJonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\\n\\nHe hailed Strank as a true American hero and a wonderful example of the remarkable contribution and sacrifices that immigrants have made to our great republic throughout its history.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a18f6b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-22T14:32:58.012732Z",
     "iopub.status.busy": "2023-10-22T14:32:58.012106Z",
     "iopub.status.idle": "2023-10-22T14:32:58.038523Z",
     "shell.execute_reply": "2023-10-22T14:32:58.037612Z"
    },
    "id": "yb4mvE5C_FQT",
    "outputId": "27d6d6a3-490c-4291-8bc2-e21edfd2bf19",
    "papermill": {
     "duration": 2.656444,
     "end_time": "2023-10-22T14:32:58.040317",
     "exception": false,
     "start_time": "2023-10-22T14:32:55.383873",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What war was the Iwo Jima battle a part of?\n",
      "Answer:  II\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the passage and question\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "inputs.to(device)\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    outputs = fine_tuned_model(**inputs)\n",
    "    start_idx = torch.argmax(outputs[0])\n",
    "    end_idx = torch.argmax(outputs[1]) + 1\n",
    "\n",
    "# Get the answer text from the passage\n",
    "answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][start_idx:end_idx]))\n",
    "\n",
    "print(\"Question:\", question)\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ebac493b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-22T14:33:03.070379Z",
     "iopub.status.busy": "2023-10-22T14:33:03.069726Z",
     "iopub.status.idle": "2023-10-22T14:33:03.083055Z",
     "shell.execute_reply": "2023-10-22T14:33:03.082171Z"
    },
    "id": "-I9jRNYnXA59",
    "papermill": {
     "duration": 2.572104,
     "end_time": "2023-10-22T14:33:03.085021",
     "exception": false,
     "start_time": "2023-10-22T14:33:00.512917",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_prediction(context, question):\n",
    "  inputs = tokenizer.encode_plus(question, context, return_tensors='pt').to(device)\n",
    "  outputs = model(**inputs)\n",
    "\n",
    "  answer_start = torch.argmax(outputs[0])\n",
    "  answer_end = torch.argmax(outputs[1]) + 1\n",
    "\n",
    "  answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][answer_start:answer_end]))\n",
    "\n",
    "  return answer\n",
    "\n",
    "def normalize_text(s):\n",
    "  \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\n",
    "  import string, re\n",
    "  def remove_articles(text):\n",
    "    regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "    return re.sub(regex, \" \", text)\n",
    "  def white_space_fix(text):\n",
    "    return \" \".join(text.split())\n",
    "  def remove_punc(text):\n",
    "    exclude = set(string.punctuation)\n",
    "    return \"\".join(ch for ch in text if ch not in exclude)\n",
    "  def lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "  return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def exact_match(prediction, truth):\n",
    "    return bool(normalize_text(prediction) == normalize_text(truth))\n",
    "\n",
    "def compute_f1(prediction, truth):\n",
    "  pred_tokens = normalize_text(prediction).split()\n",
    "  truth_tokens = normalize_text(truth).split()\n",
    "\n",
    "  # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n",
    "  if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
    "    return int(pred_tokens == truth_tokens)\n",
    "\n",
    "  common_tokens = set(pred_tokens) & set(truth_tokens)\n",
    "\n",
    "  # if there are no common tokens then f1 = 0\n",
    "  if len(common_tokens) == 0:\n",
    "    return 0\n",
    "\n",
    "  prec = len(common_tokens) / len(pred_tokens)\n",
    "  rec = len(common_tokens) / len(truth_tokens)\n",
    "\n",
    "  return round(2 * (prec * rec) / (prec + rec), 2)\n",
    "\n",
    "def question_answer(context, question,answer):\n",
    "  prediction = get_prediction(context,question)\n",
    "  em_score = exact_match(prediction, answer)\n",
    "  f1_score = compute_f1(prediction, answer)\n",
    "\n",
    "  print(f'Question: {question}')\n",
    "  print(f'Prediction: {prediction}')\n",
    "  print(f'True Answer: {answer}')\n",
    "  print(f'Exact match: {em_score}')\n",
    "  print(f'F1 score: {f1_score}\\n')\n",
    "    \n",
    "  return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d9ca70d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-22T14:33:08.253156Z",
     "iopub.status.busy": "2023-10-22T14:33:08.252797Z",
     "iopub.status.idle": "2023-10-22T14:33:26.029903Z",
     "shell.execute_reply": "2023-10-22T14:33:26.028854Z"
    },
    "papermill": {
     "duration": 20.258318,
     "end_time": "2023-10-22T14:33:26.032020",
     "exception": false,
     "start_time": "2023-10-22T14:33:05.773702",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What will be nominated?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: three different videos\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does the Harrison Ford video feature?\n",
      "Prediction:  the flag-raising\n",
      "True Answer: getting his chest waxed,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What videos will you send?\n",
      "Prediction: \n",
      "True Answer: environmental\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is Ford getting waxed?\n",
      "Prediction: \n",
      "True Answer: his chest\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who got his chest waxed?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Harrison Ford\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How do you send in your video?\n",
      "Prediction:  planting an American flag\n",
      "True Answer: Use the iReport form\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What type of videos should you nominate?\n",
      "Prediction:  famous World\n",
      "True Answer: think are the best.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did Steve Bruce describe Amire Zaki as?\n",
      "Prediction: \n",
      "True Answer: unprofessional.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Which team does Zaki play for?\n",
      "Prediction:  for?</s></s>One of the Marines shown in a famous World War II photograph raising the U.S. flag on Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States\n",
      "True Answer: Wigan Athletic\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Which club did Amir Zaki fail to return to?\n",
      "Prediction:  citizenship papers.\n",
      "True Answer: Wigan Athletic\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What punishment will be meted out for his disappearance?\n",
      "Prediction: \n",
      "True Answer: a fine\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who went missing for two weeks and said he was taking a break from football?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Adriano\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who said there is no immediate plans for deployment?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: President Obama\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many troops does Canada have in Afghanistan?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: 35,000.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many more troops is the US planning to send?\n",
      "Prediction: \n",
      "True Answer: 6,000\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What are the plans of Obama after this deployment?\n",
      "Prediction: \n",
      "True Answer: to commit more U.S. troops to the ongoing war in Afghanistan,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many troops are being send to Afghanistan this year?\n",
      "Prediction:  troops\n",
      "True Answer: 6,000\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Women who don't conform will risk spending how long in jail?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: 12 hours\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What are the rules of the new order?\n",
      "Prediction:  flag-raising\n",
      "True Answer: to close their shops during daily prayers,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What controls Baidoa?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Al-Shabaab,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What can happen to the women who don't obey the order?\n",
      "Prediction:  citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "True Answer: face jail time,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Women's clothing must cover what?\n",
      "Prediction: \n",
      "True Answer: their bodies and heads\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When is the order in effect?\n",
      "Prediction:  Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935.\n",
      "True Answer: Tuesday.\n",
      "Exact match: False\n",
      "F1 score: 0.04\n",
      "\n",
      "Question: What must the clothing cover?\n",
      "Prediction: \n",
      "True Answer: their bodies and heads from view,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where was Forrest killed?\n",
      "Prediction:  on the island\n",
      "True Answer: in southwest Atlanta, Georgia,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What three men were accused in the death?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: and Jquante Crews,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Which boxing champion was killed?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Vernon Forrest,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What was the age of Vernon Forrest at the time of his death?\n",
      "Prediction: Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3,\n",
      "True Answer: 38,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What reward was offered?\n",
      "Prediction:  U.S. citizenship\n",
      "True Answer: $17,000\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many men were accused of murder?\n",
      "Prediction:  many\n",
      "True Answer: three\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What boxing champion?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Vernon Forrest,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What day in July was Vernon Forrest killed?\n",
      "Prediction:  March 1\n",
      "True Answer: 25.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What charges included  murder?\n",
      "Prediction:  murder\n",
      "True Answer: aggravated assault with a deadly weapon and possession of a firearm by a convicted felon,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What football star cleared of charge?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Steven Gerrard\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What was Gerrard's rationale?\n",
      "Prediction: \n",
      "True Answer: he believed he was about to be attacked himself.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did Gerrard admitted?\n",
      "Prediction:  World War II photograph raising the U.S. flag on Iwo Jima\n",
      "True Answer: throwing three punches\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What player has cleared waivers?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Steven Gerrard\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What was the verdict of the jury?\n",
      "Prediction:  true American hero and a wonderful example of the remarkable contribution and sacrifices that immigrants have made to our great republic throughout its history.\n",
      "True Answer: not guilty\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What was the reason for the punishment?\n",
      "Prediction: \n",
      "True Answer: threatening behavior.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Iran criticizes who?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: U.S. President-elect Barack Obama\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What are US and Iran relations tensioned about?\n",
      "Prediction: \n",
      "True Answer: nuclear program.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who said Obama should apply campaign message?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Ali Larijani\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What should Obama apply according to speaker?\n",
      "Prediction:  U.S. citizenship\n",
      "True Answer: his campaign message of change\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: US - Iran tensions are high over what?\n",
      "Prediction: \n",
      "True Answer: nuclear program.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who was the President of the US at this time?\n",
      "Prediction: \n",
      "True Answer: Barack Obama\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who has tensions over Tehran's nuclear ambitions?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: U.S.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who criticized Obama ?\n",
      "Prediction: Jonathan Scharfen,\n",
      "True Answer: Iran's parliament speaker\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Parliamentary speaker says who should apply campaign message of change?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: U.S. President-elect Barack Obama\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What kind of weapons are being discussed?\n",
      "Prediction: \n",
      "True Answer: nuclear\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who criticized Obama for saying nuclear weapon development is unaccaptable?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Iran's parliament speaker\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who spent nine years in prison?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Tim Masters,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who seeks a dismissal of Tim Masters murder case?\n",
      "Prediction: Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services\n",
      "True Answer: Colorado prosecutor\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What was Masters convicted of?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: first-degree murder charge\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When was Masters released following the toss of his conviction?\n",
      "Prediction: \n",
      "True Answer: Tuesday\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who seeks dismissal?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Colorado prosecutor\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who was released on Tuesday?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Tim Masters,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What was released?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: new DNA evidence\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What was Masters convicted of in 1999?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: first-degree murder\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What happened to the U.N. compound?\n",
      "Prediction:  happened\n",
      "True Answer: hit and set on fire,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does the lawmaker say?\n",
      "Prediction: \n",
      "True Answer: Israeli military action in Gaza is comparable to that of German soldiers during\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who are Israel being asked to talk to\n",
      "Prediction:  Mary Pero\n",
      "True Answer: Hamas,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What has the UK PM called indefensible\n",
      "Prediction:  indefensible</s>\n",
      "True Answer: the shelling of the compound\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What type of choice has Hamas made\n",
      "Prediction:  U.S. citizenship\n",
      "True Answer: step up attacks against innocent civilians.\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: The second shot hit what?\n",
      "Prediction:  Iwo Jima\n",
      "True Answer: struck Grant in the upper right arm,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: The man was rescued from what in northern Australia?\n",
      "Prediction:  World War II photograph raising the U.S. flag on Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia\n",
      "True Answer: the jaws of a crocodile\n",
      "Exact match: False\n",
      "F1 score: 0.07\n",
      "\n",
      "Question: The men were collecting what on the river bank in the Northern Territory?\n",
      "Prediction: \n",
      "True Answer: crocodile eggs\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: where Man rescues co-worker?\n",
      "Prediction:  Iwo Jima\n",
      "True Answer: northern Australia\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did Giuliana Rancic do?\n",
      "Prediction: \n",
      "True Answer: undergoing a double mastectomy and reconstructive surgery,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What surgery did Rancic have?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: double mastectomy and reconstructive\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: She says it feels great to be what?\n",
      "Prediction: \n",
      "True Answer: back at work,\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Giuliana Rancic was back on the set where?\n",
      "Prediction:  on the set where?</s></s>One of the Marines shown in a famous World War II photograph raising the U.S. flag on Iwo Jima\n",
      "True Answer: \"E! News\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who was back on the set at E!?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Rancic\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where was Giuliana Rancic?\n",
      "Prediction: \n",
      "True Answer: \"E! News\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did Rancic say about it?\n",
      "Prediction: \n",
      "True Answer: was a wonderful homecoming,\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Rancic, 37, had the surgery after lumpectomies failed to eradicate her what?\n",
      "Prediction: \n",
      "True Answer: breast cancer.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did she say?\n",
      "Prediction:  Strank as a true American hero and a wonderful example of the remarkable contribution and sacrifices that immigrants have made to our great republic throughout its history.\n",
      "True Answer: \"Even though I moved a tad slower than usual today, everyone welcomed me back with open arms and it was a wonderful homecoming,\"\n",
      "Exact match: False\n",
      "F1 score: 0.09\n",
      "\n",
      "Question: Who was greeted in Seoul?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: the announcement\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What happened in 1994?\n",
      "Prediction:  derived U.S. citizenship when his father was naturalized\n",
      "True Answer: Kim Il Sung died\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What was the reaction of South Korean military?\n",
      "Prediction: \n",
      "True Answer: raising its alert level,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who died in 1994?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Kim Il Sung\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When did Kim II Sung die?\n",
      "Prediction:  March 1, 1945,\n",
      "True Answer: 1994\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who says most people in the south are calm about the situation?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Woosuk Ken Choi,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What level was raised?\n",
      "Prediction:  World War II photograph\n",
      "True Answer: alert\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What was the reaction in Seoul?\n",
      "Prediction: \n",
      "True Answer: astonishment\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: There was general astonishment where?\n",
      "Prediction: \n",
      "True Answer: Seoul,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did cast member A.J. Jewell's death cause?\n",
      "Prediction: \n",
      "True Answer: of \"The Real Housewives of Atlanta\" reunion special,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is the former fiance of Kandi?\n",
      "Prediction: \n",
      "True Answer: Ashley \"A.J.\" Jewell,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who was the former fiance of Kandi Burruss?\n",
      "Prediction:  Mary Pero\n",
      "True Answer: Ashley \"A.J.\" Jewell,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What show was scheduled to tape its reunion special recently?\n",
      "Prediction:  Marine Corps Memorial -- which depicts the flag-raising\n",
      "True Answer: \"The Real Housewives of Atlanta\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Whose death caused the postponement of taping?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Kandi Burruss' former fiancé, Ashley \"A.J.\" Jewell,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When was the reunion scheduled for?\n",
      "Prediction:  a ceremony Tuesday at the Marine Corps Memorial\n",
      "True Answer: last week,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When was the \"Real Housewives of Atlanta\" scheduled to tape their reunion special?\n",
      "Prediction:  scheduled\n",
      "True Answer: last week,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where did the deadly earthquake happen?\n",
      "Prediction:  Iwo Jima\n",
      "True Answer: Haiti.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where did he lead the effort?\n",
      "Prediction:  on Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia\n",
      "True Answer: in Haiti.\n",
      "Exact match: False\n",
      "F1 score: 0.1\n",
      "\n",
      "Question: Is the cause of ibs known?\n",
      "Prediction:  U.S. Citizenship and Immigration Services recently discovered that Strank never\n",
      "True Answer: remains unknown,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What percent of North Americans have ibs?\n",
      "Prediction: \n",
      "True Answer: 10 to 15\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does the study analysis say works?\n",
      "Prediction:  U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "True Answer: Peppermint oil, soluble fiber, and antispasmodic drugs can indeed help people with irritable bowel syndrome,\n",
      "Exact match: False\n",
      "F1 score: 0.07\n",
      "\n",
      "Question: Approximately how many people in North America have IBS?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: 10 to 15 percent\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What treatments work for ibs?\n",
      "Prediction:  flag-raising\n",
      "True Answer: including fiber supplements, probiotics, antidepressants, behavioral-based therapies, psychotherapy, food modification, acupuncture, and laxatives.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who  has filed suit with international court?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Australian officials\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does the moratorium allow hunting whales for?\n",
      "Prediction: \n",
      "True Answer: scientific reasons.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who says decision to head to court \"regrettable\"?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Japanese Foreign Ministry spokesman Hidenobu Sobashima\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What has Austrailia filed  suit over?\n",
      "Prediction: \n",
      "True Answer: to stop Japan from exploiting the research loophole.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who allows hunting whales for scientific reasons?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Japanese officials\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what resumes TNT?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: \"The Closer.\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what is a sag award\n",
      "Prediction:  U.S. citizenship\n",
      "True Answer: Screen Actors Guild\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When does TNT resume the series?\n",
      "Prediction:  resume\n",
      "True Answer: Monday night\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who did Superman battle in \"Clan of the Fiery Cross\"?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Ku Klux\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What have affected people in real life?\n",
      "Prediction:  national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi\n",
      "True Answer: comic book characters\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What group did Superman battle in \"Clan of the Fiery Cross\"?\n",
      "Prediction:  Iwo Jima\n",
      "True Answer: the Ku Klux Klan,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who did Superman battle in the radio series?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Ku Klux Klan,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who blocked a scientist from getting a patent?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Donald Duck\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What cartoon character blocked a scientist from getting a patent?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Donald Duck\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where was the teenage boy shot?\n",
      "Prediction:  on the island\n",
      "True Answer: Athens,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many civilians were injured?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: 34\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What caused protests to explode?\n",
      "Prediction: Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "True Answer: killing of a 15-year-old boy\n",
      "Exact match: False\n",
      "F1 score: 0.02\n",
      "\n",
      "Question: Authorities vow to do what in regards to the rioting?\n",
      "Prediction: Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising\n",
      "True Answer: re-impose order\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where is the rioting happening?\n",
      "Prediction: \n",
      "True Answer: across Greece\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Number of civilians injure during riots?\n",
      "Prediction: \n",
      "True Answer: 34\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many civilians were injured in the rioting?\n",
      "Prediction: \n",
      "True Answer: 34\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where was the teenager shot at?\n",
      "Prediction:  Iwo Jima\n",
      "True Answer: Athens,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is rioting?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: young self-styled anarchists\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What do authorities vow to re-impose?\n",
      "Prediction: \n",
      "True Answer: order\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is suing the ICE?\n",
      "Prediction:  the ICE\n",
      "True Answer: Raymond Soeoth of Indonesia and Amadou Diouf of Senegal in West Africa,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: WHAT HAVE 1073 DETAINEES HAD SINCE 2003?\n",
      "Prediction:  citizenship papers.\n",
      "True Answer: \"medical escorts\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Which senator vows to investigate the allegations?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Sen. Joe Lieberman,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who did the detainees sue\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: the government.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What number of detainees had medical escorts since 2003?\n",
      "Prediction: \n",
      "True Answer: 1,073\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did they tell CNN?\n",
      "Prediction: \n",
      "True Answer: were injected with the drugs against their will.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is ICE alleged to have done to detainees?\n",
      "Prediction:  Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "\n",
      "He hailed Strank as a true American hero and a wonderful example of the remarkable contribution and sacrifices that immigrants have made to our great republic throughout its history.\n",
      "True Answer: forcibly injecting them with psychotropic drugs\n",
      "Exact match: False\n",
      "F1 score: 0.02\n",
      "\n",
      "Question: What news station interviewed the detainee\n",
      "Prediction:  Associated Press\n",
      "True Answer: CNN\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who did the detainees file a suit against?\n",
      "Prediction: \n",
      "True Answer: Immigration and Customs Enforcement\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what is the senator doing\n",
      "Prediction: \n",
      "True Answer: intends to follow up with ICE\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what were detainees allegedly injected with?\n",
      "Prediction:  citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "True Answer: psychotropic drugs\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who is lieberman?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: \"Senator\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who had medical escorts?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: 1,073 immigration detainees\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is Sen. Lieberman vowing?\n",
      "Prediction: \n",
      "True Answer: detainees are not drugged unless there\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What number of detainees have had \"medical escorts\" since 2003?\n",
      "Prediction: \n",
      "True Answer: 1,073\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the ICE suit about?\n",
      "Prediction:  was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935.\n",
      "True Answer: forcibly injecting them with psychotropic drugs while trying to shuttle them out of the country during their deportation.\n",
      "Exact match: False\n",
      "F1 score: 0.05\n",
      "\n",
      "Question: who files suit?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: American Civil Liberties Union\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what did he say\n",
      "Prediction: \n",
      "True Answer: was injected with drugs by ICE agents against his will.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What dog breed is described as \"active athletes\"?\n",
      "Prediction:  dog\n",
      "True Answer: Portuguese water\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What are Portugese water dogs like?\n",
      "Prediction: \n",
      "True Answer: are \"active athletes,\" far from couch potatoes,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is more likely to rip up the couch, than lounge on it?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Portuguese water dogs\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What issue is the breeders concerned about?\n",
      "Prediction: \n",
      "True Answer: a thorough understanding of the dogs' needs,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is Bo the dog?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: gift to the Obama girls from Sen. Ted Kennedy.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is his name?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Jeffrey Jamaleldine\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where did he go to college?\n",
      "Prediction: \n",
      "True Answer: in Missouri\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Soldier was one of more than 20,000 \"green-card warriors\"\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Jamaleldine\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What do people still believe?\n",
      "Prediction: \n",
      "True Answer: \"You can go from rags to riches\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What type of soldier is this?\n",
      "Prediction: \n",
      "True Answer: U.S. Army scout\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does his dad wonder?\n",
      "Prediction: Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship\n",
      "True Answer: Why he's more American than a German,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what's the cyclist's Olympic record?\n",
      "Prediction: \n",
      "True Answer: fifth\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who was a three-time road race world champion by 1988?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Longo-Ciprelli\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where did they win medals?\n",
      "Prediction:  Iwo Jima\n",
      "True Answer: Atlanta,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who won the record\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Longo-Ciprelli\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is the president?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Bush\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Will there be any restrictions on funding the wars\n",
      "Prediction:  funding\n",
      "True Answer: without the\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What victory did bush get\n",
      "Prediction:  U.S. citizenship\n",
      "True Answer: the bill\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the benefit for veterans?\n",
      "Prediction:  citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "True Answer: education\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What else does the bill contain?\n",
      "Prediction:  flag-raising\n",
      "True Answer: that expands education benefits for veterans who have served since the 9/11 attacks, provides a 13-week extension of unemployment benefits and more than $2 billion in disaster assistance for parts of the Midwest that have been hit by record floods.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What causes a victory for President Bush?\n",
      "Prediction:  certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank,\n",
      "True Answer: the bill\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what does legislation fund\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: wars in Iraq and Afghanistan\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what does bill contains\n",
      "Prediction:  the flag-raising\n",
      "True Answer: nearly $162 billion in war funding\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what did bush do\n",
      "Prediction:  II photograph raising the U.S. flag on Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "\n",
      "He hailed Strank as a true American hero and a wonderful example of the remarkable contribution and sacrifices that immigrants have made to our great republic throughout its history.\n",
      "True Answer: signed a bill that will pay for the wars in Iraq and Afghanistan\n",
      "Exact match: False\n",
      "F1 score: 0.03\n",
      "\n",
      "Question: What is the budget for the spending bill?\n",
      "Prediction:  the budget\n",
      "True Answer: $162 billion\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did he sign?\n",
      "Prediction:  U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "True Answer: a bill that will pay for the wars in Iraq and Afghanistan\n",
      "Exact match: False\n",
      "F1 score: 0.06\n",
      "\n",
      "Question: Can non European players be in the squad?\n",
      "Prediction: \n",
      "True Answer: frees up a place for another non-European Union\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is granted dual nationality?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Ronaldinho\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does this move mean for the squad?\n",
      "Prediction:  a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "\n",
      "He hailed Strank as a true American hero and a wonderful example of the remarkable contribution and sacrifices that immigrants have made to our great republic throughout its history.\n",
      "True Answer: frees up a place\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where was he given dual nationality?\n",
      "Prediction:  U.S. citizenship when his father was naturalized in 1935.\n",
      "True Answer: Spain\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What has Robaldinho been granted by Spain?\n",
      "Prediction:  a certificate of U.S. citizenship\n",
      "True Answer: dual nationality\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who stars in \"The Da Vinci Code\"?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Ewan McGregor\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the name of the other actor?\n",
      "Prediction: Jonathan Scharfen,\n",
      "True Answer: Ayelet Zurer\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the name of the lead actor in the movie?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Tom Hanks\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: The president of which body said  \"It's all a lie\"?\n",
      "Prediction:  U.S. Citizenship and Immigration Services\n",
      "True Answer: the Catholic League.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Which actor starred in both movies?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Tom Hanks\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is Tom's famous cast mate?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Ewan McGregor\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What was siad by the Catholic League president?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: \"I have a strong objection to the genre of mixing fact with fiction,\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Angels & Demons was the sequel to which other film?\n",
      "Prediction: Angels & Demons\n",
      "True Answer: \"The Da Vinci Code\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What HBO show was he on?\n",
      "Prediction: \n",
      "True Answer: \"The Sopranos,\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who did he meet with to discuss the issue?\n",
      "Prediction: Sgt.\n",
      "True Answer: Obama and McCain camps\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who did he meet with?\n",
      "Prediction:  Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "True Answer: Obama and McCain camps\n",
      "Exact match: False\n",
      "F1 score: 0.03\n",
      "\n",
      "Question: What does the adovocacy group promote?\n",
      "Prediction:  flag-raising\n",
      "True Answer: mental health and recovery.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is the co-founder of the advocacy group No Kidding, Me Too?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Joe Pantoliano\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did the actor act in before?\n",
      "Prediction:  World War II photograph raising the U.S. flag on Iwo Jima\n",
      "True Answer: \"The Sopranos,\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who can Dublin rival?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Silicon Valley.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where are headquartered Google and Facebook?\n",
      "Prediction:  Virginia\n",
      "True Answer: Dublin.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What companies have their headquaters in Ireland?\n",
      "Prediction: \n",
      "True Answer: Facebook and Google,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What provides Dogpatch Labs Europe?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: a space for aspiring entrepreneurs to brainstorm with like-minded people.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who already has headquarters in Ireland?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Facebook and Google,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who provides space for entrepreneurs?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Dogpatch Labs\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did Twitter announce?\n",
      "Prediction:  U.S. Citizenship and Immigration Services\n",
      "True Answer: its intention to set up headquarters in Dublin.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Which river has plunged to record low levels?\n",
      "Prediction: \n",
      "True Answer: Tigris\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does Iraq want Turkey to do?\n",
      "Prediction:  derived U.S. citizenship\n",
      "True Answer: to increase the flow of water passing through its network of dams.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What river has plunged to record lows?\n",
      "Prediction:  record lows?</s></s>One of the Marines shown in a famous World War II photograph raising the U.S. flag on Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank,\n",
      "True Answer: Tigris\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What happened to the Tigris River?\n",
      "Prediction:  Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island\n",
      "True Answer: has plunged to record low levels,\n",
      "Exact match: False\n",
      "F1 score: 0.03\n",
      "\n",
      "Question: What country does Iraq and Syria want to increase its water flow?\n",
      "Prediction:  Iraq and Syria\n",
      "True Answer: Turkey,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did Turkey do?\n",
      "Prediction: Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "True Answer: provided Syria and Iraq 500 cubic meters of water a second,\n",
      "Exact match: False\n",
      "F1 score: 0.04\n",
      "\n",
      "Question: What have they withdrawn from each others' capitals?\n",
      "Prediction: \n",
      "True Answer: ambassadors\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where is the rivers' source located?\n",
      "Prediction: \n",
      "True Answer: Turkey,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Is pre-marital sex legal in Saudi Arabia?\n",
      "Prediction:  derived U.S. citizenship when his father was\n",
      "True Answer: is illegal\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did Jawad talk about?\n",
      "Prediction: \n",
      "True Answer: foreplay, sexual conquests and how he picks up women,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: For what reason did Mazen Abdul Jawad apologize?\n",
      "Prediction: \n",
      "True Answer: his comments while Saudi authorities discuss whether he should be charged with a crime,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What are Saudi authorities debating?\n",
      "Prediction: \n",
      "True Answer: whether he should be charged with a crime,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did Mazen Abdul apologize for?\n",
      "Prediction:  World War II photograph raising the U.S. flag on Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "\n",
      "He hailed Strank as a true American hero and a wonderful example of the remarkable contribution and sacrifices that immigrants have made to our great republic throughout its history.\n",
      "True Answer: bragging about his sex life on television\n",
      "Exact match: False\n",
      "F1 score: 0.02\n",
      "\n",
      "Question: What show was Jawad on?\n",
      "Prediction:  raising the U.S. flag on Iwo Jima\n",
      "True Answer: \"Red Lines,\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: North Korea recently threatened to \"wipe out\" what country  if provoked?\n",
      "Prediction: North Korea\n",
      "True Answer: United States\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who does Japanese media reporte North Korea may fire a missile at?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Hawaii.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who did North Korea threaten to \"wipe out\" if provoked?\n",
      "Prediction: \n",
      "True Answer: the United States\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: U.S. does not believe who intends to launch long-range missile soon?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: North Korea\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who was warned to be clear due to \"military firing exercise\"?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: mariners\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does US believe?\n",
      "Prediction:  a true American hero\n",
      "True Answer: North Korea intends to launch a long-range missile in the near future,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did North Korea recently threaten?\n",
      "Prediction: \n",
      "True Answer: \"wipe out\" the United States if provoked.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What may North Korea due to Hawaii on July 4?\n",
      "Prediction: \n",
      "True Answer: fire a missile toward\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When did the japanese media report?\n",
      "Prediction:  February 23, 1945.\n",
      "True Answer: July 4.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who refuses to broadcast ad?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: The BBC\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is BBC funded by?\n",
      "Prediction: \n",
      "True Answer: an obligatory license fee paid\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the name of the charity group?\n",
      "Prediction: \n",
      "True Answer: British Red Cross, Oxfam, Save the Children and 10 other charities,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What do the protesters occupy?\n",
      "Prediction: \n",
      "True Answer: Glasgow office\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where did protestors occupy?\n",
      "Prediction:  Arlington, Virginia,\n",
      "True Answer: the foyer of the BBC building in Glasgow, Scotland\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what is the ad about?\n",
      "Prediction: Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "True Answer: aid to Gaza,\n",
      "Exact match: False\n",
      "F1 score: 0.03\n",
      "\n",
      "Question: on which date disasters emergency committee will launch appeal?\n",
      "Prediction: \n",
      "True Answer: Monday.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Carter's sentencing was postponed so he could get what?\n",
      "Prediction:  U.S. citizenship\n",
      "True Answer: some dental work done,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Rapper Dwayne Carter will be sentenced for what kind of conviction?\n",
      "Prediction: \n",
      "True Answer: gun\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does the work?\n",
      "Prediction: The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising\n",
      "True Answer: removal of his diamond-studded braces.\n",
      "Exact match: False\n",
      "F1 score: 0.03\n",
      "\n",
      "Question: Who will be condemned?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Rapper Lil Wayne\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Work includes removal of diamond-encrusted what?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: braces.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Which gene did the ALS association discover?\n",
      "Prediction:  Strank\n",
      "True Answer: ALS6,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many people does Lou Gehrig's disease effect?\n",
      "Prediction: \n",
      "True Answer: 5,600\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What number of people get ALS each year?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: 5,600\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does the ALS call the gene discovery?\n",
      "Prediction:  Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "\n",
      "He hailed Strank as a true American hero and a wonderful example of the remarkable contribution and sacrifices that immigrants have made to our great republic throughout its history.\n",
      "True Answer: \"momentous discovery\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many people has ALS or Lou Gehrig's disease?\n",
      "Prediction: \n",
      "True Answer: 5,600\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How much was seized?\n",
      "Prediction: \n",
      "True Answer: of methamphetamine and $7.8 million in cash\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How long was the investigation?\n",
      "Prediction:  World War II\n",
      "True Answer: 15-month\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where there any other drugs recovered?\n",
      "Prediction:  U.S. Citizenship and Immigration Services\n",
      "True Answer: 123 pounds of cocaine and 4.5 pounds of heroin,\n",
      "Exact match: False\n",
      "F1 score: 0.14\n",
      "\n",
      "Question: What else did the authorities recover?\n",
      "Prediction:  U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "True Answer: 123 pounds of cocaine and 4.5 pounds of heroin,\n",
      "Exact match: False\n",
      "F1 score: 0.09\n",
      "\n",
      "Question: How much cash did the authorities seize?\n",
      "Prediction:  citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "True Answer: $7.8 million\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many pounds of marijuana?\n",
      "Prediction: \n",
      "True Answer: 650\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What was the investigation dubbed?\n",
      "Prediction: \n",
      "True Answer: \"Operation Crank Call,\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where was the overcrowded ferry?\n",
      "Prediction:  Iwo Jima\n",
      "True Answer: Bangladesh,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where were the people traveling to?\n",
      "Prediction:  Arlington, Virginia,\n",
      "True Answer: Bhola\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many died in ferry capsize?\n",
      "Prediction: \n",
      "True Answer: 28\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where did the ferry depart from?\n",
      "Prediction: \n",
      "True Answer: Dhaka,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: By how many was the boat overcrowded?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: 2,000 people,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many people were on board the ferry?\n",
      "Prediction: \n",
      "True Answer: 2,000\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What was conveyed?\n",
      "Prediction: \n",
      "True Answer: our sincerity\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did the president of Toyota say he takes full responsibility for?\n",
      "Prediction: Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "True Answer: cars\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did the president of Toyota say?\n",
      "Prediction: \n",
      "True Answer: he takes full responsibility for safety issues in the company's\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who should be held responsible?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Akio Toyoda\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who should be responsible?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: the chief executive officer,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did Toyota's president say?\n",
      "Prediction: \n",
      "True Answer: he takes full responsibility for safety issues in the company's\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What are the safety issues?\n",
      "Prediction: \n",
      "True Answer: related to sudden acceleration.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When was it isolated?\n",
      "Prediction: \n",
      "True Answer: 1983\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who pioneered lithium treatment?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Dr. Cade\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When was HIV isolated?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: 1983\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Many gay rights activists applaud Obama's what?\n",
      "Prediction:  applaud Obama's\n",
      "True Answer: on supporting full marriage equality,\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what did the president do\n",
      "Prediction: \n",
      "True Answer: He acknowledged \"we have more work to do,\" including on the issue of bullying.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what will obama do\n",
      "Prediction: \n",
      "True Answer: \"we have more work to do,\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who do the gay rights activists applaud\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: the administration's progress,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who says he will continue to advocate for equality?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: President Barack Obama,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who will continue to advocate for equality\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: President Barack Obama,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What are hot spots for drug use?\n",
      "Prediction:  hot spots for drug use?</s></s>One of the Marines shown in a famous World War II photograph raising the U.S. flag on Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia\n",
      "True Answer: clubs and bars in Hong Kong and Shenzhen,\n",
      "Exact match: False\n",
      "F1 score: 0.05\n",
      "\n",
      "Question: Where else are drug hotspots?\n",
      "Prediction:  Virginia\n",
      "True Answer: public toilets and playgrounds.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what types of drugs consumed\n",
      "Prediction:  drugs\n",
      "True Answer: ketamine.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is ketamine?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: an animal tranquilizer,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the top drug choice in Hong Kong?\n",
      "Prediction: \n",
      "True Answer: ketamine,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what includes an ice sculpture of the Grinch?\n",
      "Prediction:  Marine Corps Memorial\n",
      "True Answer: frozen world located in the Gaslight Theater.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What offers great shopping?\n",
      "Prediction: Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "\n",
      "He hailed Strank as a true American hero and a wonderful example of the remarkable contribution\n",
      "True Answer: Opry Mills,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where is the show ICE! being held?\n",
      "Prediction:  Arlington, Virginia,\n",
      "True Answer: Gaslight Theater.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: when did the decorations start going up?\n",
      "Prediction:  February 23, 1945.\n",
      "True Answer: in July\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When did the decorations begin to go up?\n",
      "Prediction:  February 23, 1945.\n",
      "True Answer: July\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When do the decorations go up?\n",
      "Prediction: \n",
      "True Answer: July\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: length of time pilots to be treated\n",
      "Prediction: </s>\n",
      "True Answer: at least 12 months.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: how long is the treatment\n",
      "Prediction:  Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "True Answer: 12 months.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what does faa say\n",
      "Prediction: \n",
      "True Answer: the new policy will improve safety by bringing to the surface pilots who either ignore signs of depression or lie about their use of medication for fear of losing their licenses to fly.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: pilots must have been treated for at least 12 months for what reason?\n",
      "Prediction: \n",
      "True Answer: mild to moderate depression\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does de FAA says about the policy?\n",
      "Prediction: \n",
      "True Answer: \"absolutely\" improve safety,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what has become a way in which to emphasize ideas on Twitter?\n",
      "Prediction:  an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi\n",
      "True Answer: the hashtag\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What we can use to tag a name on twitter or Facebook?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: \"@\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: how many were wounded\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Four other people\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What was the number of suicide bombers?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: two\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who said 2 suicide bombers carried out the attack\n",
      "Prediction:  2 suicide bombers\n",
      "True Answer: NATO's International Security Assistance Force\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did Afghan authorities describe?\n",
      "Prediction: \n",
      "True Answer: The attacker hid the explosive device inside his turban,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: how many bombers were there\n",
      "Prediction:  bombers\n",
      "True Answer: two\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who said the face of the peace initiative has been attacked\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Gen. John R. Allen, commander of ISAF,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did the commander say?\n",
      "Prediction:  Strank as a true American hero and a wonderful example of the remarkable contribution and sacrifices that immigrants have made to our great republic throughout its history.\n",
      "True Answer: \"face of the peace initiative has been attacked.\"\n",
      "Exact match: False\n",
      "F1 score: 0.06\n",
      "\n",
      "Question: who was attacked\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Burhanuddin Rabbani,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: how many people were wounded in the attack\n",
      "Prediction:  many\n",
      "True Answer: Four\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What wa sthe name of the suspect?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Adam Yahiye Gadahn,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what does he say about his citizenship\n",
      "Prediction: \n",
      "True Answer: of America, the symbol of oppression and tyranny and advocate of terror in the world?\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who is in the video?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Adam Yahiye Gadahn, also known as Azzam the American,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who criticizes obama\n",
      "Prediction: Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen,\n",
      "True Answer: Adam Yahiye Gadahn,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did Adam Yahiye Gadahn say?\n",
      "Prediction: \n",
      "True Answer: \"Let me here tell you something about myself and my biography, in which there is a benefit and a lesson,\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Which area was the convert from?\n",
      "Prediction:  United States\n",
      "True Answer: rural California,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who says the man denied wife liberty of coming and going with face uncovered?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Immigration Minister Eric Besson\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did the man deny his wife of?\n",
      "Prediction:  U.S. citizenship\n",
      "True Answer: liberty to come and go with her face uncovered,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Which country is scheduled to vote on whether to ban full veils?\n",
      "Prediction:  United States\n",
      "True Answer: France\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did France deny a woman?\n",
      "Prediction:  a certificate of citizenship\n",
      "True Answer: naturalization request\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Which country denied a Moroccan woman's naturalization request?\n",
      "Prediction:  United States\n",
      "True Answer: France\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what was denied\n",
      "Prediction: \n",
      "True Answer: citizenship\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what was man denied\n",
      "Prediction: \n",
      "True Answer: citizenship\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who defeated Froch?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Mikkel\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who defeated Carl Froch?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Mikkel\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many rounds were there in the match?\n",
      "Prediction:  many\n",
      "True Answer: 12\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who defeats Carl to win?\n",
      "Prediction:  Japanese and U.S. forces\n",
      "True Answer: Mikkel\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Kessler secures unanimous points decision after what number of rounds?\n",
      "Prediction: \n",
      "True Answer: 12\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where are the Thai soldiers accused of crossing into?\n",
      "Prediction:  United States\n",
      "True Answer: Cambodian territory\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who claimed the soldiers had crossed into the area?\n",
      "Prediction:  Citizenship and Immigration Services\n",
      "True Answer: Cambodian officials\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What temple is at the center of the debate?\n",
      "Prediction:  Marine Corps Memorial\n",
      "True Answer: Preah Vihear\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: The Thai army said what?\n",
      "Prediction:  Strank as a true American hero\n",
      "True Answer: soldiers had not gone anywhere they were not permitted to be.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What century is the Preah Vihear temple from?\n",
      "Prediction:  3,\n",
      "True Answer: 11th\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did Thai soldiers cross into?\n",
      "Prediction:  Iwo Jima\n",
      "True Answer: Cambodian territory\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is arsenal manager?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Arsene Wenger\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is the Arsenal manager?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Arsene Wenger\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does Luka Modric suffer from?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: a fracture to his right fibula,\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Which team beat Arsenal 2-1?\n",
      "Prediction:  Arsenal\n",
      "True Answer: Manchester United.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did Wenger kick?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: an empty water bottle\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who does Luka Modric play for?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Croatia\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who will receive an apology?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Arsenal\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what was traveling?\n",
      "Prediction: \n",
      "True Answer: 2,000 people\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many people were on board?\n",
      "Prediction: Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men\n",
      "True Answer: about 2,000\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How much was the ferry capable of carrying?\n",
      "Prediction: \n",
      "True Answer: 1,500\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what did authorities recover\n",
      "Prediction:  U.S. citizenship\n",
      "True Answer: 54 bodies\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what is eid al adha?\n",
      "Prediction:  certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States\n",
      "True Answer: Muslim festival\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what capacity did the boat have?\n",
      "Prediction:  flag-raising\n",
      "True Answer: 1,500\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what was the boat capacity?\n",
      "Prediction:  boat\n",
      "True Answer: 1,500\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many bodies were recovered?\n",
      "Prediction:  many\n",
      "True Answer: 54\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where was the ferry headed?\n",
      "Prediction:  United States\n",
      "True Answer: Bhola\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Taliban plans what month offensive?\n",
      "Prediction:  Japanese and U.S. forces there ended.\n",
      "True Answer: January\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When do they feel strong\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: during the snowing season,\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is the Taliban leader?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Hakeemullah Mehsud\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What day was the blast in Peshawar?\n",
      "Prediction:  March 1, 1945,\n",
      "True Answer: Monday's\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many people were killed in Monday's blast?\n",
      "Prediction: \n",
      "True Answer: Eleven\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many were killed  in Peshawar?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: people\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Do you know how many were killed\n",
      "Prediction: \n",
      "True Answer: Eleven\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who led 2-0 at halftime?\n",
      "Prediction: \n",
      "True Answer: Ghana\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who are the four-time champions?\n",
      "Prediction: Strank and five other men\n",
      "True Answer: Brazil\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who beat Costa Rica?\n",
      "Prediction: \n",
      "True Answer: Brazil\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where is the under-20 World Cup being held?\n",
      "Prediction:  Virginia\n",
      "True Answer: Egypt.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What was the score of Brazil vs Costa Rica?\n",
      "Prediction: \n",
      "True Answer: 5-0,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who did Ghana beat?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Hungary\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who faces the 4 times champion?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Los Ticos\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the price of the Large Hadron Collider?\n",
      "Prediction:  posthumously awarded a certificate of U.S. citizenship\n",
      "True Answer: $10 billion\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: how much are the cost of particle accelerator?\n",
      "Prediction: \n",
      "True Answer: $10 billion\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the Hadron Collider?\n",
      "Prediction:  flag-raising\n",
      "True Answer: the world's largest particle\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does the machine do?\n",
      "Prediction:  raising the U.S. flag on Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising\n",
      "True Answer: look at how the universe formed by analyzing particle collisions.\n",
      "Exact match: False\n",
      "F1 score: 0.02\n",
      "\n",
      "Question: Who worked alongside Tom Cruise?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Scott Altman\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who did Altman work with in this film?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Tom\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What profession did Scott Altman work for?\n",
      "Prediction: \n",
      "True Answer: retired Navy F-14 fighter pilot\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Was Scott Altman a fighter pilot?\n",
      "Prediction:  derived U.S. citizenship\n",
      "True Answer: retired Navy F-14\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Which 1986 hit film did Altman perform a stunt double in?\n",
      "Prediction: \n",
      "True Answer: \"Top Gun\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: in which hospital the child receives tracheotomy?\n",
      "Prediction: \n",
      "True Answer: SSM Cardinal Glennon Children's Medical Center in St. Louis.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: where is the hospital?\n",
      "Prediction:  Arlington, Virginia,\n",
      "True Answer: St. Louis, Missouri.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: when baby joseph died?\n",
      "Prediction:  March 1, 1945,\n",
      "True Answer: Tuesday afternoon.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: that suffers Joseph Maraachli?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: a progressive neurological disease\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: The infant received a tracheotomy at\n",
      "Prediction: \n",
      "True Answer: a children's hospital in St. Louis, Missouri.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who suffered from a neurological disease?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Joseph\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the reason that women have mammograms?\n",
      "Prediction: \n",
      "True Answer: cancer.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what does the cycle have to do with it?\n",
      "Prediction:  flag-raising\n",
      "True Answer: \"The best time of your cycle to do a mammogram is going to be when your period is over, maybe the week after your period is done when the breasts are not going to be tender.\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did the man have a degree in?\n",
      "Prediction: \n",
      "True Answer: MBA in finance\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did the man get stuck in?\n",
      "Prediction:  Iwo Jima\n",
      "True Answer: a rabbit hole,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What was the man's name?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Karthik Rajaram\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did the father have an MBA in\n",
      "Prediction:  United States\n",
      "True Answer: finance\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who was a fulbright schokar\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Krishna Rajaram,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where was one of the son's attending school?\n",
      "Prediction:  Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia,\n",
      "True Answer: UCLA.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many letters did the man leave\n",
      "Prediction:  citizenship\n",
      "True Answer: three\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What are government troops and rebels battling for?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: strongholds in the north of Sri Lanka,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Number of civilians that are trapped according to aid groups?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: many as 250,000\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many civilians are trapped?\n",
      "Prediction: \n",
      "True Answer: as 250,000\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many civilians are trapped, according to aid groups?\n",
      "Prediction: \n",
      "True Answer: 250,000\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What country is being discussed here?\n",
      "Prediction: \n",
      "True Answer: Sri Lanka,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How long have the Ethnic Tamil minority been fighting?\n",
      "Prediction: \n",
      "True Answer: since 1983.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many civilians are trapped due to this conflict?\n",
      "Prediction:  many\n",
      "True Answer: as\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What do the Tamils want?\n",
      "Prediction:  U.S. citizenship on Tuesday.\n",
      "True Answer: an independent homeland\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What happend to the last functioning medical facility in the zone?\n",
      "Prediction:  medical facility in the zone?</s></s>One of the Marines shown in a famous World War II photograph raising the U.S. flag on Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "True Answer: The fighting has forced the closure of Pudukkudiyiruppu hospital in the Vanni region,\n",
      "Exact match: False\n",
      "F1 score: 0.11\n",
      "\n",
      "Question: Year the ethnic Tamil minority have been fighting since?\n",
      "Prediction:  Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "True Answer: 1983.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How long have the Tamil minority been fighting for independence?\n",
      "Prediction:  3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "\n",
      "He hailed Strank as a true American hero and a wonderful example of the remarkable contribution and sacrifices that immigrants have made to our great republic throughout its history.\n",
      "True Answer: an independent homeland since 1983.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the date of the case?\n",
      "Prediction:  Tuesday.\n",
      "True Answer: March 22,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who will it be argued before?\n",
      "Prediction: \n",
      "True Answer: a federal judge in Mississippi\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Which group filed a motion?\n",
      "Prediction:  U.S. Citizenship and Immigration Services\n",
      "True Answer: The American Civil Liberties Union\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When will the case take place?\n",
      "Prediction: \n",
      "True Answer: March 22,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who files against the school district?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: rights group\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Which school district is subject to the injunction?\n",
      "Prediction:  school\n",
      "True Answer: Mississippi\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did the group file with the court?\n",
      "Prediction: \n",
      "True Answer: a motion for a preliminary injunction against a Mississippi school district and high school\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who made passengers remove nipple rings?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Transportation Security Administration\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did the agency say?\n",
      "Prediction:  that Strank never was given citizenship papers.\n",
      "True Answer: airport appear to have properly followed procedures when they allegedly forced a woman to remove her nipple rings -- one with pliers -- but acknowledged the procedures should be changed.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What backs officers who made passenger remove nipple rings?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Transportation Security Administration\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who found piercings at airport?\n",
      "Prediction:  U.S. Citizenship and Immigration Services\n",
      "True Answer: Mandi Hamlin\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who says she heard male agents snicker?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Mandi Hamlin\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did the woman say?\n",
      "Prediction:  true American hero and a wonderful example of the remarkable contribution and sacrifices that immigrants have made to our great republic throughout its history.\n",
      "True Answer: she was humiliated\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who said procedures need to be changed?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Transportation Security Administration\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What needs to be changed?\n",
      "Prediction: \n",
      "True Answer: the procedures\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who is suspended\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: in a campus library,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What was mocked at the party?\n",
      "Prediction:  World War II photograph raising the U.S. flag on Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank,\n",
      "True Answer: Black History Month\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What was the rally opposing?\n",
      "Prediction:  Japanese and U.S. forces\n",
      "True Answer: racial intolerance.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: how many people involved\n",
      "Prediction:  five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men\n",
      "True Answer: hundreds\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: when is the incident\n",
      "Prediction:  February 23, 1945.\n",
      "True Answer: Thursday\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: was he punished\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: suspended\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where did the student hang the noose?\n",
      "Prediction:  Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia\n",
      "True Answer: campus library,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what was student attempting to accomplish\n",
      "Prediction:  planting an American flag on top of Mount Suribachi\n",
      "True Answer: intent to terrorize\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did Bryan ride for more than 11,000 miles?\n",
      "Prediction: \n",
      "True Answer: a motor scooter\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did the activist's route form?\n",
      "Prediction:  flag-raising\n",
      "True Answer: a peace sign.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did the activist want to do?\n",
      "Prediction:  on Iwo Jima\n",
      "True Answer: meeting with the president to discuss her son.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many miles did Bryan ride his scooter?\n",
      "Prediction:  five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five\n",
      "True Answer: 11,000\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When was the tribunal established?\n",
      "Prediction:  1935\n",
      "True Answer: in late 1994.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: when The United Nations established the genocide tribunal?\n",
      "Prediction:  The\n",
      "True Answer: in late 1994.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: bagosora is charged with which crime?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: genocide,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is the mastermind?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Theoneste Bagosora,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When did massacres occur?\n",
      "Prediction:  an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "True Answer: 1994\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: how many people dead?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: 800,000\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what country did massacre occur?\n",
      "Prediction:  Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States\n",
      "True Answer: Rwanda\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what position did bagosora hold?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: a colonel in the Rwandan army,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What have beer drinkers left?\n",
      "Prediction: \n",
      "True Answer: glass shards\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What kind of dogs are wearing protective shoes?\n",
      "Prediction:  dogs\n",
      "True Answer: police\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What do dog shoes cost?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: 60 euros\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What are police dogs now wearing?\n",
      "Prediction:  citizenship papers.\n",
      "True Answer: protective shoes\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What was the reason that the dogs wear protective shoes?\n",
      "Prediction: Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "True Answer: Too many glass shards left by beer drinkers in the city center,\n",
      "Exact match: False\n",
      "F1 score: 0.03\n",
      "\n",
      "Question: What is the reason for giving them shoes?\n",
      "Prediction:  certificate of U.S. citizenship\n",
      "True Answer: Too many glass shards left by beer drinkers in the city center,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What do the shoes cost?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: $89\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What are police dogs wearing?\n",
      "Prediction:  citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "True Answer: protective shoes\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who also wore dog shoes?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: walk\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who says America needs a leader who understands the future we seek?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: former Virginia Gov. Mark Warner\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What political party is Mark Warner associated with?\n",
      "Prediction: Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS,\n",
      "True Answer: Democratic\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Bush never asked Americans to do what?\n",
      "Prediction: \n",
      "True Answer: step up.\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Much of Warner's address focused on what?\n",
      "Prediction: \n",
      "True Answer: bipartisan rhetoric Obama has espoused\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Warner says America need what?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: a president who understands the world today, the future we seek and the change we\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who, specifically, did Mark Warner criticize?\n",
      "Prediction:  Strank,\n",
      "True Answer: President Bush\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What type of rhetoric did Warner focus on?\n",
      "Prediction:  flag-raising\n",
      "True Answer: bipartisan\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Bush never asked what?\n",
      "Prediction:  citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "True Answer: us to step up.\"\n",
      "Exact match: False\n",
      "F1 score: 0.07\n",
      "\n",
      "Question: What did most of Warner's address focus about?\n",
      "Prediction:  the flag-raising\n",
      "True Answer: the kind of bipartisan rhetoric Obama has espoused on the campaign trail.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What kind of rhetoric did Mark Warner allude to frequently?\n",
      "Prediction:  true American hero\n",
      "True Answer: bipartisan\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did Bush never ask Americans to do?\n",
      "Prediction:  given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "True Answer: step up.\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who does One Laptop per Child target?\n",
      "Prediction:  One Laptop per Child\n",
      "True Answer: the world's poorest children.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What can laptops do?\n",
      "Prediction: \n",
      "True Answer: allow students to engage in learning differently, enjoy a customized approach and hone critical thinking skills,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who knows how preachy and awkward the movies get?\n",
      "Prediction: Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services\n",
      "True Answer: Tripplehorn,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who directed the vignettes?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Aniston, Demi Moore and Alicia Keys\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does Jeanne Tripplehorn know about the movies?\n",
      "Prediction: \n",
      "True Answer: can get.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What story does \"Five\" tell?\n",
      "Prediction:  World War II\n",
      "True Answer: stories of different women coping with breast cancer\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who each directed a vignette?\n",
      "Prediction: \n",
      "True Answer: Aniston, Demi Moore and Alicia Keys\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who directed a Vignette?\n",
      "Prediction: Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen,\n",
      "True Answer: Aniston, Demi Moore and Alicia Keys\n",
      "Exact match: False\n",
      "F1 score: 0.02\n",
      "\n",
      "Question: What is the subject of the vignettes?\n",
      "Prediction:  flag-raising\n",
      "True Answer: different women coping with breast cancer\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is \"Five\" about?\n",
      "Prediction: \n",
      "True Answer: tells stories of different women coping with breast cancer in five vignettes.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What stories does Five tell?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: different women coping with breast cancer in\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What would the news outlet say happened?\n",
      "Prediction: \n",
      "True Answer: series of summer concerts at the O2.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What musician has scheduled a new conference?\n",
      "Prediction:  Michael Strank,\n",
      "True Answer: Michael Jackson\n",
      "Exact match: False\n",
      "F1 score: 0.5\n",
      "\n",
      "Question: In which season will the concerts be held?\n",
      "Prediction: \n",
      "True Answer: summer\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What has ben the subject of rumors?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Pop star Michael Jackson\n",
      "Exact match: False\n",
      "F1 score: 0.29\n",
      "\n",
      "Question: Where was the news conference held?\n",
      "Prediction:  Arlington, Virginia,\n",
      "True Answer: London's O2 arena,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What news outlet says he will hold a series of summer concerts?\n",
      "Prediction:  news\n",
      "True Answer: Britain's Sky\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: In what city is the O2 arena?\n",
      "Prediction:  Arlington, Virginia,\n",
      "True Answer: London's\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What has been the subject of rumors?\n",
      "Prediction:  flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "True Answer: Michael Jackson\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What do new materials do?\n",
      "Prediction:  Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "True Answer: truck safer,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did the Ford expert say?\n",
      "Prediction: \n",
      "True Answer: \"The extensive use of advanced technologies and materials in the 2009 F-150 required us to develop new, specific procedures and repair recommendations,\" said Gerry Bonanni,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What will insurers do?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: try and reduce the cost of auto repairs and insurance premiums for consumers\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What makes the vehicles safer?\n",
      "Prediction: \n",
      "True Answer: ultra-high-strength steel and boron\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does the Ford expert say?\n",
      "Prediction: \n",
      "True Answer: \"The extensive use of advanced technologies and materials in the 2009 F-150 required us to develop new, specific procedures and repair recommendations,\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who did UAE deny visa to?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Israeli tennis player Shahar Peer\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What company is no longer sponsoring the tournament?\n",
      "Prediction:  CIS,\n",
      "True Answer: The Wall Street Journal Europe\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the sport the player is active in?\n",
      "Prediction:  flag-raising\n",
      "True Answer: tennis\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what sport did he play\n",
      "Prediction:  raising the U.S. flag\n",
      "True Answer: tennis\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: where did this happen\n",
      "Prediction: \n",
      "True Answer: Dubai\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many tips did the worker call in?\n",
      "Prediction: \n",
      "True Answer: three\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What child's remains were found?\n",
      "Prediction: \n",
      "True Answer: Caylee Anthony\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How long ago did the friend tell police to check the area?\n",
      "Prediction:  1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "True Answer: month before the meter\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who went missing?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Caylee Anthony,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What was found in the search area?\n",
      "Prediction: \n",
      "True Answer: \"significant skeletal remains\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who called in several tips?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: meter reader\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who told police to check the area five months ago?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: KioMarie Cruz,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What news station reported on this case?\n",
      "Prediction:  Associated Press\n",
      "True Answer: CNN\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who crossed the Atlantic in a micro yacht?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Hugo Vihlen\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who made evolutionary discoveries?\n",
      "Prediction:  U.S. Citizenship and Immigration Services\n",
      "True Answer: Naturalist Charles Darwin\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What happened in 1620?\n",
      "Prediction:  came to the United States when he was 3, derived U.S. citizenship when his father was naturalized\n",
      "True Answer: Pilgrims sail to Plymouth Rock\n",
      "Exact match: False\n",
      "F1 score: 0.1\n",
      "\n",
      "Question: When did Darwin make his discoveries?\n",
      "Prediction:  February 23, 1945.\n",
      "True Answer: 1831\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where did the Pilgrim's voyage to?\n",
      "Prediction:  United States\n",
      "True Answer: Plymouth Rock\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When was the Beagle voyage?\n",
      "Prediction:  he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "True Answer: 1831\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When was the Mayflower voyage?\n",
      "Prediction:  3,\n",
      "True Answer: 1620\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who did they back?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: the Dalai Lama\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who wnated Tibet's independence?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: small minority\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does Lama seek?\n",
      "Prediction:  U.S. citizenship\n",
      "True Answer: autonomy.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does a small minority demand for Tibet?\n",
      "Prediction:  U.S. citizenship\n",
      "True Answer: independence,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What do Tibetian leaders back?\n",
      "Prediction: \n",
      "True Answer: the Dalai Lama's current \"middle way approach,\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does the Dalai Lama seek from Bejing?\n",
      "Prediction: \n",
      "True Answer: autonomy.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does Dalai Lama seeks?\n",
      "Prediction: \n",
      "True Answer: autonomy.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does the minority want?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Tibet's independence,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What do the Tibetan exile leaders back?\n",
      "Prediction: Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935.\n",
      "True Answer: \"middle way approach,\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who designed courses in Eastern Europe?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Gary Player\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: where has 6 courses opened\n",
      "Prediction: \n",
      "True Answer: on the Black Sea coast in Bulgaria.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what has Player designed\n",
      "Prediction: \n",
      "True Answer: two courses on the Black Sea coast in Bulgaria.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who has been designer for Golf in several Eastern European countries?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Gary Player\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many courses have opened?\n",
      "Prediction: Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five\n",
      "True Answer: six\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what has made progress\n",
      "Prediction: \n",
      "True Answer: development of two courses on the Black Sea coast in Bulgaria.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: All together, how many Golf developments Bulgaria has?\n",
      "Prediction: \n",
      "True Answer: two\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is the  Aztecas street gang affiliated with?\n",
      "Prediction: \n",
      "True Answer: cartel.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is the suspect?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Ricardo Valles de la Rosa,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is Ricardo Valles de la Rosa's age?\n",
      "Prediction: Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3,\n",
      "True Answer: 42 years old\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who arrested a suspect on Friday?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Mexican military\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who arrested the suspect Friday?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Mexican military\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is the new suspect sought in connection with the slaying?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Ricardo Valles de la Rosa,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When was the Mexican military arrested?\n",
      "Prediction:  1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "True Answer: Monday.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what is Bin's first name?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Omar\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Omar bin Laden split with whom in 2000?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: al Qaeda.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is Bin Laden?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: the most-wanted man in the world\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When did he split from his father?\n",
      "Prediction:  1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "True Answer: in 2000\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the name of Bin Laden's son?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Omar bin Laden\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Bin Laden's son was named who?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Omar\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: when was 9/11?\n",
      "Prediction:  March 1, 1945,\n",
      "True Answer: September 11, 2001.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is CNN's Baghdad correspondent?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Michael Ware\n",
      "Exact match: False\n",
      "F1 score: 0.4\n",
      "\n",
      "Question: Who is the leader of the Iranian-backed militia?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Muqtada al-Sadr,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what did CNN Baghdad say?\n",
      "Prediction: \n",
      "True Answer: correspondent Michael Ware cast doubt on Woodward's assertion\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What team does Bob Woodward credit?\n",
      "Prediction: \n",
      "True Answer: \"fusion teams,\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What day will the winners be announced?\n",
      "Prediction:  Tuesday.\n",
      "True Answer: Friday,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: when is going to be announced the winner?\n",
      "Prediction:  Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "True Answer: Friday,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: whats the name of the colombian senator?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Piedad Cordoba,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: how many contenders are for nobel peace prize?\n",
      "Prediction: \n",
      "True Answer: three\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When is the winner going to be announced?\n",
      "Prediction: When\n",
      "True Answer: Friday,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many top contenders are there for the Nobel Peace Prize?\n",
      "Prediction: \n",
      "True Answer: three\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many years until another hearing for Atkins?\n",
      "Prediction: \n",
      "True Answer: three\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who held Sharon Tate down and stabbed her 16 times?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Susan Atkins,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When did the panel set another hearing for?\n",
      "Prediction:  Tuesday\n",
      "True Answer: in three years,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What happened at Susan Atkins parole hearing?\n",
      "Prediction: \n",
      "True Answer: denied\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Did Sharon Tate's murderer get parole?\n",
      "Prediction:  posthumously awarded a certificate of U.S. citizenship on Tuesday\n",
      "True Answer: denied\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many times was Sharon Tate stabbed?\n",
      "Prediction: \n",
      "True Answer: 16\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is Atkins battling?\n",
      "Prediction:  Japanese and U.S. forces\n",
      "True Answer: terminal brain cancer.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where did the dogs detect her scent?\n",
      "Prediction:  on Iwo Jima\n",
      "True Answer: near the George Washington Bridge,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the disappeared woman's name?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Liza Murphy\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What number do you call if you have information?\n",
      "Prediction:  United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services\n",
      "True Answer: 201-262-2800.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When did the woman disappear?\n",
      "Prediction:  March 1, 1945,\n",
      "True Answer: August 19, 2007.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did the husband try after wards?\n",
      "Prediction:  on Iwo Jima\n",
      "True Answer: to take his own life\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: where was a man killed ?\n",
      "Prediction:  on the island\n",
      "True Answer: Port-au-Prince, Haiti\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What was the cause of death?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: \"The people kill him with the blocks,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did the photos show?\n",
      "Prediction:  Strank and five others raising a flag on Iwo Jima.\n",
      "True Answer: the man facing up, with his arms out to the side.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what did the man say?\n",
      "Prediction: \n",
      "True Answer: \"This is robbery. He went to rob the people. He went to steal money -- American dollars,\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what showed the gruesome scene?\n",
      "Prediction:  famous World War II photograph raising the U.S. flag on Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial\n",
      "True Answer: photos\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who provides humanitarian aid to the Somalis?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Daryeel Bulasho Guud\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the DBG agency and where do they operate?\n",
      "Prediction:  U.S. Citizenship and Immigration Services\n",
      "True Answer: Nairobi, Kenya,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What nationalities were the aid workers?\n",
      "Prediction:  United States\n",
      "True Answer: Somalis\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What aid agency suspended operations?\n",
      "Prediction:  CIS,\n",
      "True Answer: DBG,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What are some reasons why Somalis depend on humanitarian aid?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: severe famine\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What has the aid agency done?\n",
      "Prediction:  recently discovered that Strank never was given citizenship papers.\n",
      "True Answer: suspend all\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Since when do Somalis depend on humanitarian aid?\n",
      "Prediction:  3, derived U.S. citizenship when his father was naturalized in 1935.\n",
      "True Answer: 1991-1993,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who was shot in Somalia?\n",
      "Prediction:  Michael Strank,\n",
      "True Answer: Three aid workers\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How long have Robinson and Bridges been dating?\n",
      "Prediction:  3, derived U.S. citizenship when his father was naturalized in 1935.\n",
      "True Answer: two years,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: how long have they been dating\n",
      "Prediction:  1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "True Answer: two years,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: how many children does he have\n",
      "Prediction:  five\n",
      "True Answer: second child\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is having a baby?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Chris Robinson and girlfriend Allison Bridges\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How long has Robinson and girlfriend been dating?\n",
      "Prediction:  3,\n",
      "True Answer: two years,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is Robinson's ex-girlfriend?\n",
      "Prediction: \n",
      "True Answer: Kate\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the 5 1/2 year old's name?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Ryder Russell,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the name of her show ?\n",
      "Prediction: \n",
      "True Answer: \"The Rosie Show,\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What O`Donnell said about her love life?\n",
      "Prediction: \n",
      "True Answer: \"I think if I had known that she was gay, I wouldn't have been brave enough to talk to her,\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did O'Donnell say recently?\n",
      "Prediction:  discovered that Strank never was given citizenship papers.\n",
      "True Answer: \"I think if I had known that she was gay, I wouldn't have been brave enough to talk to her,\"\n",
      "Exact match: False\n",
      "F1 score: 0.14\n",
      "\n",
      "Question: Where  was O'Donnell  attracted to Rounds  ?\n",
      "Prediction:  Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia,\n",
      "True Answer: Starbucks\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who recently said she's in love ?\n",
      "Prediction: \n",
      "True Answer: O'Donnell,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the name of her new show?\n",
      "Prediction:  Marine\n",
      "True Answer: \"The Rosie Show,\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: On what O`Donnell has been attracted to?\n",
      "Prediction:  United States\n",
      "True Answer: Michelle Rounds\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When did the Rosie Show debut on the OWN network?\n",
      "Prediction:  Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "True Answer: Monday\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who was O'donnel attracted to after seeing her in starbucks?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Michelle Rounds\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who was absent from the trial?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Gov. Rod Blagojevich\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who is absent from trial?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Gov. Rod Blagojevich\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who heard the recorded phone calls?\n",
      "Prediction: \n",
      "True Answer: state senators\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is holding interviews?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Gov. Rod Blagojevich\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who did the senators hear from?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Rod Blagojevich\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: From who was the testimony?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: FBI Special Agent Daniel Cain,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who heard recorded phone calls?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: state senators\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did commentators focus on?\n",
      "Prediction: Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "True Answer: \"bystander effect\":\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what are commentators focused on?\n",
      "Prediction: \n",
      "True Answer: \"bystander effect\":\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what do the studies show?\n",
      "Prediction:  Marines shown in a famous World War II photograph raising the U.S. flag on Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "True Answer: students often know ahead of time when and where violence will flare up on campus.\n",
      "Exact match: False\n",
      "F1 score: 0.11\n",
      "\n",
      "Question: What crime do students not report?\n",
      "Prediction:  raising the U.S. flag on Iwo Jima\n",
      "True Answer: gang rape\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What do studies show?\n",
      "Prediction: \n",
      "True Answer: that students often know ahead of time when and where violence will flare up on campus.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is Robert Kimmitt?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Deputy Treasury Secretary\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does MME talk to UAE's Minister of Foreign Trade about?\n",
      "Prediction: Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "\n",
      "He hailed Strank as a true American hero and a wonderful example of the remarkable contribution and sacrifices that immigrants have made to our great republic throughout its history.\n",
      "True Answer: Sovereign Wealth Funds\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Name of the Deputy Treasury Secretary?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Robert Kimmitt.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is the U.A.E.'s Minister of Foreign Trade?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Sheikha Lubna Al Qasimi,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What was the purpose of Brown's Gulf tour?\n",
      "Prediction: \n",
      "True Answer: in an attempt to secure more funds from the region.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where did UK Prime Minister Gordon Brown tour?\n",
      "Prediction:  United States\n",
      "True Answer: the Gulf\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did Robert Kimmitt discuss?\n",
      "Prediction:  flag-raising\n",
      "True Answer: Sovereign Wealth Funds\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Name of the prime minister of the UK?\n",
      "Prediction: Jonathan Scharfen,\n",
      "True Answer: Gordon Brown\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What position does Robert Kimmitt hold?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Deputy Treasury Secretary\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what is missing?\n",
      "Prediction:  citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "True Answer: A Pablo Picasso sketchbook with 33 pencil drawings\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what does it look like?\n",
      "Prediction:  flag-raising\n",
      "True Answer: The sketchbook has a red varnished cover with the word \"Album\" inscribed on\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: how many drawings missing\n",
      "Prediction:  Strank\n",
      "True Answer: 33 pencil\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: The artist used the sketchbook when?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: between 1917 and 1924\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: when was the notebook used?\n",
      "Prediction:  February 23, 1945.\n",
      "True Answer: between 1917 and 1924\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What color is the cover?\n",
      "Prediction:  U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States\n",
      "True Answer: red\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who was killed in the hijacking?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Reggae legend Lucky Dube,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who has killed in the attempted hijacking?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Reggae legend Lucky Dube,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Does this affect the upcoming World Cup?\n",
      "Prediction:  World Cup\n",
      "True Answer: his death cast a shadow over festivities\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where did the hijacking take place?\n",
      "Prediction:  Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia\n",
      "True Answer: Johannesburg\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did the hijacker try to steal?\n",
      "Prediction:  U.S. citizenship\n",
      "True Answer: car,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: In what city was Dube killed?\n",
      "Prediction:  on the island\n",
      "True Answer: Johannesburg\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When was Dube killed?\n",
      "Prediction:  March 1, 1945,\n",
      "True Answer: around 8 p.m. local time Thursday\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Did hijacker try to steal a car?\n",
      "Prediction:  hij\n",
      "True Answer: his\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What country was reggae legend Lucky Dube from?\n",
      "Prediction:  United States\n",
      "True Answer: South Africa's\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who was killed in an attempted hijacking?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Lucky Dube,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Did he during an attempted carjacking\n",
      "Prediction:  Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "\n",
      "He hailed Strank as a true American hero and a wonderful example of the remarkable contribution and sacrifices that immigrants have made to our great republic throughout its history.\n",
      "True Answer: Dube, 43, was killed\n",
      "Exact match: False\n",
      "F1 score: 0.03\n",
      "\n",
      "Question: Where did 1 million gather?\n",
      "Prediction: \n",
      "True Answer: in Angola\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the name of the Pope?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Benedict XVI\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Amount of people gathered to hear the Pope?\n",
      "Prediction: Amount of people\n",
      "True Answer: 1 million\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: where was the event\n",
      "Prediction:  Arlington, Virginia,\n",
      "True Answer: Angola\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does the Pope express?\n",
      "Prediction: \n",
      "True Answer: Benedict also expressed \"deep sorrow\" at the death of two women killed in a stampede at one of his events in Angola on Saturday,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: how many dead?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: two\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where was Mass?\n",
      "Prediction: Sgt. Michael Strank, who was born in Czechoslovakia\n",
      "True Answer: in Angola,\n",
      "Exact match: False\n",
      "F1 score: 0.2\n",
      "\n",
      "Question: What continent is the Pope visiting?\n",
      "Prediction:  United States\n",
      "True Answer: Africa.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Which airline has lighting that subtly shifts throughout the day?\n",
      "Prediction:  CIS,\n",
      "True Answer: Virgin America\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Which airline has in-cabin lighting?\n",
      "Prediction:  United States\n",
      "True Answer: Virgin America\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many driverless pods were there?\n",
      "Prediction: \n",
      "True Answer: 18\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where are the driverless pods being tested\n",
      "Prediction:  Arlington, Virginia,\n",
      "True Answer: at airports\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many pods are being tested?\n",
      "Prediction:  pods\n",
      "True Answer: are\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What shifts throughout the day?\n",
      "Prediction: \n",
      "True Answer: in-cabin lighting system\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: In which country is Madonna helping orphans?\n",
      "Prediction:  Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia,\n",
      "True Answer: Malawi,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who discovers the passion of the Angolan footballers?\n",
      "Prediction:  Citizenship and Immigration Services\n",
      "True Answer: David McKenzie\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who discovered the passion of the Angolan football squad?\n",
      "Prediction:  U.S. Citizenship and Immigration Services\n",
      "True Answer: David McKenzie\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Alina Cho speaks to who about her efforts to help other Malawi's orphans?\n",
      "Prediction: Alina Cho\n",
      "True Answer: Madonna\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is helping Malawian orphans?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Madonna\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What sport does the World Cup legend play?\n",
      "Prediction: \n",
      "True Answer: football\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the name of the World Cup legend?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Diego Maradona\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Inside Africa catches up with a World Cup legend spreading football excitement in which country?\n",
      "Prediction:  Africa\n",
      "True Answer: South\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who speaks to Madonna?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Alina Cho\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where did he go?\n",
      "Prediction:  United States\n",
      "True Answer: drove to a gym\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: where were the bodies found?\n",
      "Prediction:  Iwo Jima\n",
      "True Answer: the bedrooms of their two-floor home\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what city did slayings occur in?\n",
      "Prediction:  Iwo Jima\n",
      "True Answer: St. Louis,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What was on the walls of the home?\n",
      "Prediction:  flag-raising\n",
      "True Answer: threatening messages\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who will decide whether to file charges?\n",
      "Prediction:  U.S. Citizenship and Immigration Services\n",
      "True Answer: state's attorney\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where did the husband say he was?\n",
      "Prediction:  Czechoslovakia\n",
      "True Answer: gym to work out.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who made the documentary?\n",
      "Prediction:  an Associated Press\n",
      "True Answer: Sabina Guzzanti\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Documentary was screened where?\n",
      "Prediction:  Iwo Jima\n",
      "True Answer: Cannes Film Festival,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What was the name of the documentary?\n",
      "Prediction:  World\n",
      "True Answer: \"Draquila\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who boycotted the film?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Sandro Bondi\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: The film, \"Draquila,\" takes issue with the way the prime minister handled what earthquake?\n",
      "Prediction:  World\n",
      "True Answer: L'Aquila\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Italian Culture Minister boycotted Cannes  why?\n",
      "Prediction: \n",
      "True Answer: Bondi issued a statement, dismissing the documentary as \"propaganda\" and saying it \"offends the truth and all of the\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the documentary about?\n",
      "Prediction:  World War II photograph raising the U.S. flag on Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "\n",
      "He hailed Strank as a true American hero and a wonderful example of the remarkable contribution and sacrifices that immigrants have made to our great republic throughout its history.\n",
      "True Answer: prime minister's handling of the L'Aquila earthquake,\n",
      "Exact match: False\n",
      "F1 score: 0.01\n",
      "\n",
      "Question: What is the filmmaker's name?\n",
      "Prediction: Jonathan Scharfen,\n",
      "True Answer: Sabina Guzzanti\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who was the documentary about?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Silvio Berlusconi.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is open 24/7?\n",
      "Prediction:  Marine Corps War Memorial in Virginia\n",
      "True Answer: The control room\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what year was H1N1\n",
      "Prediction:  Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935.\n",
      "True Answer: 2009\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who needs to use caution?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: those traveling near the Somali coast\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where will passengers fly?\n",
      "Prediction:  Iwo Jima\n",
      "True Answer: over the\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where do passengers have to fly to continue their journey?\n",
      "Prediction:  Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia\n",
      "True Answer: Dubai\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who issued a travel warning?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: German Foreign Ministry,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many passengers were mentioned as travelling in this situation?\n",
      "Prediction:  passengers\n",
      "True Answer: 246\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Which countries advise extreme caution while traveling near the Somali coast?\n",
      "Prediction:  Czechoslovakia\n",
      "True Answer: U.S. State Department and British Foreign Office\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where will passengers fly to continue their journey?\n",
      "Prediction:  Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia,\n",
      "True Answer: Dubai\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many passengers does it involve?\n",
      "Prediction:  five\n",
      "True Answer: 246\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Which ship was the incident aboard?\n",
      "Prediction:  Iwo Jima\n",
      "True Answer: MS Columbus,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where did the US and UK say that people should avoid?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: the Somali coast\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where was Omar previously denied?\n",
      "Prediction:  U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank,\n",
      "True Answer: asylum in Britain.\n",
      "Exact match: False\n",
      "F1 score: 0.08\n",
      "\n",
      "Question: Did Spain give a reason for turning down the asylum?\n",
      "Prediction: \n",
      "True Answer: was given\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who was denied asylum in Britain?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Omar bin Laden\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where did Omar bin Laden first try to flee to?\n",
      "Prediction:  United States\n",
      "True Answer: Britain.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who did Spain deny asylum to?\n",
      "Prediction:  Michael Strank,\n",
      "True Answer: Omar bin Laden,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who was denied asylum?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Omar bin Laden\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who didn't give a reason?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: U.N. High Commissioner for Refugees\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What family member of Omar bin Laden was associated with terrorism?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: his father\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did he want his father to abandon?\n",
      "Prediction: \n",
      "True Answer: terrorism.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Mugabe and Tsvangirai have signed agreement paving way for what?\n",
      "Prediction:  agreement\n",
      "True Answer: power-sharing talks to take place in the next few weeks.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Whose government was called to be illegitimate?\n",
      "Prediction:  Michael Strank,\n",
      "True Answer: Zimbabwean\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does order expand?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: U.S. sanctions against\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did Bush calls Robert Mugabe's government?\n",
      "Prediction:  Mug\n",
      "True Answer: \"illegitimate.\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who signed the order?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: President Bush\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What two countries vetoed UN resolutions relevant to this situation?\n",
      "Prediction: Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States\n",
      "True Answer: Russia and China\n",
      "Exact match: False\n",
      "F1 score: 0.12\n",
      "\n",
      "Question: Who is Tsvangirai?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: opposition candidate\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did President Bush sign order to expand?\n",
      "Prediction: \n",
      "True Answer: U.S. sanctions against\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who signed an agreement paving way for power-sharing talks?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Mugabe and Tsvangirai\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What country does Robert Mugabe lead?\n",
      "Prediction:  Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "\n",
      "He hailed Strank as a true American hero and a wonderful example of the remarkable contribution and sacrifices that immigrants have made to our great republic\n",
      "True Answer: Zimbabwe,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What country are the sanctions against?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Zimbabwe,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who vetoed the UN resolution?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Russia and China\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did tthe CEO say?\n",
      "Prediction: \n",
      "True Answer: the Airbus A330-200 encountered heavy turbulence about 02:15 a.m. local time Monday\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What encountered heavy turbulence?\n",
      "Prediction: \n",
      "True Answer: the Airbus A330-200\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did the A330-200 encounter?\n",
      "Prediction:  Iwo Jima\n",
      "True Answer: heavy turbulence\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did the Airbus A330-200 encounter\n",
      "Prediction:  derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi\n",
      "True Answer: heavy turbulence\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many people were on the flight?\n",
      "Prediction:  people\n",
      "True Answer: 228\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many people where onboard the flight\n",
      "Prediction:  many people\n",
      "True Answer: 228\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did some consider to be a possible cause\n",
      "Prediction: \n",
      "True Answer: lightning strike\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did Obama said?\n",
      "Prediction: Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "\n",
      "He hailed Strank as a true American hero and a wonderful example of the remarkable contribution and sacrifices that immigrants have made to our great republic throughout its history.\n",
      "True Answer: \"We've seen Washington launch policy after policy, yet our dependence on foreign oil has only grown, even as the world's resources are disappearing,\"\n",
      "Exact match: False\n",
      "F1 score: 0.03\n",
      "\n",
      "Question: How much will be invested?\n",
      "Prediction: \n",
      "True Answer: $150 billion\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How much would Obama invest?\n",
      "Prediction: \n",
      "True Answer: $150 billion over 10 years\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who was named secretary of energy?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Steven Chu\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who named Secretary of energy?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Steven Chu\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who created climate policy?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Carol Browner\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who hosted \"Star Search\"?\n",
      "Prediction: \n",
      "True Answer: Ed McMahon\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who hosted \"The Tonight Show\"?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Carson\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What tv shows hosted McMahon?\n",
      "Prediction:  tv\n",
      "True Answer: \"TV's Bloopers and Practical Jokes,\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is McMahon most famous for?\n",
      "Prediction:  World War II photograph\n",
      "True Answer: longtime pitchman and Johnny Carson sidekick\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what health problems suffered McMahon?\n",
      "Prediction:  health problems suffered McMahon?</s></s>One of the Marines shown in a famous World War II photograph raising the U.S. flag on Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank,\n",
      "True Answer: pneumonia and other medical\n",
      "Exact match: False\n",
      "F1 score: 0.04\n",
      "\n",
      "Question: Which TV programs did McMahon host?\n",
      "Prediction:  flag-raising\n",
      "True Answer: \"TV's Bloopers and Practical Jokes,\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: New book quotes Reid discussing what?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: saying privately in 2008 that Obama could be successful as a black candidate in part because of his \"light-skinned\" appearance and speaking patterns \"with no Negro dialect, unless he wanted to have one.\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does Congressional Black Caucus reject?\n",
      "Prediction: \n",
      "True Answer: calls for Reid's dismissal.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does GOP chair say about the Senate majority leader's language?\n",
      "Prediction: \n",
      "True Answer: \"embarrassing and racially insensitive,\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Congressional Black Caucus rejects calls for?\n",
      "Prediction:  rejects calls\n",
      "True Answer: him to step down as majority leader.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What are training dogs for?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: to sniff out cell phones.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What are dogs trained to do?\n",
      "Prediction: \n",
      "True Answer: sniff out cell phones.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What clears Texas Legislature?\n",
      "Prediction:  U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank,\n",
      "True Answer: that would crack down on convicts caught with phones and allow prison systems to monitor and detect cell signals.\n",
      "Exact match: False\n",
      "F1 score: 0.1\n",
      "\n",
      "Question: What does the bill crack down on?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: convicts caught with phones\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What are sniffer dogs in Texas prisons looking for?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: sniff out cell phones.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What do inmates in Texas have?\n",
      "Prediction: Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship\n",
      "True Answer: cell phones\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is texas?\n",
      "Prediction: The Marine Corps War Memorial\n",
      "True Answer: state\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does the book include?\n",
      "Prediction:  the flag-raising\n",
      "True Answer: a paragraph about the king and crown prince\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who was arrested?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Harry Nicolaides,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what did authorities consider it as\n",
      "Prediction: \n",
      "True Answer: illegal to defame, insult or threaten the crown.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When was Harry Nicolaides arrested over his 2005 book?\n",
      "Prediction: \n",
      "True Answer: August 31.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what age was Harry Nicolaides\n",
      "Prediction:  3,\n",
      "True Answer: 41,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what does book include?\n",
      "Prediction:  the flag-raising\n",
      "True Answer: a paragraph about the king and crown prince\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what did Man's lawyer say\n",
      "Prediction:  a true American hero and a wonderful example of the remarkable contribution and sacrifices that immigrants have made to our great republic throughout its history.\n",
      "True Answer: he was released Friday and taken to the Australian embassy\n",
      "Exact match: False\n",
      "F1 score: 0.13\n",
      "\n",
      "Question: what did authorites deem?\n",
      "Prediction: \n",
      "True Answer: a violation of a law that makes it illegal to defame, insult or threaten the crown.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where does the husband tweet from?\n",
      "Prediction:  from?\n",
      "True Answer: Haiti\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: from where Troy issues tweets?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Haiti\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who are missionary couple with houseful of children?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Tara and Troy Livesay\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When did wife blog?\n",
      "Prediction: \n",
      "True Answer: Wednesday morning.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where does Troy issue tweets from?\n",
      "Prediction: \n",
      "True Answer: Haiti\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What are the couple?\n",
      "Prediction:  Strank's younger sister, Mary Pero\n",
      "True Answer: country directors\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who is reputed leader of klan?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Raymond \"Chuck\" Foster\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where is Cynthia Lynch from?\n",
      "Prediction:  Czechoslovakia\n",
      "True Answer: Tulsa, Oklahoma.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the KKK?\n",
      "Prediction: \n",
      "True Answer: Ku Klux Klan\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What are Americans doing as a way to cope with tough economic times?\n",
      "Prediction:  derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "True Answer: bartering\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When does bartering rise dramatically?\n",
      "Prediction: \n",
      "True Answer: \"When the economy turns unfriendly,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is the barter network president?\n",
      "Prediction: Jonathan Scharfen,\n",
      "True Answer: Michael Krane,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does bartering involve?\n",
      "Prediction:  flag-raising\n",
      "True Answer: trading goods and services without exchanging money\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does barterng involve?\n",
      "Prediction:  flag-raising\n",
      "True Answer: trading goods and services without exchanging money\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What are Americans doing to cope with tough economic times?\n",
      "Prediction: \n",
      "True Answer: bartering\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who is not  considered an African-American woman for high court?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Elena Kagan\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what are they ticked off ab out\n",
      "Prediction:  Strank never was given citizenship papers\n",
      "True Answer: nomination of Elena Kagan to fill the seat of retiring Supreme Court Justice John Paul\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What has ticked off a number of leaders?\n",
      "Prediction:  awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "True Answer: nomination of Elena Kagan to fill the seat of retiring Supreme Court Justice John Paul\n",
      "Exact match: False\n",
      "F1 score: 0.05\n",
      "\n",
      "Question: what woman has he considered\n",
      "Prediction:  Mary Pero\n",
      "True Answer: Elena Kagan\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does the draft report say about Iran?\n",
      "Prediction:  Strank never was given citizenship papers.\n",
      "True Answer: could be secretly working on a nuclear weapon\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What could Iran be secretly working on?\n",
      "Prediction: \n",
      "True Answer: nuclear weapon\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who said the report is a major development?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Fareed Zakaria.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what did zakaria say\n",
      "Prediction: \n",
      "True Answer: \"To be casually talking about military action because we're getting frustrated seems to me somewhat dangerous,\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is not on the side of the Iranian regime?\n",
      "Prediction: \n",
      "True Answer: Iran's Green Movement of protesters\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does he talk of against Iran?\n",
      "Prediction:  Iran?</s></s>One of the Marines shown in a famous World War II photograph raising the U.S. flag on Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "True Answer: military strike\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what was the report about\n",
      "Prediction: \n",
      "True Answer: Iran could be secretly working on a nuclear weapon\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When was Ma Khin Khin Leh sentenced to life in prison?\n",
      "Prediction: Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "True Answer: July 1999,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many prisoners were freed from Myanmar?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Nineteen\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where was leader Aung San Suu Kyi confined to?\n",
      "Prediction:  Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia\n",
      "True Answer: in her home\n",
      "Exact match: False\n",
      "F1 score: 0.05\n",
      "\n",
      "Question: Number of politicl prisoners freed in Myanmar?\n",
      "Prediction: \n",
      "True Answer: Nineteen\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where were the school teacher and 18 other political prisoners freed from?\n",
      "Prediction: \n",
      "True Answer: Myanmar\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is still confined to home?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Aung San Suu Kyi\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who was confined to his home?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Aung San Suu Kyi\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Why was Ma Khin Leh sentenced to life?\n",
      "Prediction:  World War II photograph\n",
      "True Answer: because her husband, a student activist, had helped plan a protest demonstration in Bago in July 1999,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is the pro-democracy leader confined to her home in Myanmar?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Aung San Suu Kyi\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who wa sentenced to life?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Ma Khin Khin Leh,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who was sentenced to life?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Ma Khin Khin Leh,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is among the 19 prisoners?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Ma Khin Khin Leh,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is being planned?\n",
      "Prediction: \n",
      "True Answer: tallest building,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What will reach over a kilometer in height?\n",
      "Prediction:  a kilometer in height?</s></s>One of the Marines shown in a famous World War II photograph raising the U.S. flag on Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial\n",
      "True Answer: tallest building,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: a new generation of what?\n",
      "Prediction: \n",
      "True Answer: innovative, exciting skyscrapers\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Which towers have good designs in the US?\n",
      "Prediction: \n",
      "True Answer: highlight of the rebuilt World Trade Center.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What height will Kingdom City be?\n",
      "Prediction: \n",
      "True Answer: over a kilometer (3,281 feet)\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What will the height of Kingdom City be?\n",
      "Prediction: \n",
      "True Answer: over a kilometer (3,281 feet) high.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many hostages did the Farc hold?\n",
      "Prediction:  five\n",
      "True Answer: 750\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who was kidnapped August 4?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Oscar Tulio Lizcano\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does FARC stand for?\n",
      "Prediction:  FARC\n",
      "True Answer: Revolutionary Armed Forces of Colombia,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who kidnapped the ex-congressman?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: leftist rebels\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: In what country are the estimated 750 hostages being held?\n",
      "Prediction:  United States\n",
      "True Answer: Colombia.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When was Lizcano kidnapped?\n",
      "Prediction: Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "True Answer: August 4, 2000\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many hostages does the FARC hold?\n",
      "Prediction: Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five\n",
      "True Answer: 750\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Lizcano fled how long ago?\n",
      "Prediction:  3,\n",
      "True Answer: about three days\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who was kidnapped on August 4, 2000?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Oscar Tulio Lizcano\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What was the value of the fortune Daniel Radcliffe received when he turned 18?\n",
      "Prediction:  U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "True Answer: £20 million\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What age is Daniel?\n",
      "Prediction:  3,\n",
      "True Answer: 18,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where was the money held?\n",
      "Prediction:  Arlington, Virginia,\n",
      "True Answer: in a trust fund\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is he going to do?\n",
      "Prediction:  derived U.S. citizenship\n",
      "True Answer: have some sort of party,\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What happens when Daniel Radcliffe turns 18?\n",
      "Prediction: \n",
      "True Answer: gains access to a reported £20 million\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Which earnings were held?\n",
      "Prediction: \n",
      "True Answer: £20 million\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who stars in Harry Potter?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Daniel Radcliffe\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What day is Daniel Radcliffe's birthday?\n",
      "Prediction:  Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "\n",
      "He hailed Strank as a true American hero and a wonderful example of the remarkable contribution and sacrifices that immigrants have made to our great republic throughout its history.\n",
      "True Answer: Monday,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who inherits £20M on Monday?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Daniel Radcliffe\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is he saving for?\n",
      "Prediction:  U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States\n",
      "True Answer: books\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What happened to all the money Radcliffe made from the Harry Potter movies?\n",
      "Prediction:  citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "True Answer: held in a trust fund\n",
      "Exact match: False\n",
      "F1 score: 0.09\n",
      "\n",
      "Question: Where did the money in the trust fund come from?\n",
      "Prediction: \n",
      "True Answer: the first five Potter films\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is Radcliffe's net worth?\n",
      "Prediction:  net worth\n",
      "True Answer: £20 million\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What are his plans with the money?\n",
      "Prediction: \n",
      "True Answer: books\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is the star of Harry Potter?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Daniel Radcliffe\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What amount of money is Daniel getting?\n",
      "Prediction:  certificate\n",
      "True Answer: £20 million\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what did the young actor say?\n",
      "Prediction: \n",
      "True Answer: \"I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is search of the temple expected to turn up?\n",
      "Prediction:  citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "True Answer: possible victims of physical and sexual abuse.\n",
      "Exact match: False\n",
      "F1 score: 0.04\n",
      "\n",
      "Question: Where is Jeffs in jail awaiting trial?\n",
      "Prediction:  Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia,\n",
      "True Answer: Texas\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the number of children removed from ranch?\n",
      "Prediction: Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's\n",
      "True Answer: 137\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: In which state is Jeffs jailed while awaiting trial?\n",
      "Prediction:  United States\n",
      "True Answer: Arizona\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many children were removed?\n",
      "Prediction: \n",
      "True Answer: 137\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the number of people removed from ranch?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: 183\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many children were removed from the ranch?\n",
      "Prediction: \n",
      "True Answer: 137\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many people were removed?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: 183\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many years was Jeffs sentenced to prison last year?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: 10\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What state jail is Jeffs waiting trial in?\n",
      "Prediction:  Czechoslovakia and came to the United States\n",
      "True Answer: Utah\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What instrument does she play?\n",
      "Prediction:  flag-raising\n",
      "True Answer: piano\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Whose new album is she producing?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: \"Quiet Nights,\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is Diana Krall's new album?\n",
      "Prediction:  Diana Krall's\n",
      "True Answer: \"Quiet Nights,\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what bridge did Monet draw?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: London's Waterloo\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what room did Monet stay in at the Savoy?\n",
      "Prediction:  Marine Corps Memorial\n",
      "True Answer: 618\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: From which room was Monet's Waterloo Bridge painted?\n",
      "Prediction:  Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia,\n",
      "True Answer: 618\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what hotel did Monet stay at?\n",
      "Prediction:  Marine Corps War Memorial in Virginia\n",
      "True Answer: Savoy\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Whose drawing is on display at the Savoy?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Claude Monet\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When did the crash happen?\n",
      "Prediction:  March 1, 1945,\n",
      "True Answer: Monday\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what is the number of passengers?\n",
      "Prediction:  five\n",
      "True Answer: 82\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what amount of bodies have been found so far?\n",
      "Prediction:  bodies\n",
      "True Answer: 14\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where was black box located?\n",
      "Prediction:  Arlington, Virginia,\n",
      "True Answer: at a depth of about 1,300 meters in the Mediterranean Sea.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What was found?\n",
      "Prediction: \n",
      "True Answer: flight data recorder from an Ethiopian Airlines plane\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many bodies were recovered?\n",
      "Prediction: \n",
      "True Answer: 14\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where was the flight going?\n",
      "Prediction:  Iwo Jima\n",
      "True Answer: Addis Ababa,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: where was the back box found?\n",
      "Prediction:  U.S. Citizenship and Immigration Services\n",
      "True Answer: at a depth of about 1,300 meters in the Mediterranean Sea.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did 26 year old return from doing?\n",
      "Prediction:  United States when he was 3, derived U.S. citizenship\n",
      "True Answer: after giving birth to baby daughter Jada,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How long did the final last and what was the score\n",
      "Prediction:  less than a month\n",
      "True Answer: 6-2 6-1\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did Belgian defeat American in?\n",
      "Prediction:  Iwo Jima\n",
      "True Answer: final of the Sony Ericsson Open\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who did Kim demolish?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Venus Williams\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who beat williams\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Kim\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what was the score\n",
      "Prediction:  Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "True Answer: 6-2 6-1\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many titles has the winner of the tournament collected on the tour\n",
      "Prediction: \n",
      "True Answer: third\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Which player did Venue Williams lose to in the Miami final\n",
      "Prediction: \n",
      "True Answer: Kim\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who had a baby\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Kim\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who is al-moayad\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: a Yemeni cleric\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did the court say?\n",
      "Prediction: \n",
      "True Answer: not receive a fair trial.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who did al-Moayad boast about giving money to?\n",
      "Prediction: \n",
      "True Answer: al Qaeda leader Osama bin Laden.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who supported terrorism?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Sheik Mohammed Ali al-Moayad and Mohammed Mohsen Zayed,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who gave money to Osama bin Laden?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: al-Moayad\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What denied the pair a fair trial?\n",
      "Prediction:  fair\n",
      "True Answer: prosecutors\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what does court say\n",
      "Prediction: \n",
      "True Answer: they did not receive a fair trial.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who was convicted of supporting terrorism?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Sheik Mohammed Ali al-Moayad and Mohammed Mohsen Zayed,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who is Khalid Sheikh Mohammed ?\n",
      "Prediction:  Khalid Sheikh Mohammed ?</s></s>One of the Marines shown in a famous World War II photograph raising the U.S. flag on Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank,\n",
      "True Answer: the mastermind behind the September 11, 2001, terrorist attacks on the United States.\n",
      "Exact match: False\n",
      "F1 score: 0.04\n",
      "\n",
      "Question: What happened to Khalid Sheikh Mohammed?\n",
      "Prediction:  awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "True Answer: was waterboarded 183 times in a month,\n",
      "Exact match: False\n",
      "F1 score: 0.04\n",
      "\n",
      "Question: What did former CIA officer say?\n",
      "Prediction:  Michael Strank,\n",
      "True Answer: Kiriakou said Zubaydah had been waterboarded for \"about 30 seconds, 35 seconds\" and agreed to cooperate with interrogators the following day.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who denounced the decision?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Hayden\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who did denounces decision to release memos?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Hayden\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where did the passenger purchase airline tickets from?\n",
      "Prediction:  Czechoslovakia\n",
      "True Answer: Expedia.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did the passenger purchase?\n",
      "Prediction:  U.S. citizenship\n",
      "True Answer: two tickets to Italy\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What travel company issued a corrected ticket?\n",
      "Prediction:  U.S. Citizenship and Immigration Services\n",
      "True Answer: Expedia.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What was misspelled when received the tickets?\n",
      "Prediction:  Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "\n",
      "He hailed Strank as a true American hero and a wonderful example of the remarkable contribution and sacrifices that immigrants have made to our great republic throughout its history.\n",
      "True Answer: his wife's name,\n",
      "Exact match: False\n",
      "F1 score: 0.01\n",
      "\n",
      "Question: Who faces 22 felony counts in connection with sex tape?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Chester Arthur Stiles,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who discovered the tape?\n",
      "Prediction:  U.S. Citizenship and Immigration Services\n",
      "True Answer: Darrin Tuck,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the evidence that shows a girl under 3 being assaulted?\n",
      "Prediction:  famous World War II photograph\n",
      "True Answer: videotape\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many felony counts does Stiles face?\n",
      "Prediction: \n",
      "True Answer: 22\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What group of people know about the case and hae strong feelings about it?\n",
      "Prediction: \n",
      "True Answer: a jury\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did the tape show?\n",
      "Prediction:  U.S. flag on Iwo Jima\n",
      "True Answer: images of the small girl being sexually assaulted.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many felony counts is Arthur Stiles facing?\n",
      "Prediction:  felony\n",
      "True Answer: 22\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who was charged with slaying the woman?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Philip Markoff, a pre-med student at Boston University\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who makes public plea?\n",
      "Prediction:  Strank's younger sister, Mary Pero.\n",
      "True Answer: Boston Police Department,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who was charged\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Philip Markoff,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what was he charged with\n",
      "Prediction:  raising the U.S. flag on Iwo Jima\n",
      "True Answer: murder\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What else was the suspect charged with?\n",
      "Prediction: \n",
      "True Answer: the armed robbery and kidnapping of another victim,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what did the police say\n",
      "Prediction: \n",
      "True Answer: said. \"It appears that there was a struggle between the victim and the suspect in the threshold of the hotel room immediately prior to the shooting,\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: whom did police charge?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Philip Markoff,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: The suspect is how old?\n",
      "Prediction:  3,\n",
      "True Answer: 22-year-old\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does the columnist describe draper as?\n",
      "Prediction:  a true American hero\n",
      "True Answer: complicated man\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did columnist describe Draper as?\n",
      "Prediction: \n",
      "True Answer: sexy.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what is frustrating?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: the refusal or inability to \"turn it off\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What year was Bhutto's father hanged?\n",
      "Prediction:  1935\n",
      "True Answer: 1979\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What happened to Bhutto in October?\n",
      "Prediction:  killed in action on the island\n",
      "True Answer: narrowly escaped injury\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What happened to her father?\n",
      "Prediction:  was naturalized\n",
      "True Answer: hanged in 1979\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who attempted to assassinate Bhutto in October?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: suicide bombing\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who was the first female prime minister of a Muslim country?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Bhutto,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who was hanged in 1979?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Zulfikar Ali Bhutto,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did Bhutto refuse to allow?\n",
      "Prediction: \n",
      "True Answer: assassins\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is Georgia supposed to join?\n",
      "Prediction: \n",
      "True Answer: NATO\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what do european countries do\n",
      "Prediction:  made to our great republic\n",
      "True Answer: expressed concerns about the missile defense system.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what did bush say\n",
      "Prediction:  Strank as a true American hero and a wonderful example of the remarkable contribution and sacrifices that immigrants have made to our great republic throughout its history.\n",
      "True Answer: A planned missile defense system in Eastern Europe poses no threat to Russia,\n",
      "Exact match: False\n",
      "F1 score: 0.06\n",
      "\n",
      "Question: What do European countries share?\n",
      "Prediction:  U.S. citizenship\n",
      "True Answer: Russian concerns that the defensive shield could be used for offensive aims.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: to where they carried the injured?\n",
      "Prediction:  Iwo Jima\n",
      "True Answer: Nasser Medical Institute in Cairo,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many tourists are injured?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: 19\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is \"A Lion Among Men,\" about?\n",
      "Prediction:  flag-raising\n",
      "True Answer: -- the motherless cub defended by Elphaba in \"Wicked.\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did Maguire write for 14 years?\n",
      "Prediction: \n",
      "True Answer: children's books\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many Maguire books of Wicked has he sold?\n",
      "Prediction: \n",
      "True Answer: 2.5 million copies,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How long has Maguire been writing childrens's books?\n",
      "Prediction: \n",
      "True Answer: 14 years\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the name of Gregory's new book?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: \"A Lion Among Men,\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many years did Maguire write children's books?\n",
      "Prediction:  1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945\n",
      "True Answer: 14\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many copies has \"Wicked\" sold?\n",
      "Prediction:  copies\n",
      "True Answer: 2.5 million\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many times did the novel 'Wicked' sell?\n",
      "Prediction: \n",
      "True Answer: \"Wicked,\" has sold more than 2.5 million copies,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is Gregory Maguire's new book about?\n",
      "Prediction: \n",
      "True Answer: tells the story of the Cowardly Lion\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is wanted for questioning in the death of a two-year-old girl?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Arthur\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: where was the child's body found?\n",
      "Prediction:  U.S. Citizenship and Immigration Services\n",
      "True Answer: in a stream in Shark River Park in Monmouth County\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is wanted for questioning in the death of a two year old girl?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Arthur\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is he wanted for\n",
      "Prediction:  U.S. citizenship\n",
      "True Answer: questioning in the death of a two-year-old girl,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where was the body discovered\n",
      "Prediction:  U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia,\n",
      "True Answer: in a stream in Shark River Park\n",
      "Exact match: False\n",
      "F1 score: 0.06\n",
      "\n",
      "Question: The child was last seen with who?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Arthur\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who is victor?\n",
      "Prediction:  U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank,\n",
      "True Answer: Manuel Mejia Munera was a drug lord with ties to paramilitary groups,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who did the defense minister issue a warning to ?\n",
      "Prediction:  Marines\n",
      "True Answer: drug traffickers\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What was found with a dead drug lord ?\n",
      "Prediction:  in action\n",
      "True Answer: short- and long-range weapons\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where was Manuel Munera on the wanted list ?\n",
      "Prediction:  Czechoslovakia\n",
      "True Answer: Colombia's\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did the government find?\n",
      "Prediction: \n",
      "True Answer: identity documents\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What title did Eikenberry hold whilst he served time in Kabul\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: U.S. security coordinator\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When did Eikenberry advise the secretary of defense?\n",
      "Prediction:  Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank,\n",
      "True Answer: for strategy, plans and policy on the Army staff.\n",
      "Exact match: False\n",
      "F1 score: 0.14\n",
      "\n",
      "Question: After how many years service did Eikenberry retire from the army\n",
      "Prediction:  1935\n",
      "True Answer: nearly 40\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did Eikenberry do?\n",
      "Prediction:  awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "True Answer: sent private cables to Obama last week,\n",
      "Exact match: False\n",
      "F1 score: 0.03\n",
      "\n",
      "Question: what did he advise US secretey of defense\n",
      "Prediction:  Michael Strank,\n",
      "True Answer: China, Taiwan, Hong Kong and Mongolia,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Name the four countries that he advised the US secretary of defense on\n",
      "Prediction:  United States\n",
      "True Answer: China, Taiwan, Hong Kong and Mongolia,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: who retired from the army\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Eikenberry\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is Apples iphone 4s voice assistant feature called?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Siri.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What was siri based on?\n",
      "Prediction:  World War II photograph raising the U.S. flag on Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising\n",
      "True Answer: artificial intelligence.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is siri based on\n",
      "Prediction:  flag-raising\n",
      "True Answer: onstage demos.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When did apple acquire siri\n",
      "Prediction:  February 23, 1945.\n",
      "True Answer: April 2010.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the name of the voice-assistant?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Siri\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What station did the train leave from?\n",
      "Prediction:  Iwo Jima\n",
      "True Answer: Liverpool Street\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What number of suspects are in the videos?\n",
      "Prediction:  Marines\n",
      "True Answer: three\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where does the train leave?\n",
      "Prediction:  Iwo Jima\n",
      "True Answer: Liverpool Street Station\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did the jurors see?\n",
      "Prediction: One of the Marines shown in a famous World War II photograph raising the U.S. flag on Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "\n",
      "He hailed Strank as a true American hero and a wonderful example of the remarkable contribution and sacrifices that immigrants have made to our great republic throughout its history.\n",
      "True Answer: transit bombings\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did videos also show?\n",
      "Prediction:  Marines shown in a famous World War II photograph raising the U.S. flag on Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "True Answer: the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.\n",
      "Exact match: False\n",
      "F1 score: 0.04\n",
      "\n",
      "Question: What does the video show?\n",
      "Prediction:  Strank and five others raising a flag on Iwo Jima.\n",
      "True Answer: transit bombings\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What will the jurors see?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Videos of the chaos and horrified reactions after the July 7, 2005, London\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What number of men were charged with conspiracy?\n",
      "Prediction: Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five\n",
      "True Answer: three\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What does the painting show?\n",
      "Prediction:  the flag-raising\n",
      "True Answer: Picasso's muse and mistress, Marie-Therese Walter.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What was the previous record?\n",
      "Prediction: \n",
      "True Answer: $104,327,006\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How much did the painting sell for?\n",
      "Prediction:  U.S. citizenship\n",
      "True Answer: $106,482,500\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What painting sold for $106.482,500?\n",
      "Prediction: \n",
      "True Answer: \"Nude, Green Leaves and Bust\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who was Picasso's muse?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Marie-Therese Walter.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where is the facility?\n",
      "Prediction:  Arlington, Virginia,\n",
      "True Answer: in Salt Lake City,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did FAA say about the situation?\n",
      "Prediction: He hailed Strank as a true American hero and a wonderful example of the remarkable contribution and sacrifices that immigrants have made to our great republic throughout its history.\n",
      "True Answer: \"The\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: where is the problem\n",
      "Prediction:  U.S. Citizenship and Immigration Services\n",
      "True Answer: a Federal Aviation Administration facility,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: where is all flight-plan information processed\n",
      "Prediction:  Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia,\n",
      "True Answer: through a facility in Salt Lake City, Utah,\n",
      "Exact match: False\n",
      "F1 score: 0.12\n",
      "\n",
      "Question: what does faa say\n",
      "Prediction:  Strank as a true American hero and a wonderful example of the remarkable contribution and sacrifices that immigrants have made to our great republic throughout its history.\n",
      "True Answer: \"The\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where is flight plan information processed?\n",
      "Prediction:  Arlington, Virginia,\n",
      "True Answer: Salt Lake City, Utah,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the problem the facility is having?\n",
      "Prediction: \n",
      "True Answer: processing data,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is there an unknown number of?\n",
      "Prediction:  citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "True Answer: flights affected\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What problem were FAA having?\n",
      "Prediction:  Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "True Answer: communications breakdown\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What has deteriorated this year?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: the peace with Israel\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: In what year was the president murder?\n",
      "Prediction:  March 1, 1945,\n",
      "True Answer: 1981,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who was assasinated?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: President Mohamed Anwar al-Sadat\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What happened in October 1981?\n",
      "Prediction: \n",
      "True Answer: assassination of\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who has Sadt's daughter implicated?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Mubarak,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What amount many members of Zoe's Ark are under arrest?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Six\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What president wants the journalists and flight crew released\n",
      "Prediction:  president\n",
      "True Answer: Chadian\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: WHAT DOES THE CHADIAN PRESIDENT WANT?\n",
      "Prediction: \n",
      "True Answer: journalists and the flight crew will be freed,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who was arrested in Chad?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Six members of Zoe's Ark\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is under arrest in Chad?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: But the four women and three men are\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is happening in Chad?\n",
      "Prediction:  awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "True Answer: Hundreds of women protest\n",
      "Exact match: False\n",
      "F1 score: 0.01\n",
      "\n",
      "Question: What is the Chadian president's reaction?\n",
      "Prediction: \n",
      "True Answer: Idriss Deby hopes the journalists and the flight crew\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where are the children from?\n",
      "Prediction:  Czechoslovakia\n",
      "True Answer: Chad\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who wants journalists, flight crew released?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Chadian President Idriss Deby\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What number were arrested in Chad\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Six members\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: few were arrested in chad?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Six members of Zoe's Ark\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the Chadian president asking for?\n",
      "Prediction:  U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935.\n",
      "True Answer: journalists and the flight crew will be freed,\n",
      "Exact match: False\n",
      "F1 score: 0.04\n",
      "\n",
      "Question: Where did they try to fly from\n",
      "Prediction: \n",
      "True Answer: Chad\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who interviewed the children that tried to fly out of Chad?\n",
      "Prediction: Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press\n",
      "True Answer: The Red Cross, UNHCR and UNICEF\n",
      "Exact match: False\n",
      "F1 score: 0.02\n",
      "\n",
      "Question: What does the president want?\n",
      "Prediction:  citizenship\n",
      "True Answer: hopes the journalists and the flight crew will be freed,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Which charity organisations have been interviewing children?\n",
      "Prediction:  Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services\n",
      "True Answer: Red Cross, UNHCR and UNICEF\n",
      "Exact match: False\n",
      "F1 score: 0.04\n",
      "\n",
      "Question: WHO ARE UNDER ARREST IN CHAD?\n",
      "Prediction: Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men\n",
      "True Answer: Three French journalists, a seven-member Spanish flight crew and one Belgian\n",
      "Exact match: False\n",
      "F1 score: 0.02\n",
      "\n",
      "Question: where was the blast\n",
      "Prediction:  Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi\n",
      "True Answer: a municipal building in Baghdad's Sadr City,\n",
      "Exact match: False\n",
      "F1 score: 0.02\n",
      "\n",
      "Question: Who was killed in the Mosul suicide bomb?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: two soldiers and two civilians from the Defense and State departments\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many americans died\n",
      "Prediction: \n",
      "True Answer: Four\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many were killed in the bombing?\n",
      "Prediction: \n",
      "True Answer: Four\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who was killed in Mosul?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: two soldiers and two civilians from the Defense and State departments\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who did the victim work for?\n",
      "Prediction: \n",
      "True Answer: U.S. Defense Department\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who was shot in California?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Samuel Herr,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where is Daniel Wozniak held without bail?\n",
      "Prediction:  Virginia\n",
      "True Answer: California\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Police says he shot Herr at California training base for what?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: financial gain,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is charged with murder?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Daniel Wozniak,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is charged with two counts of murder?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Daniel Wozniak,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is Daniel Wozniak charged with?\n",
      "Prediction: \n",
      "True Answer: two counts of murder.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Wozniak is being held where?\n",
      "Prediction: \n",
      "True Answer: Costa Mesa Police Department\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where did Daniel Wozniak shoot Samuel Herr?\n",
      "Prediction:  on top of Mount Suribachi\n",
      "True Answer: Los Alamitos Joint Forces Training Base\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who got engaged to Ryan Adams?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Mandy Moore\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is Moore famous for?\n",
      "Prediction:  World War II photograph raising the U.S. flag on Iwo Jima\n",
      "True Answer: role as a bride in the 2007 movie \"License to Wed\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is Moore better known for?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: success as a recording artist\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is she engaged to?\n",
      "Prediction:  Mary Pero\n",
      "True Answer: Ryan Adams.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: where Site raised $17,000 before crashing on Tuesday due to high volume?\n",
      "Prediction:  Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia,\n",
      "True Answer: City of Los Angeles' Web\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many police were on hand?\n",
      "Prediction:  Strank,\n",
      "True Answer: Three thousand\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Costs include what?\n",
      "Prediction: Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising\n",
      "True Answer: putting extra police on the streets, trash pickup, sanitation, traffic control and more for the Tuesday event,\n",
      "Exact match: False\n",
      "F1 score: 0.06\n",
      "\n",
      "Question: How many cops were at the event?\n",
      "Prediction: \n",
      "True Answer: Three thousand\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did costs include?\n",
      "Prediction:  flag-raising\n",
      "True Answer: putting extra police on the streets, trash pickup, sanitation, traffic control and more\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Site raised how much money before crashing?\n",
      "Prediction:  money\n",
      "True Answer: $17,000\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: what City set up Web page asking Jackson fans to donate money?\n",
      "Prediction:  City\n",
      "True Answer: Los Angeles'\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How much money was raised?\n",
      "Prediction:  money\n",
      "True Answer: $17,000\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Which political party did Nepal belong to?\n",
      "Prediction:  United States\n",
      "True Answer: Communist\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What was Nepal's old job?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: former general secretary of the Communist Party,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When did Pushpa Kamal Dahal resign ?\n",
      "Prediction:  March 1, 1945,\n",
      "True Answer: May 4\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is Nepal's age?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: 56,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who was the only candidate?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Madhav Kumar Nepal\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the age of Madhav  Kumar Nepal?\n",
      "Prediction: Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3,\n",
      "True Answer: 56,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who resigned as Prime Minister?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Pushpa Kamal Dahal, the Maoist chairman,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When did the Maoist chairman resign?\n",
      "Prediction: Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945,\n",
      "True Answer: May 4\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What age is Madhav Nepal?\n",
      "Prediction:  3,\n",
      "True Answer: 56,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the Palm Jumeirah?\n",
      "Prediction: \n",
      "True Answer: A huge man-made island\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where is Palm Jumeirah island?\n",
      "Prediction: \n",
      "True Answer: off the coast of Dubai\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who will be at the opening party?\n",
      "Prediction: Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services recently discovered that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero\n",
      "True Answer: Oprah Winfrey, Michael Jordan, Robert De Niro, Janet Jackson\n",
      "Exact match: False\n",
      "F1 score: 0.03\n",
      "\n",
      "Question: When was the opening party?\n",
      "Prediction:  Tuesday\n",
      "True Answer: Thursday night.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where is Dubai?\n",
      "Prediction:  Czechoslovakia\n",
      "True Answer: Arab Emirates\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How much has the addition of the man-made island increased the Dubai coastline?\n",
      "Prediction: \n",
      "True Answer: 100 percent\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the name of the man-made island?\n",
      "Prediction:  Iwo Jima\n",
      "True Answer: Palm Jumeirah\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where is the island located?\n",
      "Prediction:  Iwo Jima\n",
      "True Answer: off the coast of Dubai\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many died in mall shooting?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: eight.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What are malls expected to assess?\n",
      "Prediction:  U.S. Citizenship and Immigration Services\n",
      "True Answer: their emergency plans\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many people did the gunman kill?\n",
      "Prediction:  U.S. flag on Iwo Jima was posthumously awarded a certificate of U.S. citizenship on Tuesday.\n",
      "\n",
      "The Marine Corps War Memorial in Virginia depicts Strank and five others raising a flag on Iwo Jima.\n",
      "\n",
      "Sgt. Michael Strank,\n",
      "True Answer: eight.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What day did the shooting occur?\n",
      "Prediction:  February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "True Answer: Wednesday.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: How many people did the gunman shoot?\n",
      "Prediction:  many\n",
      "True Answer: eight.\"\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What do security experts say abut such incidents?\n",
      "Prediction:  that Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "True Answer: it's a matter of money.\n",
      "Exact match: False\n",
      "F1 score: 0.06\n",
      "\n",
      "Question: What is impossible to anticipate?\n",
      "Prediction:  citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "\n",
      "Strank and five other men became national icons when an Associated Press photographer captured the image of them planting an American flag on top of Mount Suribachi on February 23, 1945.\n",
      "\n",
      "Strank was killed in action on the island on March 1, 1945, less than a month before the battle between Japanese and U.S. forces there ended.\n",
      "\n",
      "Jonathan Scharfen, the acting director of CIS, presented the citizenship certificate Tuesday.\n",
      "True Answer: murderous rampage\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where was the shooting?\n",
      "Prediction:  on the island\n",
      "True Answer: Westroads Mall in Omaha, Nebraska,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What are malls expected to do in wake of Wednesday shooting?\n",
      "Prediction:  flag-raising\n",
      "True Answer: review their emergency plans and consider additional security measures\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did a gunman and Omaha Nebraska mall do?\n",
      "Prediction: \n",
      "True Answer: murderous rampage\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who opened civil rights investigation?\n",
      "Prediction: Sgt. Michael Strank, who was born in Czechoslovakia and came to the United States when he was 3, derived U.S. citizenship when his father was naturalized in 1935. However, U.S. Citizenship and Immigration Services\n",
      "True Answer: FBI's Baltimore field office\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who is being held for the death of a police officer?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Ronnie White,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What type of investigation did the FBI open?\n",
      "Prediction:  Strank never was given citizenship papers.\n",
      "\n",
      "At a ceremony Tuesday at the Marine Corps Memorial -- which depicts the flag-raising -- in Arlington, Virginia, a certificate of citizenship was presented to Strank's younger sister, Mary Pero.\n",
      "True Answer: civil rights\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who has been held following the death of police officer?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Ronnie White,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What did Ronnie White die of?\n",
      "Prediction: \n",
      "True Answer: strangulation and asphyxiation\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where was Ronnie White?\n",
      "Prediction:  Arlington, Virginia,\n",
      "True Answer: Prince George's County Correctional Center,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Where was White being held?\n",
      "Prediction:  Marine Corps War Memorial\n",
      "True Answer: Prince George's County Correctional Center,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the name of the defendant?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Casey Anthony,\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What doesn't help the mother's defense?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: her alibi\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What could hinder the prosecution?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: lack of a cause of death and the absence of any soft tissue on the toddler's skeletal remains\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: When did the toddler vanish?\n",
      "Prediction:  March 1, 1945,\n",
      "True Answer: last summer.\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: What is the cause of death?\n",
      "Prediction:  in action\n",
      "True Answer: no evidence\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n",
      "Question: Who vanished last summer?\n",
      "Prediction: Sgt. Michael Strank,\n",
      "True Answer: Caylee Anthony's\n",
      "Exact match: False\n",
      "F1 score: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f1=0\n",
    "for contexts, question, answer in zip(valid_contexts[:], valid_questions[:], valid_str_ans[:]):\n",
    "    f1 += question_answer(context, question, answer)\n",
    "avg_f1_score=f1/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bb79a30e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-22T14:33:31.212440Z",
     "iopub.status.busy": "2023-10-22T14:33:31.211623Z",
     "iopub.status.idle": "2023-10-22T14:33:31.216775Z",
     "shell.execute_reply": "2023-10-22T14:33:31.215898Z"
    },
    "papermill": {
     "duration": 2.567685,
     "end_time": "2023-10-22T14:33:31.218985",
     "exception": false,
     "start_time": "2023-10-22T14:33:28.651300",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 score=0.004669999999999997\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average F1 score={avg_f1_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266cd077",
   "metadata": {
    "papermill": {
     "duration": 2.432301,
     "end_time": "2023-10-22T14:33:36.215787",
     "exception": false,
     "start_time": "2023-10-22T14:33:33.783486",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 17542.211909,
   "end_time": "2023-10-22T14:33:41.822842",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-22T09:41:19.610933",
   "version": "2.4.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0130e63149574e1a989b9ab81b9f8722": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "02638defc2094b3b93c9ff04ffb14543": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_0512404c7b0443ab832d9417b4d84b0e",
        "IPY_MODEL_7cd3fd4a19b647df9981528221fe1923",
        "IPY_MODEL_0501c87678ed40afb21b3696fe63da47"
       ],
       "layout": "IPY_MODEL_e84c690711ab48729cf86c7bc6448938"
      }
     },
     "0501c87678ed40afb21b3696fe63da47": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_7aeb6c63ebf74326826c66f421935f7c",
       "placeholder": "​",
       "style": "IPY_MODEL_8b8b5e31a53449d78f19b8a060a83e67",
       "value": " 496M/496M [00:15&lt;00:00, 25.5MB/s]"
      }
     },
     "0512404c7b0443ab832d9417b4d84b0e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c0452a143ad444a6b62a4bc142b404c1",
       "placeholder": "​",
       "style": "IPY_MODEL_35b2352e368c4094b2717d7b981cc0e7",
       "value": "Downloading model.safetensors: 100%"
      }
     },
     "096af1e42f824aefa2db31dcc141613a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0b1e6dc697e84fdeb7da200f4a89c991": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_60aae716d5f2497982828c90616f325f",
       "placeholder": "​",
       "style": "IPY_MODEL_efacef06aa79408cbd886bb6579323b0",
       "value": " 772/772 [00:00&lt;00:00, 66.2kB/s]"
      }
     },
     "0b931cc9ba38445cab362abe0e179e10": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d51df043ed5d469fa42c88dab7856f96",
       "max": 79.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_169c121f3b154d3ea317b54425309ceb",
       "value": 79.0
      }
     },
     "0ba1fcc5356b49c1be75f13917378b2d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0e85b1cc6ccc42cd9894d207914a20be": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "11177b325dd941398249f837ac975acd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "11371b0494f949ad835eea520d1a4870": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "116b2fe883a64146ada0631ebe205f7d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "14a88ba637414a5591abbf3b19c5a965": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "14f9265b38f8444c9b8f246527033d7d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "15cb20bf0e3e4679928a056516595db5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "169c121f3b154d3ea317b54425309ceb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "170a449b900d4c09b1f71b407324fb98": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_6333c1af232246238f10337ae633c9ad",
       "placeholder": "​",
       "style": "IPY_MODEL_dea97fdb87674827b020e955172adac1",
       "value": " 2/2 [00:00&lt;00:00, 99.44it/s]"
      }
     },
     "171fb75f4eb64409864da99ec48cb633": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f01ce7b166114c38868138ea81da4813",
       "max": 1633893.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_f909c89f80164ab9a992d53705f1944e",
       "value": 1633893.0
      }
     },
     "17845ae477a24042b20259a12703cd98": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_6c3b1c3faa6d45ad831ba0d0571a128c",
       "max": 29694916.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_25cef1ddb6a04fbd9c3888eba9e1d587",
       "value": 29694916.0
      }
     },
     "1a30e2610c9e412195360c09675ea027": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_a4bd278174e94e1486fd4863eceda568",
        "IPY_MODEL_0b931cc9ba38445cab362abe0e179e10",
        "IPY_MODEL_fc7590dfb7d04aacb8f2746a535a8d61"
       ],
       "layout": "IPY_MODEL_8187c383740f455885fc0075cebfe10f"
      }
     },
     "1ecff33579d748b6bc8cae5a441434ee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "22cc35a19f9d4009aa0b09605f1553e0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_11177b325dd941398249f837ac975acd",
       "placeholder": "​",
       "style": "IPY_MODEL_643b4885431e4fa5a14de64d028f62d9",
       "value": "Downloading (…)olve/main/merges.txt: 100%"
      }
     },
     "25cef1ddb6a04fbd9c3888eba9e1d587": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "28cfab1a50d446d8960b54f7e04c1fc8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ab43158ce18646ec8393c84dced54939",
       "placeholder": "​",
       "style": "IPY_MODEL_1ecff33579d748b6bc8cae5a441434ee",
       "value": " 899k/899k [00:00&lt;00:00, 12.6MB/s]"
      }
     },
     "28e960e8682e40c089f92143e60842e8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "301cbc342cb6429888a602885922f75e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d864f5fbb97042d6a4a0dd69c63ac3a7",
       "placeholder": "​",
       "style": "IPY_MODEL_aa324d05d85c4e7da210175c8579b1d9",
       "value": " 456k/456k [00:00&lt;00:00, 34.0MB/s]"
      }
     },
     "3105414ac63f418e9fcdf412b0ebb9d0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "35b2352e368c4094b2717d7b981cc0e7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "393fabb7336f498eab7e8dd594f3a1b4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3977ad7c8e9c4ed1a642385869280d84": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "3dccf341feed46cb8c11abe554321bb4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "3e9650d721d04a0cb9d64ecfebcf16b3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3f2186426cd8408ea4e04503057e1d8c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3e9650d721d04a0cb9d64ecfebcf16b3",
       "placeholder": "​",
       "style": "IPY_MODEL_0130e63149574e1a989b9ab81b9f8722",
       "value": " 2/2 [00:00&lt;00:00, 145.78it/s]"
      }
     },
     "3f807c70ce1842e18d26657da23c43a6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_97a8cab02404454ab6c0309ae511c141",
       "placeholder": "​",
       "style": "IPY_MODEL_3977ad7c8e9c4ed1a642385869280d84",
       "value": "Downloading data: 100%"
      }
     },
     "4045432f3db34da78d87f150c1a5b303": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "45037eb1be1744fdb3d1d82d18407ba3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_14a88ba637414a5591abbf3b19c5a965",
       "placeholder": "​",
       "style": "IPY_MODEL_c850f5f7e0474e968c7703c52f043dd1",
       "value": "100%"
      }
     },
     "454eafbc73034be8880f1af1dbe748c8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4eb667ce95a34e0682c5f2825e629c6e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "52d648fa5b86419ca8d6b7d529881704": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "52e4546376e44d6ab6e3306090de4fca": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "53ae4441768941bfbc2959841fb19eb4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_57ecf57c450c4924b8a58c37751700ff",
       "placeholder": "​",
       "style": "IPY_MODEL_f555b5a6f22743dab7d5aedebbe1fc80",
       "value": "Downloading (…)cial_tokens_map.json: 100%"
      }
     },
     "53e03d17d17544768e6f7924d1e4e7e9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "56bc58f4cc90442481259fef9923bf0c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "57ecf57c450c4924b8a58c37751700ff": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5b945f05a49f4ad2a3b21ab47422d8ab": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "5be0000e71454caab0434066e700f6b0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_62a4f1a479ad43319c1a565adca6451f",
        "IPY_MODEL_ad4ced9bdb5146a1981f43320c11df08",
        "IPY_MODEL_3f2186426cd8408ea4e04503057e1d8c"
       ],
       "layout": "IPY_MODEL_955cdbaaf74245f386febbdcb05230dd"
      }
     },
     "5fd845d397ca4f78873c9b0beecc8192": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "60aae716d5f2497982828c90616f325f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "62a4f1a479ad43319c1a565adca6451f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_aca955b262be4b2583f3af48606d7371",
       "placeholder": "​",
       "style": "IPY_MODEL_56bc58f4cc90442481259fef9923bf0c",
       "value": "Extracting data files: 100%"
      }
     },
     "6333c1af232246238f10337ae633c9ad": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "643b4885431e4fa5a14de64d028f62d9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "6629b1ccaa474b328278f34cafe00b6d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_8b685d61ad184aecac9e4f9a8a521e28",
        "IPY_MODEL_ed70504c38de4554afbb61e7bda38732",
        "IPY_MODEL_28cfab1a50d446d8960b54f7e04c1fc8"
       ],
       "layout": "IPY_MODEL_454eafbc73034be8880f1af1dbe748c8"
      }
     },
     "6800c4c8efdd4fb2bfa548c6554e90f9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "6b8428c9856f46a7a1050892f78e30fd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "6c3b1c3faa6d45ad831ba0d0571a128c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6e011f8f3e2648c285999d43ceda9530": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a92513d1bdb84e9bac33a7d4de078f5e",
       "placeholder": "​",
       "style": "IPY_MODEL_e14f16a4ae61498f8cff0870714e6e76",
       "value": " 29.7M/29.7M [00:00&lt;00:00, 62.0MB/s]"
      }
     },
     "6f905929a81647478efd64fc1f15bddf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7411e5a345424bf390f5a01fec38a063": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "77e7b80c52894a31afb0da76b005ae83": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "79a4a243ef7147569294c7f6dfbcac39": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_a2eb3fc8edd5481e98bd830b7509c146",
        "IPY_MODEL_f5df0a546d754dc493bd88c599fd9f1a",
        "IPY_MODEL_79c73206c91b410e9b49fbbf1c9635aa"
       ],
       "layout": "IPY_MODEL_15cb20bf0e3e4679928a056516595db5"
      }
     },
     "79c73206c91b410e9b49fbbf1c9635aa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_dc9c303bfbe14bc19e45cb4911241bac",
       "placeholder": "​",
       "style": "IPY_MODEL_77e7b80c52894a31afb0da76b005ae83",
       "value": " 571/571 [00:00&lt;00:00, 43.1kB/s]"
      }
     },
     "7aeb6c63ebf74326826c66f421935f7c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7cd3fd4a19b647df9981528221fe1923": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_11371b0494f949ad835eea520d1a4870",
       "max": 496254442.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_28e960e8682e40c089f92143e60842e8",
       "value": 496254442.0
      }
     },
     "7f7435b9137e4437a2ae15890e94fa97": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_22cc35a19f9d4009aa0b09605f1553e0",
        "IPY_MODEL_8fcb594063bb417e926711a7c3ab152e",
        "IPY_MODEL_301cbc342cb6429888a602885922f75e"
       ],
       "layout": "IPY_MODEL_5fd845d397ca4f78873c9b0beecc8192"
      }
     },
     "8187c383740f455885fc0075cebfe10f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "84ff1c4952914145afcedc6ac1a3fe72": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "894bb595c242457fbe0604c5abaa2eef": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_6f905929a81647478efd64fc1f15bddf",
       "placeholder": "​",
       "style": "IPY_MODEL_3105414ac63f418e9fcdf412b0ebb9d0",
       "value": " 1.63M/1.63M [00:00&lt;00:00, 19.6MB/s]"
      }
     },
     "8b685d61ad184aecac9e4f9a8a521e28": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f95afc92d18b41b1850ecebc884bf561",
       "placeholder": "​",
       "style": "IPY_MODEL_6800c4c8efdd4fb2bfa548c6554e90f9",
       "value": "Downloading (…)olve/main/vocab.json: 100%"
      }
     },
     "8b8b5e31a53449d78f19b8a060a83e67": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "8fcb594063bb417e926711a7c3ab152e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_dd3775fa1c7d437b9924f261bb8ddd1d",
       "max": 456318.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_6b8428c9856f46a7a1050892f78e30fd",
       "value": 456318.0
      }
     },
     "90020a1690ca4911bc4aef3fe7e5a6ba": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "955cdbaaf74245f386febbdcb05230dd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "97a8cab02404454ab6c0309ae511c141": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9fa4f8d52988469c8f1806c19c37bb15": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_45037eb1be1744fdb3d1d82d18407ba3",
        "IPY_MODEL_ec106ed70e144a1faa6c588c9da6564e",
        "IPY_MODEL_170a449b900d4c09b1f71b407324fb98"
       ],
       "layout": "IPY_MODEL_eb4e247f662f4e54b8e5aa1e158673b1"
      }
     },
     "a2eb3fc8edd5481e98bd830b7509c146": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_393fabb7336f498eab7e8dd594f3a1b4",
       "placeholder": "​",
       "style": "IPY_MODEL_4045432f3db34da78d87f150c1a5b303",
       "value": "Downloading (…)lve/main/config.json: 100%"
      }
     },
     "a490c040bbf148488b060c59652ca804": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_c1c9473e584f4d7c96666b425edeef89",
        "IPY_MODEL_ae572e7038444258afe70f84faf82290",
        "IPY_MODEL_d17da664417c42558452c2dedef5ae37"
       ],
       "layout": "IPY_MODEL_f31bf69a74a740d5a2ad8eabfca9a2ab"
      }
     },
     "a4bd278174e94e1486fd4863eceda568": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_0ba1fcc5356b49c1be75f13917378b2d",
       "placeholder": "​",
       "style": "IPY_MODEL_5b945f05a49f4ad2a3b21ab47422d8ab",
       "value": "Downloading (…)okenizer_config.json: 100%"
      }
     },
     "a92513d1bdb84e9bac33a7d4de078f5e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "aa324d05d85c4e7da210175c8579b1d9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "ab43158ce18646ec8393c84dced54939": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "aca955b262be4b2583f3af48606d7371": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ad4ced9bdb5146a1981f43320c11df08": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_fa19f7469c57479e9f15faf01d9bf419",
       "max": 2.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_fb8de474cca94a6f81716e61a0fb4d02",
       "value": 2.0
      }
     },
     "add6f05a99d7488daf7b0d9de5750528": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ae572e7038444258afe70f84faf82290": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c8ef333b2fc14204af71ffa336399c30",
       "max": 2.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_c6f6a8d9b5a340cc96a956fb13e4c20e",
       "value": 2.0
      }
     },
     "afcfaf36aada441e853935cb407ad45f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c0452a143ad444a6b62a4bc142b404c1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c1c9473e584f4d7c96666b425edeef89": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_53e03d17d17544768e6f7924d1e4e7e9",
       "placeholder": "​",
       "style": "IPY_MODEL_c7b5ab72682c4be3b0e5d77725539bbf",
       "value": "Downloading data files: 100%"
      }
     },
     "c35677d9968e470ab214b589cf021ecb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_53ae4441768941bfbc2959841fb19eb4",
        "IPY_MODEL_ddbbc97f28124a3ba64f82ebaadcf2cf",
        "IPY_MODEL_0b1e6dc697e84fdeb7da200f4a89c991"
       ],
       "layout": "IPY_MODEL_efacda6e54e94b6b81acd7e1c9a7df74"
      }
     },
     "c4dddeb5bee54921a2f5c26ce46333fa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "c6f6a8d9b5a340cc96a956fb13e4c20e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "c7b5ab72682c4be3b0e5d77725539bbf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "c850f5f7e0474e968c7703c52f043dd1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "c8ef333b2fc14204af71ffa336399c30": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d17da664417c42558452c2dedef5ae37": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_096af1e42f824aefa2db31dcc141613a",
       "placeholder": "​",
       "style": "IPY_MODEL_3dccf341feed46cb8c11abe554321bb4",
       "value": " 2/2 [00:01&lt;00:00,  1.65it/s]"
      }
     },
     "d51df043ed5d469fa42c88dab7856f96": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d864f5fbb97042d6a4a0dd69c63ac3a7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "dc9c303bfbe14bc19e45cb4911241bac": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "dd3775fa1c7d437b9924f261bb8ddd1d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ddbbc97f28124a3ba64f82ebaadcf2cf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_0e85b1cc6ccc42cd9894d207914a20be",
       "max": 772.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_90020a1690ca4911bc4aef3fe7e5a6ba",
       "value": 772.0
      }
     },
     "dea97fdb87674827b020e955172adac1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "e14f16a4ae61498f8cff0870714e6e76": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "e35057ce8c454f43ac04f6021ceae519": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f87af19a466b4a18a3a8ce0210dc7eab",
        "IPY_MODEL_171fb75f4eb64409864da99ec48cb633",
        "IPY_MODEL_894bb595c242457fbe0604c5abaa2eef"
       ],
       "layout": "IPY_MODEL_4eb667ce95a34e0682c5f2825e629c6e"
      }
     },
     "e84c690711ab48729cf86c7bc6448938": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "eb4e247f662f4e54b8e5aa1e158673b1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ec106ed70e144a1faa6c588c9da6564e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_84ff1c4952914145afcedc6ac1a3fe72",
       "max": 2.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_116b2fe883a64146ada0631ebe205f7d",
       "value": 2.0
      }
     },
     "ece28adf6c004a568fd86b8094b9bb91": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "ed70504c38de4554afbb61e7bda38732": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_52e4546376e44d6ab6e3306090de4fca",
       "max": 898822.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_ece28adf6c004a568fd86b8094b9bb91",
       "value": 898822.0
      }
     },
     "efacda6e54e94b6b81acd7e1c9a7df74": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "efacef06aa79408cbd886bb6579323b0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "f01ce7b166114c38868138ea81da4813": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f31bf69a74a740d5a2ad8eabfca9a2ab": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f555b5a6f22743dab7d5aedebbe1fc80": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "f558ec9ca94d472abc446803db314369": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "f5df0a546d754dc493bd88c599fd9f1a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_14f9265b38f8444c9b8f246527033d7d",
       "max": 571.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_c4dddeb5bee54921a2f5c26ce46333fa",
       "value": 571.0
      }
     },
     "f87af19a466b4a18a3a8ce0210dc7eab": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_7411e5a345424bf390f5a01fec38a063",
       "placeholder": "​",
       "style": "IPY_MODEL_52d648fa5b86419ca8d6b7d529881704",
       "value": "Downloading data: 100%"
      }
     },
     "f909c89f80164ab9a992d53705f1944e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "f95afc92d18b41b1850ecebc884bf561": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fa19f7469c57479e9f15faf01d9bf419": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fb8de474cca94a6f81716e61a0fb4d02": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "fc7590dfb7d04aacb8f2746a535a8d61": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_add6f05a99d7488daf7b0d9de5750528",
       "placeholder": "​",
       "style": "IPY_MODEL_f558ec9ca94d472abc446803db314369",
       "value": " 79.0/79.0 [00:00&lt;00:00, 6.20kB/s]"
      }
     },
     "fce10c543fac44a5ba3f91ebe9353541": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_3f807c70ce1842e18d26657da23c43a6",
        "IPY_MODEL_17845ae477a24042b20259a12703cd98",
        "IPY_MODEL_6e011f8f3e2648c285999d43ceda9530"
       ],
       "layout": "IPY_MODEL_afcfaf36aada441e853935cb407ad45f"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
